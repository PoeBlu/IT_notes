<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>DeepLearning_theory_map(alpha) <!-- ignore --></title>
<a href="https://ml-cheatsheet.readthedocs.io">https://ml-cheatsheet.readthedocs.io</a>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>


<body>
 Mathematical Foundations
<table>
<tr>
<td>
  <a xsmall TODO href="XXX">Linear Algebra</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Lagrange Optimization</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Probability Theory</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Gaussian Derivatives and Integrals</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Hypothesis Testing</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Information Theory</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Computational Complexity</a>
<pre xxxsmall zoom>
</pre>
</td>

</tr>
</table>
<table>
<tr>
<td>
  <span xsmall>DECISION THEORY</a>
<pre xxxsmall zoom bgorange>
There is an overall single cost associated with our decision,
and our true task is to make a decision rule (i.e., set a decision boundary)
so as to minimize such a cost.

This is the central task of decision theory of which pattern
classification is (perhaps) the most important subfiled.

Classification is, at base, the task of recovering the model that generated
the patterns.

Different classification techniques are useful depending on the
type of candidate models themselfs.

 Statistical pattern recognition models focus on the statistical
   properties of the patterns. When not enought (training) data is available,
 knowledge of the problem domain is also used.

 Other models focus on the logical rules like syntactic pattern recognition,
  where rules or grammars describe our decision. 

 Patter classification differs from classical statistical *hypothesis testing*,
wherein the sensed data are used to decide whether or not to reject a *null hypothesis*
in favor of some alternative hypothesis. In this case, if the null-Hypothesis falls
below a "significance" threshold, we reject the null-hypothesis in favor of the
alternative.


 Because perfect classification performance is often impossible, a more general
task is to determine the probability for each of the possible categories.

sensing -> segmentation -> feature extraction -> classification -> post-processing
           ^^^^^^^^^^^^                                            (costs)        
           "one of the deepest                                                    
           problems in pattern
           recognition"

The design of a pattern recognition system usually entails the repetition of a
number of different activities: 
 - data collection: "large cost" of the development.
 - feature choice : Choice of distinguishing features.
 - model   choice : How to know whether a given model performs better
 - training       : Process of using data to determine the classifier
 - evaluation 

</pre>
  <span xsmall>Vocabulary</a>
<pre xxxsmall zoom>
Feature:  Or Property is the data output from a "feature extractor" process
         Values are passed to a classfier.

Model: Determines the set of features used for the classifier.
      It can be a single specific set of features.

Feature-vector: A given vector (ordered set of features). The "dimensions" of
    the vector are provided by the model.

Feature-space: N-dimensional space of  feature inputs.
      Classification can be seen as the split of the feature-space in
      regions where the decision-cost is minimized.

Classifier: Takes feature values inputs and evaluates the "evidence" to make
            a decision or classification, assigning the input feature-vector to
            an output classification.
          The central aim of designing a classifier is to suggest actions when
          presented with not-yet-seen patterns. This is the issue of generalization.

Segmentation: Part of the pre-processing where objects of interest are "extracted"
            from background.

Feature Extraction: Process that takes-in a pattern and produces feature values.
    Number of features is virtually always chosen to be fewer than the total 
    necessary to describe the complete taret of interest, and this leads to a loss
    in information. 

     In acts of associate-memory, the ssytem takes-in a pattern and emits another 
    pattern which is representative of a general group of patterns. It thus reduces
    the information somewhat, but rarely to the extent that pattern classification
    does. In short, because of the crucial role of a decision in pattern recognition
    information, it is fundamentally an information reduction process.

    The conceptual boundary between feature-extraction and classification is arbitrary.


Subset and SUperset problem: Formally part of *mereology*, the study of part/whole
    relationships. It appears as though the best classifiers try to incorporate
    as much of the input into the categorization as "makes sense" but not too much.

Risk: Total spected cost  of making a wrong classification/Decision.

stochastic: The property of having a random probability distribution or pattern
      that may be analysed statistically but may not be predicted precisely.
</pre>

 <span xsmall>Regression , Interpolation , Density Estimation</a>
<pre xxxsmall zoom>
Regression: We seek to find some functional description of data, often with the
    goal of predicting values for new input. 
    Linear regression - in which the function is linear in the input variables-
    is the most popular and well studied form or regression.
Interpolation: In this case we known or can easily deduce, the function for 
    certain ranges of input; the problem is then to infer the function for intermediate
    ranges of input. 

Density estimation: is the problem of estimating the density (or probability) that
    a memeber of a certain category will be found to have particular features.
</pre>

   <span xsmall>Learning</a> 
<pre xxxsmall zoom>
Learning: "Any method" that incorporates information from training samples in the
     design of a classifier.
     Formally, it refers to some form of algorithm for reducing the error on a 
     set of training data. 

Supervised-learning:  A teacher provides a category label or cost for each pattern
    in a training set, and seeks to reduce the sum of the cost for thes patterns.

UnSupervised-learning (Clustering): There is no explicit teacher, and the system
    forms clusters or "natural groupings" of the input patterns.
    "Natural" is always defined explicitly or implicitly in the clustering system
    itself.
    Different clusting algorithms lead to different Clusters.

Reinformcement Learning:
    The most typical way to train a classifer is to present an input, 
     compute its tentative category label, and use th known target categorylabel to
    inprove
    might be an images of a character, the actual output of the classifier the 
    category label "R" and the desired otuput a "B". In the reinforcement learning
    or learning-with-a-critic, no desired category signal is given; instead, the
    only teaching feedback is that the tentative category is right or wrong.

gradient-descent: A range of *gradient-descent* algorithms that alter a 
     classifier's parameters in order to reduce an error measure now permeate the
     field of statistical pattern recognition.
</pre>

  <span xsmall TODO>Who is Who</a>
<pre xxxsmall zoom>
  - <a href="https://en.wikipedia.org/wiki/Richard_O._Duda">Richard O. Duda</a>: Author of "Pattern Classification" Book
    <a href="https://dl.acm.org/author_page.cfm?id=81332496778">ACM Digital Library Refs</a>
  - <a href="https://en.wikipedia.org/wiki/Peter_E._Hart"  >Peter E. Hart</a>  : Author of "Pattern Classification" Book
    <a href="https://dl.acm.org/author_page.cfm?id=81100122968">ACM Digital Library Refs</a>
  - David G. Stork : Author of "Pattern Classification" Book
    <a href="https://dl.acm.org/author_page.cfm?id=81100152072">ACM Digital Library refs</a>
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Bayesina Decision Theory</a>
<pre xxxsmall zoom>
- Ideal case in which the probability structure underlying the categories is known perfectly.

  <b>While not very realistic, it permits us to determine the optimal (Bayes) classifier
  against which we can compare all other classifiers.</b>

</pre>
</td>
<td>
  <a xsmall TODO href="XXX">Max.likelihood&amp;Bay.Param.Estimation</a>
<pre xxxsmall zoom>
- We address the case when the full probability structure underlying the
  categories is not known, but the general forms of their distributions are.
  Thus the uncertainty about a probability distribuition is represented by 
  the values of someh unkown parameter, and we seek to deteermine these parameters
  to attain the best categorization.
</pre>
</td>
<td>
  <a xsmall TODO href="XXX">NonParametric Techn.</a>
<pre xxxsmall zoom>
- We have no prior parameterized knowledge about the underlying probability
  structure;
  Our classification will be based on information provided by training samples alone.
</pre>
</td>
<td>
  <a xsmall TODO href="XXX">Linear Discriminant Functions</a>
<pre xxxsmall zoom>
- General approach of parameter estimation. We shall assume that the so-called
  "discriminat functions" are of a very particular form - namely linear-
  in order to derive a class of incremental training rules.
</pre>
</td>
</tr>
</table>
<table>
<tr>
<td>
  <a xsmall TODO href="XXX">Multilayer Neural Networks</a>
<pre xxxsmall zoom>
- We extend the "linear discrimant functions" to a class of very powerful
  algorithms for training multilayer neural networks;
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Stochastic Methods</a>
<pre xxxsmall zoom>
- We discuss simulate annealing, the Boltmann learning algorithm and other
  sotchastic methods which can avoid some the estimation problems that plague
  other neural methods.
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Nonmetric Methods</a>
<pre xxxsmall zoom>
- We move from statistical to logical rules.
  - tree-based algorithms (CARTS)
  - syntactic-based methods based on grammars.
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Algorithm-independent Machine Learning</a>
<pre xxxsmall zoom>
- <b>The "most" important and difficult chapter of the book</b>.
- Some of the results described here - those related to bias and variance,
 degress of freedom, the desire for "simple" classifers, and computatinal
 complexity- are subtle and crucial both theorically and practically.
  The rest of the concepts can ONLY be fully understood (or used) in light
 of the results presented here.
</pre>
</td>

<td>
  <a xsmall TODO href="XXX">Unsupervised Learning&amp;Clustering</a>
<pre xxxsmall zoom>
- Address the case when input training patterns are not labeled, and
  where our recognizer must determine the cluster structure. We also treat
  a related problem, that of learning with a critic, in which the teacher 
  provides only a single bit of information during the presentation of a
  training pattern - "yes", to indicate that the classification provided
  is correct or "not" otherwise.
</pre>
</td>
</tr>
</table>
<br/>

 <a xsmall TODO href="https://keras.io/">KERAS</a> <a xsmall TODO href="https://github.com/keras-team/keras/tree/master/examples">[Examples@Github]</a>
<table>
<tr>
<td>
  Summary
<pre xxxsmall zoom>
standard flow:
 define network -> compile -> train
</pre>


</td>
<td>
<a xsmall href="XXX">Sequential model(linear stack of layers)</a><br/>
  <span xsmall>LAYER CREATION</span>
<pre xxxsmall zoom>
- pass list of layer instances to the constructor:

from keras.models import Sequential
from keras.layers import Dense, Activation

model = Sequential(                 # <b>STEP 1 Define layers</b>
 [                                  # (one input layer in this example)
    Dense(32, input_shape=(784,)),  # ← Model needs FIRST LAYER input shape.
                                    #   input shape set through 'input_dim'                 (2D layers)
                                    #                           'input_dim'r+'input_length' (3D temp layers)
                                    # 32 : 32 hidden units (layers?)
    Activation('relu'),             fixed batch size     : (stateful recurrent nets): set through 'batch_size'
    Dense(10),
    Activation('softmax'),
 ]
)
</pre>

 <span xsmall>Compile (multi-class, binary, mean-sq.err,custom)</span> 
<pre xxxsmall zoom>
+----------------------------------------------------------------------------------------------------+
|                                COMPILES ARGUMENTS                                                  |
+----------------------------------------------------------------------------------------------------+

OPTIMIZER:                      | LOSS FUNCTION:                         | LIST OF METRICS:
string-ID of existing optimizer | string-ID of existing loss funct       | string-ID 
 ('rmsprop', 'adagrad',...)     | ('categorical_crossentropy', 'mse',..) |  (metrics=['accuracy'])
OR Optimizer class instance.    | OR objective function.                 | OR Custom metric function


MULTI-CLASS CLASS.PROBLEM         | BINARY CLASS.PROBLEM         | MEAN SQUARED ERROR    | # CUSTOM METRICS
model.compile(                    | model.compile(               | REGRE.PROBLEM         | import keras.backend as K
  optimizer='rmsprop',            |   optimizer='rmsprop',       | model.compile(        | 
  loss='categorical_crossentropy',|   loss='binary_crossentropy',|   optimizer='rmsprop',| def mean_pred(y_true, y_pred):
  metrics=['accuracy'])           |   metrics=['accuracy'])      |   loss='mse')         |   return K.mean(y_pred)
                                                                                         | 
                                                                                         | model.compile(
                                                                                         |   optimizer='rmsprop',
                                                                                         |   loss='binary_crossentropy',
                                                                                         |   metrics=['accuracy', mean_pred])
</pre>
  TRAINING:
<pre xxxsmall zoom>
Ex.
import numpy as np              # ← INPUT DATA/LABELS ARE NUMPY ARRAYS.
input_data = np.random.random(  # ← Dummy data (input_layer.input_dim=100)
       (1000, 100))

BINARY CLASSIFICATION PROBLEM  | MULTI-CLASS (10) Class.problem
input_labels =                 | input_labels =       
  np.random.randint(           |   np.random.randint(
  2, size=(1000, 1))           |   10, size=(1000, 1))
                               | 
                               | # Convert labels -> cat.one-hot encoding
                               | input_one_hot_lbls =  # 
                               |    keras.utils.       #                              
                               |     to_categorical(
                               |       labels, num_classes=10)
                               |
model.fit(                     | model.fit(            # ← train the model, (typically using 'fit')
 input_data,                   |   input_data,         #   input data
 input_labels,                 |   input_one_hot_lbls, #   input labels         
 epochs=10,                    |   epochs=10,          #   10 epochs iteration  
 batch_size=32                 |   batch_size=32       #   batches of 32 samples
)                              | )

</pre>
</td>
<td>
   Examples:<hr/>
  <span xsmall>multilayer perceptron (mlp) for multi-class softmax classification:</span>
<pre xxxsmall zoom>
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD

import numpy as np                      # Generate dummy data
x_train = np.random.random((1000, 20))
y_train = keras.utils.to_categorical(
  np.random.randint(10, size=(1000, 1)),
  num_classes=10)
x_test = np.random.random((100, 20))
y_test = keras.utils.to_categorical(
  np.random.randint(10, size=(100, 1)),
   num_classes=10)

model = Sequential()
# Dense(64) is a fully-connected layer with 64 hidden units.
# in the first layer, you must specify the expected input data shape:
# here, 20-dimensional vectors.
model.add(Dense(64, activation='relu', input_dim=20))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

model.fit(x_train, y_train,
          epochs=20,
          batch_size=128)
score = model.evaluate(x_test, y_test, batch_size=128)
</pre>
  <span xsmall>MLP for binary classification:</span>
<pre xxxsmall zoom>
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout

# Generate dummy data
x_train = np.random.random((1000, 20))
y_train = np.random.randint(2, size=(1000, 1))
x_test = np.random.random((100, 20))
y_test = np.random.randint(2, size=(100, 1))

model = Sequential()
model.add(Dense(64, input_dim=20, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train,
          epochs=20,
          batch_size=128)
score = model.evaluate(x_test, y_test, batch_size=128)
</pre>

  <span xsmall>VGG-like convnet:</span>
<pre xxxsmall zoom>
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import SGD

# Generate dummy data
x_train = np.random.random((100, 100, 100, 3))
y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)
x_test = np.random.random((20, 100, 100, 3))
y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)

model = Sequential()
# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.
# this applies 32 convolution filters of size 3x3 each.
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd)

model.fit(x_train, y_train, batch_size=32, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=32)
</pre>

  <span xsmall>Sequence classification with LSTM:</span>
<pre xxxsmall zoom>
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import LSTM

max_features = 1024

model = Sequential()
model.add(Embedding(max_features, output_dim=256))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=16)
</pre>

  <span xsmall>Sequence classification with 1D convolutions:</span>
<pre xxxsmall zoom>
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D

seq_length = 64

model = Sequential()
model.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100)))
model.add(Conv1D(64, 3, activation='relu'))
model.add(MaxPooling1D(3))
model.add(Conv1D(128, 3, activation='relu'))
model.add(Conv1D(128, 3, activation='relu'))
model.add(GlobalAveragePooling1D())
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=16)

Stacked LSTM for sequence classification

In this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher-level temporal representations.

The first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector).
</pre>

  <span xsmall>stacked LSTM</span>
<pre xxxsmall zoom>
from keras.models import Sequential
from keras.layers import LSTM, Dense
import numpy as np

data_dim = 16
timesteps = 8
num_classes = 10

# expected input data shape: (batch_size, timesteps, data_dim)
model = Sequential()
model.add(LSTM(32, return_sequences=True,
               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32
model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32
model.add(LSTM(32))  # return a single vector of dimension 32
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# Generate dummy training data
x_train = np.random.random((1000, timesteps, data_dim))
y_train = np.random.random((1000, num_classes))

# Generate dummy validation data
x_val = np.random.random((100, timesteps, data_dim))
y_val = np.random.random((100, num_classes))

model.fit(x_train, y_train,
          batch_size=64, epochs=5,
          validation_data=(x_val, y_val))
</pre>

  <span xsmall>Same stacked LSTM model rendered "stateful"</span>
<pre xxxsmall zoom>
- Stateful recurrent model:  is one for which the internal states (memories) 
   obtained after processing a batch of samples are reused as initial states for
   the samples of the next batch.
     This allows to process longer sequences while keeping computational 
   complexity manageable.

You can read more about stateful RNNs in the FAQ.

from keras.models import Sequential
from keras.layers import LSTM, Dense
import numpy as np

data_dim = 16
timesteps = 8
num_classes = 10
batch_size = 32

# Expected input batch shape: (batch_size, timesteps, data_dim)
# Note that we have to provide the full batch_input_shape since the network is stateful.
# the sample of index i in batch k is the follow-up for the sample i in batch k-1.
model = Sequential()
model.add(LSTM(32, return_sequences=True, stateful=True,
               batch_input_shape=(batch_size, timesteps, data_dim)))
model.add(LSTM(32, return_sequences=True, stateful=True))
model.add(LSTM(32, stateful=True))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# Generate dummy training data
x_train = np.random.random((batch_size * 10, timesteps, data_dim))
y_train = np.random.random((batch_size * 10, num_classes))

# Generate dummy validation data
x_val = np.random.random((batch_size * 3, timesteps, data_dim))
y_val = np.random.random((batch_size * 3, num_classes))

model.fit(x_train, y_train,
          batch_size=batch_size, epochs=5, shuffle=False,
          validation_data=(x_val, y_val))

</pre>
</td>
<td>
<a xsmall TODO href="https://keras.io/optimizers/">Usage of optimizers</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
<a xsmall TODO href="https://keras.io/losses/">Usage of loss functions</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
<a xsmall TODO href="https://keras.io/models/sequential/">The Sequential Model API</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
<a xsmall TODO href="https://keras.io/getting-started/functional-api-guide/">Functional API (Complex Models)</a>
<pre xxxsmall zoom>
- functional API is the way to go for defining complex models (multi-output models,
  directed acyclic graphs, or models with shared layers)

Ex 1: a densely-connected network
 (Sequential model is probably better for this simple case)
- tensor -> layer instance -> tensor

from keras.layers import Input, Dense
from keras.models import Model

inputs = Input(shape=(784,)) # ← input tensor/s
x = Dense(64, activation='relu')(inputs) # ←x: layer instances
x = Dense(64, activation='relu')(x)      # ←y: layer instances
predictions = Dense(10, activation='softmax')(x)

model = Model(inputs=inputs, outputs=predictions)    // @ma
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(data, labels)  # starts training

All models are callable, just like layers

With the functional API, it is easy to reuse trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just reusing the architecture of the model, you are also reusing its weights.

x = Input(shape=(784,))
# This works, and returns the 10-way softmax we defined above.
y = model(x)

This can allow, for instance, to quickly create models that can process sequences of inputs. You could turn an image classification model into a video classification model, in just one line.

from keras.layers import TimeDistributed

# Input tensor for sequences of 20 timesteps,
# each containing a 784-dimensional vector
input_sequences = Input(shape=(20, 784))

# This applies our previous model to every timestep in the input sequences.
# the output of the previous model was a 10-way softmax,
# so the output of the layer below will be a sequence of 20 vectors of size 10.
processed_sequences = TimeDistributed(model)(input_sequences)

Multi-input and multi-output models

Here's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.

Let's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc. The model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.

Here's what our model looks like:

multi-input-multi-output-graph

Let's implement it with the functional API.

The main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.

from keras.layers import Input, Embedding, LSTM, Dense
from keras.models import Model

# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.
# Note that we can name any layer by passing it a "name" argument.
main_input = Input(shape=(100,), dtype='int32', name='main_input')

# This embedding layer will encode the input sequence
# into a sequence of dense 512-dimensional vectors.
x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)

# A LSTM will transform the vector sequence into a single vector,
# containing information about the entire sequence
lstm_out = LSTM(32)(x)

Here we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model.

auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)

At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:

auxiliary_input = Input(shape=(5,), name='aux_input')
x = keras.layers.concatenate([lstm_out, auxiliary_input])

# We stack a deep densely-connected network on top
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)

# And finally we add the main logistic regression layer
main_output = Dense(1, activation='sigmoid', name='main_output')(x)

This defines a model with two inputs and two outputs:

model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])

We compile the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, you can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs.

model.compile(optimizer='rmsprop', loss='binary_crossentropy',
              loss_weights=[1., 0.2])

We can train the model by passing it lists of input arrays and target arrays:

model.fit([headline_data, additional_data], [labels, labels],
          epochs=50, batch_size=32)

Since our inputs and outputs are named (we passed them a "name" argument), we could also have compiled the model via:

model.compile(optimizer='rmsprop',
              loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},
              loss_weights={'main_output': 1., 'aux_output': 0.2})

# And trained it via:
model.fit({'main_input': headline_data, 'aux_input': additional_data},
          {'main_output': labels, 'aux_output': labels},
          epochs=50, batch_size=32)

Shared layers

Another good use for the functional API are models that use shared layers. Let's take a look at shared layers.

Let's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance).

One way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and then adds a logistic regression; this outputs a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs.

Because the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets.

Let's build this with the functional API. We will take as input for a tweet a binary matrix of shape (280, 256), i.e. a sequence of 280 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters).

import keras
from keras.layers import Input, LSTM, Dense
from keras.models import Model

tweet_a = Input(shape=(280, 256))
tweet_b = Input(shape=(280, 256))

To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want:

# This layer can take as input a matrix
# and will return a vector of size 64
shared_lstm = LSTM(64)

# When we reuse the same layer instance
# multiple times, the weights of the layer
# are also being reused
# (it is effectively *the same* layer)
encoded_a = shared_lstm(tweet_a)
encoded_b = shared_lstm(tweet_b)

# We can then concatenate the two vectors:
merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)

# And add a logistic regression on top
predictions = Dense(1, activation='sigmoid')(merged_vector)

# We define a trainable model linking the
# tweet inputs to the predictions
model = Model(inputs=[tweet_a, tweet_b], outputs=predictions)

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.fit([data_a, data_b], labels, epochs=10)

Let's pause to take a look at how to read the shared layer's output or output shape.
The concept of layer "node"

Whenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a "node" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2...

In previous versions of Keras, you could obtain the output tensor of a layer instance via layer.get_output(), or its output shape via layer.output_shape. You still can (except get_output() has been replaced by the property output). But what if a layer is connected to multiple inputs?

As long as a layer is only connected to one input, there is no confusion, and .output will return the one output of the layer:

a = Input(shape=(280, 256))

lstm = LSTM(32)
encoded_a = lstm(a)

assert lstm.output == encoded_a

Not so if the layer has multiple inputs:

a = Input(shape=(280, 256))
b = Input(shape=(280, 256))

lstm = LSTM(32)
encoded_a = lstm(a)
encoded_b = lstm(b)

lstm.output

>> AttributeError: Layer lstm_1 has multiple inbound nodes,
hence the notion of "layer output" is ill-defined.
Use `get_output_at(node_index)` instead.

Okay then. The following works:

assert lstm.get_output_at(0) == encoded_a
assert lstm.get_output_at(1) == encoded_b

Simple enough, right?

The same is true for the properties input_shape and output_shape: as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of "layer output/input shape" is well defined, and that one shape will be returned by layer.output_shape/layer.input_shape. But if, for instance, you apply the same Conv2D layer to an input of shape (32, 32, 3), and then to an input of shape (64, 64, 3), the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to:

a = Input(shape=(32, 32, 3))
b = Input(shape=(64, 64, 3))

conv = Conv2D(16, (3, 3), padding='same')
conved_a = conv(a)

# Only one input so far, the following will work:
assert conv.input_shape == (None, 32, 32, 3)

conved_b = conv(b)
# now the `.input_shape` property wouldn't work, but this does:
assert conv.get_input_shape_at(0) == (None, 32, 32, 3)
assert conv.get_input_shape_at(1) == (None, 64, 64, 3)

More examples

Code examples are still the best way to get started, so here are a few more.
Inception module

For more information about the Inception architecture, see Going Deeper with Convolutions.

from keras.layers import Conv2D, MaxPooling2D, Input

input_img = Input(shape=(256, 256, 3))

tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)
tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)

tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)
tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)

tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)
tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)

output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)

Residual connection on a convolution layer

For more information about residual networks, see Deep Residual Learning for Image Recognition.

from keras.layers import Conv2D, Input

# input tensor for a 3-channel 256x256 image
x = Input(shape=(256, 256, 3))
# 3x3 conv with 3 output channels (same as input channels)
y = Conv2D(3, (3, 3), padding='same')(x)
# this returns x + y.
z = keras.layers.add([x, y])

Shared vision model

This model reuses the same image-processing module on two inputs, to classify whether two MNIST digits are the same digit or different digits.

from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten
from keras.models import Model

# First, define the vision modules
digit_input = Input(shape=(27, 27, 1))
x = Conv2D(64, (3, 3))(digit_input)
x = Conv2D(64, (3, 3))(x)
x = MaxPooling2D((2, 2))(x)
out = Flatten()(x)

vision_model = Model(digit_input, out)

# Then define the tell-digits-apart model
digit_a = Input(shape=(27, 27, 1))
digit_b = Input(shape=(27, 27, 1))

# The vision model will be shared, weights and all
out_a = vision_model(digit_a)
out_b = vision_model(digit_b)

concatenated = keras.layers.concatenate([out_a, out_b])
out = Dense(1, activation='sigmoid')(concatenated)

classification_model = Model([digit_a, digit_b], out)

Visual question answering model

This model can select the correct one-word answer when asked a natural-language question about a picture.

It works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers.

from keras.layers import Conv2D, MaxPooling2D, Flatten
from keras.layers import Input, LSTM, Embedding, Dense
from keras.models import Model, Sequential

# First, let's define a vision model using a Sequential model.
# This model will encode an image into a vector.
vision_model = Sequential()
vision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))
vision_model.add(Conv2D(64, (3, 3), activation='relu'))
vision_model.add(MaxPooling2D((2, 2)))
vision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
vision_model.add(Conv2D(128, (3, 3), activation='relu'))
vision_model.add(MaxPooling2D((2, 2)))
vision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
vision_model.add(Conv2D(256, (3, 3), activation='relu'))
vision_model.add(Conv2D(256, (3, 3), activation='relu'))
vision_model.add(MaxPooling2D((2, 2)))
vision_model.add(Flatten())

# Now let's get a tensor with the output of our vision model:
image_input = Input(shape=(224, 224, 3))
encoded_image = vision_model(image_input)

# Next, let's define a language model to encode the question into a vector.
# Each question will be at most 100 word long,
# and we will index words as integers from 1 to 9999.
question_input = Input(shape=(100,), dtype='int32')
embedded_question = Embedding(input_dim=10000, output_dim=256, input_length=100)(question_input)
encoded_question = LSTM(256)(embedded_question)

# Let's concatenate the question vector and the image vector:
merged = keras.layers.concatenate([encoded_question, encoded_image])

# And let's train a logistic regression over 1000 words on top:
output = Dense(1000, activation='softmax')(merged)

# This is our final model:
vqa_model = Model(inputs=[image_input, question_input], outputs=output)

# The next stage would be training this model on actual data.

Video question answering model

Now that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. "what sport is the boy playing?" -> "football").

from keras.layers import TimeDistributed

video_input = Input(shape=(100, 224, 224, 3))
# This is our video encoded via the previously trained vision_model (weights are reused)
encoded_frame_sequence = TimeDistributed(vision_model)(video_input)  # the output will be a sequence of vectors
encoded_video = LSTM(256)(encoded_frame_sequence)  # the output will be a vector

# This is a model-level representation of the question encoder, reusing the same weights as before:
question_encoder = Model(inputs=question_input, outputs=encoded_question)

# Let's use it to encode the question:
video_question_input = Input(shape=(100,), dtype='int32')
encoded_video_question = question_encoder(video_question_input)

# And this is our video question answering model:
merged = keras.layers.concatenate([encoded_video, encoded_video_question])
output = Dense(1000, activation='softmax')(merged)
video_qa_model = Model(inputs=[video_input, video_question_input], outputs=output)


</pre>
</td>

<td>
<a xsmall TODO href="https://keras.io/getting-started/faq/#how-can-i-use-stateful-rnns">KERAS FAQ:</a>
<pre xxxsmall zoom>
</pre>
</td>




</tr>
</table>
</body>

<!--
REF: https://media.consensys.net/using-machine-learning-to-understand-the-ethereum-blockchain-1778485d603a
Using Machine Learning to Understand the Ethereum Blockchain

A hotbed field of study in data science analysis at the moment is machine learning, a form of AI that uses algorithms to study large sets of data. It’s used for everything from sequencing DNA to studying financial markets and brain-machine interfaces. There are many different kinds of machine learning, with differing data requirements and objectives. In the past year, ConsenSys has made a push to develop its analytics and data science capabilities with projects like Alethio, an analytics platform helps users visualize, interpret, and react to blockchain data in real time.

The immutable, public records and decentralized nature of blockchain networks provide an exciting sandbox for data scientists, offering whole new world of data to analyze and patterns to recognize. To begin understanding how we go about pulling meaning out of this seemingly chaotic data environment, we’ll begin by describing two main categories of machine learning that are being developed by data scientists at Consensys, and give a few examples of how each can be applied in practice.
Supervised vs. Unsupervised Learning

Unsupervised Learning involves finding patterns in a large data sets and using them to extract meaning. Unsupervised learning models are not predictive in nature — though they could play a role in a larger predictive modeling system. Rather, unsupervised learning seeks to reduce a large and complex dataset to simpler high-level patterns or themes. These themes can then be used as a reference to characterize individual data points and put them into a useful context.

Anomaly and novelty detection systems are examples of unsupervised learning models. By reducing a large dataset into a small number of common themes, one can learn what it means for a particular transaction or account point to be “normal.” By comparing any given transaction or account to this learned definition of normal, we can determine the extent to which they are anomalous compared to the global average (anomaly detection), or compared to a recent historical average (novelty detection). These anomaly detection systems can then be used to alert users whether anything unusual is happening on the whole blockchain, or within a particular subset of interesting accounts or transactions. Alethio currently offers an anomaly detection system for transactions, blocks, and accounts.

Other kinds of analysis offered by Alethio that could arguably be considered unsupervised learning including ranking algorithms, or influence analysis like page rank. While these are not commonly referred to as machine learning algorithms at all (rather, just algorithms), they do serve the same purpose of finding overall patterns in a dataset and using them to add context.

Supervised learning seeks to take a set of observations with known features, and uses them to estimate the corresponding value of some other variable (a response or label) for each observation. This could be broken down into two common categories: prediction and classification. Trying to use historical data to estimate the future value of variable (a response) is known as prediction. Trying to use existing data about an entity to determine whether that entity belongs to a certain category (assigning a “label”) is known as classification.

Generally speaking, the “knowns” on the blockchain consist of raw, protocol-level data that is available on-chain, such as transaction data. This raw data can be used to extract features for accounts, such as their total balance, average transaction frequency, average age of currency held, etc. Recent efforts by Alethio to augment protocol-level data with semantic lifting have expanded the set of “knowns” beyond the protocol layer to include application-level data, such as whether a contract is a token, and to which standard it complies. All of these known quantities can be used as the basis for features in a supervised learning model.

On the other hand, the unknown quantity (the label or response) is by definition not a piece of currently-available on-chain data; otherwise it would already be known and captured by our data pipelines. The unknown quantity might be the future value of some on-chain data, such as the balance of an account on some future date. More commonly, the unknown quantity is some value that is never available on-chain at all. If you are trying to predict whether the account belongs to some category, such as being a decentralized exchange, a DOS account, or a Ponzi scheme, you will need to look off-chain for this data.

The Importance of Datasets

This is where the data requirements for unsupervised learning on the blockchain become an important problem (read: opportunity!). In order to train and calibrate a supervised learning model, there must be some large initial set of data for which the value of the labels or responses is known. This calibrates the model so that the predicted and actual response are as close as possible. This means that when a new observation comes in where the response is unknown, the prediction will be close to the true value, assuming the new observation is being generated by a similar process that generated the original dataset. Once the training phase is complete and the model calibrated, it can then be applied to new observations where the response is unknown.

In the case of price prediction, this means having a large database of historical prices. In the case of classification of accounts, this means having an initial set of accounts that are already labeled as being a decentralized exchange, a DOS account, or a Ponzi.

In these classification examples, the labels in the dataset used for training are often only available through significant effort. One possibility would be to pull data from websites like coinmarketcap or etherscan, building ETLs to import interesting data from other blockchain businesses, or through the painstaking effort of trained research assistants who gather data about on-chain accounts by surfing the web and analyzing source code.

The realization of the importance of gathering external data about accounts (metadata) for the purposes of machine learning was the motivation for creating a new spoke at ConsenSys called Rakr. Through collaboration with Alethio and other spokes and services within the mesh, Rakr hopes to provide a platform for gathering and sharing this valuable metadata. While the implications of integrating blockchain metadata with raw on-chain data go far beyond machine learning, the applicability of this metadata for supervised machine learning will continue to be a primary use case for the Rakr platform. By combining Alethio’s powerful analytics platform with the valuable metadata provided by Rakr, the applications of data science at ConsenSys will be limited only by the imagination.
In Practice

The first example of a supervised learning model produced at ConsenSys was the Ponzi model developed by Alethio, which will be described in more detail during the sequel to this article. The development of this model lays the groundwork for many future analytics possibilities for Alethio. Alethio hopes to expand this model to a more general fraud model in the near term.

More generally, the feature extraction pipelines built during this model development effort can be reused to classify any account according to one of the labels in the Rakr database, including whether an account/contract is an exchange, an art DAO, a casino, a DOS-related account, and much more. As the set of interesting metadata provided by Rakr continues to grow, more new models will become possible. And as the analytics capabilities of Alethio grow and more useful features are created, these models will become more powerful and versatile.

Being able to know whether a given account is a fraud or related to a DOS attack is crucial for managing financial and network risk on the Ethereum network. If we want to productionize models that provide actionable insights about new accounts and very recent behavioral data, they must satisfy special requirements. For example, we must make sure that they are being updated in real time, and that the features being used for classification and prediction are reliable and complete at the time the model is run. This means that certain features that can be used for classification of “old” accounts, such as “whether a contract eventually self-destructed,” cannot be applied to accounts in real-time. Since the value of the feature may change in the future, it’s true value is not really known at the time the model is run.

Real-time machine learning models present unique challenges and opportunities that go beyond those of historical modeling techniques. With that said, the ability to classify accounts as frauds goes beyond real-time risk management; classification models can still be valuable even if they are applied “in the past”. Being able to accurately classify historical frauds is useful for research purposes, even if those accounts are no longer active. More generally, attaching tags to accounts on the blockchain allows users to define semantically interesting subsets of accounts on the blockchain (such as “ICOs” or “exchanges”), rendering the blockchain searchable based on criteria that humans care about.

Creating a database of empirical human knowledge about on-chain entities is already a valuable and challenging task, and a necessary foundation for many other products and services. But with over 30,000,000 Ethereum accounts and contracts to date and roughly 100,000 new accounts created every day, it is simply impossible for humans to tag the entire history of ethereum accounts, most of which have no useful information (such as contract source, a website, or any other identifying information) that could be used by humans to classify or tag them. This is why the machine learning models are crucial: because they are infinitely scalable, and can be used to classify accounts using only the raw data characterizing their on-chain behavior.

By augmenting human knowledge about the blockchain with powerful analytics and machine learning, we envision a blockchain where every account and entity is enriched with useful classifications and properties, whether empirical and created by humans, or predicted and created by statistical models. This will be a major step forward for the transparency and accessibility of knowledge on the blockchain, which are an essential aspects required for blockchain technology to flourish.

Keep an eye out for the next article by Paul Lintilhac, which will give an exposition of one of Alethio’s recent data science initiatives: the Ponzi Model.
___________________________

Unsupervised: 

Supervised:
___________
https://en.wikipedia.org/wiki/Kalman_filter
____________
https://github.com/wireservice/agate

agate is a Python data analysis library that is optimized for humans instead of machines. It is an alternative to numpy and pandas that solves real-world problems with readable code.

_________________
https://datanalytics.com/libro_r/
_________________
http://www.elmomentodecisivo.com/
________________
https://www.datanalytics.com/
_________________
https://www.infoq.com/presentations/algorithms-counting-reddit?utm_source=notification_email&utm_campaign=notifications&utm_medium=link&utm_content=content_in_followed_topic&utm_term=daily
____________________
https://www.infoq.com/presentations/data-ml-pipelines-stitchfix?utm_source=notification_email&utm_campaign=notifications&utm_medium=link&utm_content=content_in_followed_topic&utm_term=daily
_______________
https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html
_______________
https://ai.googleblog.com/
_______________
https://ai.google/
___________________
REF: https://dzone.com/articles/consensus-clustering-via-apache-spark
    In this article, we will discuss a technique called Consensus Clustering to assess the stability
    of clusters generated by a clustering algorithm with respect to small perturbations in the data
    set. We will review a sample application built using the Apache Spark machine learning library 
    to show how consensus clustering can be used with K-means, Bisecting K-means, and Gaussian 
    Mixture, three distinct clustering algorithms

___________________
Ontology (Aristoteles) : http://classics.mit.edu/Aristotle/categories.1.1.html
_________________________________________
https://www.serverwatch.com/server-news/nvidia-accelerates-server-workloads-with-rapids-gpu-advances.html
!!!!  
_____________________
https://www.linux.com/blog/holberton/2018/10/sourced-engine-simple-elegant-way-analyze-your-code

source{d} Engine: A Simple, Elegant Way to Analyze your Code
With the recent advances in machine learning technology, it is only a matter of time before developers can expect to run full diagnostics and information retrieval on their own source code. This can include autocompletion, auto-generated user tests, more robust linters, automated code reviews and more. I recently reviewed a new product in this sphere -- the source{d} Engine.
source{d} offers a suite of applications that uses machine learning on code to complete source code analysis and assisted code reviews. Chief among them is the source{d} Engine, now in public beta; it uses a suite of open source tools (such as Gitbase, Babelfish, and Enry) to enable large-scale source code analysis. Some key uses of the source{d} Engine include language identification, parsing code into abstract syntax trees, and performing SQL Queries on your source code such as:

    What are the top repositories in a codebase based on number of commits?

    What is the most recent commit message in a given repository?

    Who are the most prolific contributors in a repository
___________________________
https://www.eleconomista.es/empresas-finanzas/noticias/9449298/10/18/COMUNICADO-Huawei-lanza-una-plataforma-de-desarrollo-de-IA-con-ciclo-de-vida-completo-mas-rapida.html
"ModelArts es una plataforma de desarrollo de inteligencia artificial más rápida e inclusiva que cualquier otra plataforma de desarrollo de IA del mercado", dijo Zheng Yelai, vicepresidente de Huawei y presidente de la unidad de negocio Huawei Cloud. "Creemos que los desarrolladores de IA sabrán apreciar lo rápido que se inicia, completa entrenamientos e implanta modelos".

El etiquetado y la preparación de datos es un proceso largo en el desarrollo de la inteligencia artificial, y representa casi el 50% del tiempo necesario. ModelArts tiene un marco de gobernanza de datos integrado para el etiquetado y la preparación de datos durante el desarrollo de IA. El marco implementa un entrenamiento iterativo para reducir el volumen de datos que tienen que ser etiquetados manualmente, lo que aumenta por 100 la eficiencia del etiquetado y la preparación de datos.

Además, ModelArts integra diversas tecnologías de optimización, especialmente el sistema de paralelo híbrido con cascada para reducir a la mitad el entrenamiento requerido en un determinado modelo, conjunto de datos o conjunto de recursos de hardware.

La implantación de modelos de IA es un proceso complejo. Con ModelArts, los modelos de entrenamiento pueden moverse a dispositivos, la periferia y la nube con solo un clic. Los trabajos de inferencia en línea o por lotes se proporcionan a través de la nube para cumplir con los diferentes requisitos de las aplicaciones, como la implantación simultánea o distribuida.

ModelArts también incorpora varias tecnologías de IA, como el aprendizaje automático, el diseño de modelos y la configuración de parámetros para acelerar el desarrollo de la inteligencia artificial.

En términos de gestión del ciclo de vida del desarrollo de IA, ModelArts abarca la recogida de datos sin procesar, el etiquetado de datos, la creación de trabajos de entrenamiento, la selección de algoritmos, la creación de modelos y la creación de servicios de inferencia. ModelArts permite a los desarrolladores de IA compartir datos, modelos y API de inteligencia artificial.
Visión de IA

Por otra parte, HiLens consta de una plataforma de desarrollo de aplicaciones de visión con IA y de un dispositivo visual potenciado con capacidades de IA. HiLens cuenta con Skill, un nuevo concepto de desarrollo de IA. Skill consiste en un código de control y modelos entrenados en ModelArts. HiLens también es compatible con modelos entrenados en otros marcos convencionales. Las capacidades desarrolladas en HiLens pueden implantarse en cualquier dispositivo que tenga integrados los chips Ascend de IA.

El dispositivo visual HiLens se compone de una cámara inteligente compatible con inferencias. Los desarrolladores pueden usar el dispositivo HiLens para crear aplicaciones de visión e implantarlas en dispositivos y en la nube. El dispositivo visual HiLens integra el chip Ascend 310, que puede procesar 100 fotogramas por segundo y detectar caras en milisegundos. Además, los livianos contenedores integrados minimizan el uso de recursos y de ancho de banda de red, y pueden descargarse e iniciarse de forma rápida
_________________________________________
https://www.infoq.com/news/2018/11/PyTorch-Developer-Preview
_________________________________________
IA Classification:
  • Cognitive Processing (including Natural Language Processing, Computer Vision, Speech Recognition)
  • Conversasional Systems and Virtual Assistants (Question&Answering, ChatBots)
  • Machine Learning & Deep Learning
  • Reference frameworks (as IBM Watson, Microsoft Cognitive, Cognitive Services in AWS, and Google and others)
_____________________________
https://www.infoq.com/news/2018/11/Google-AI-Voice
_____________________________
______________________
Learn to clearly differentiate between the buzzwords—for example, machine learning, artificial intelligence, deep learning, data science, computer vision, and robotics. Read or listen to talks by experts on each of them. Watch this <a href="https://www.youtube.com/watch?v=tKa0zDDDaQk" target="_blank">amazing video by Brandon Rohrer</a>, an influential data scientist. Or this video about the <a href="https://www.youtube.com/watch?v=Ura_ioOcpQI" target="_blank">clear differences between various roles</a> associated with data science.
______________________
!!!
https://opensource.com/article/18/10/machine-learning-python-essential-hacks-and-tricks
Includes cheatsheets for NumPy, Pandas, Matplotlib and Seaborn, Scikit-learn
______________________

Narrow AI Taxonomy

Applications      RPA     Chatbots    Document Systems     ....


Narrow AI

      Domains   Vision    Sound   NLP      Knowledge&Reasoning   Multiagent-Systems

      IA         Machine-Learning      Genetic       Network    Robotics&Signals
                  (Deep-Learning)      Algorithms    Analysis

      Core       Planning,        Statistics and      Simulation
                 Search and       data mining
                 Optimization

      Infra     Cognitive API's     On-premises     Edge-computing    CPU/GPU/ASIC
_____________________________
https://github.com/autumnai/leaf

Open Machine Intelligence Framework for Hackers. (GPU/CPU)

Leaf is a few months old, but thanks to its architecture and Rust, it is already one of the fastest Machine Intelligence Frameworks available.

    See more Deep Neural Networks benchmarks on [Deep Learning Benchmarks][deep-learning-benchmarks-website].

Leaf is portable. Run it on CPUs, GPUs, and FPGAs, on machines with an OS, or on machines without one. Run it with OpenCL or CUDA. Credit goes to Collenchyma and Rust.


https://medium.com/@mjhirn/tensorflow-wins-89b78b29aafb

We started with the development of Leaf briefly before Google released Tensorflow. For two weeks Leaf’s hypothesis seemed unique.

Although Leaf has top-notch performance and an uniquely simple yet expressive API, it will lose against Tensorflow. Leaf’s current theoretical benefits[1] are less significant, than the benefits that Tensorflow provides[2] over the early, scientific frameworks like Torch, Caffe, Theano.

The next generation of tools, that help developers to build machine learning applications will build on Tensorflow, or more specifically on higher-level frameworks, like Keras, who abstract over multiple AI Engines[3]. Back in November, when we started, this trend was less obvious to us[4].

Now that good-enough tools, to build maintainable machine-learning applications, like Tensorflow and Keras, exist, venture capital flows more and more into companies who try to create immense value in verticals with new AI-driven applications, instead of infrastructure providers.

We wanted Leaf to become the #1 machine learning framework for developers, but it became apparent, that Leaf will not receive substantial traction outside the Rust community.

Which is why Max and I will suspend the development of Leaf and focus on new ventures.

I am staying in the space of startups and AI, working with a VC to explore a new type of early stage investment fund.

Thank you so much everyone who supported us on the way to more than 4.000 Github stars and almost into Github’s Top 1000. We are very grateful.

We will continue to give back to the community with our future projects.

[1]: Significantly easier/slimmer abstractions over computation and scheduling, first-class citizen support for CUDA/OpenCL/Rust and co., clean foundation for auto diff via dual numbers and differentiable programming, compression of neural network models.

[2]: Tensorflow and its ecosystem (incl. GCloud) provides a proven solution/process for testing, deploying and maintaining models.

[3]: E.g. Facebook is working on Flow, AutoML and Asimo, which are tools to make the creation of machine learning models even easier. If they build on Tensorflow or Torch or another framework needs to be seen.

[4]: This is why the the other frameworks will vanish or take niche positions. This includes Microsoft’s CNTK.


_____________________________
https://www.wired.com/2016/05/facebook-trying-create-ai-can-create-ai/


    Finally, Neural Networks That Actually Work

"It's almost like being the coach rather than the player," says Demis Hassabis, co-founder of DeepMind, the Google outfit behind the history-making AI that beat the world's best Go player. "You're coaxing these things, rather than directly telling them what to do."

That's why many of these companies are now trying to automate this trial and error—or at least part of it. If you automate some of the heavily lifting, the thinking goes, you can more rapidly push the latest machine learning into the hands of rank-and-file engineers—and you can give the top minds more time to focus on bigger ideas and tougher problems. This, in turn, will accelerate the progress of AI inside the Internet apps and services that you and I use every day.

In other words, for computers to get smarter faster, computers themselves must handle even more of the grunt work. The giants of the Internet are building computing systems that can test countless machine learning algorithms on behalf of their engineers, that can cycle through so many possibilities on their own. Better yet, these companies are building AI algorithms that can help build AI algorithms. No joke. Inside Facebook, engineers have designed what they like to call an "automated machine learning engineer," an artificially intelligent system that helps create artificially intelligent systems. It's a long way from perfection. But the goal is to create new AI models using as little human grunt work as possible.
Feeling the Flow

After Facebook's $104 billion IPO in 2012, Hussein Mehanna and other engineers on the Facebook ads team felt an added pressure to improve the company's ad targeting, to more precisely match ads to the hundreds of millions of people using its social network. This meant building deep neural networks and other machine learning algorithms that could make better use of the vast amounts of data Facebook collects on the characteristics and behavior of those hundreds of millions of people.

    'The more ideas you try, the better. The more data you try, the better.'

According to Mehanna, Facebook engineers had no problem generating ideas for new AI, but testing these ideas was another matter. So he and his team built a tool called Flow. "We wanted to build a machine-learning assembly line that all engineers at Facebook could use," Mehanna says. Flow is designed to help engineers build, test, and execute machine learning algorithms on a massive scale, and this includes practically any form of machine learning—a broad technology that covers all services capable of learning tasks largely on their own.

Basically, engineers could readily test an endless stream of ideas across the company's sprawling network of computer data centers. They could run all sorts of algorithmic possibilities—involving not just deep learning but other forms of AI, including logistic regression to boosted decision trees—and the results could feed still more ideas. "The more ideas you try, the better," Mehanna says. "The more data you try, the better." It also meant that engineers could readily reuse algorithms that others had built, tweaking these algorithms and applying them to other tasks.

Soon, Mehanna and his team expanded Flow for use across the entire company. Inside other teams, it could help generate algorithms that could choose the links for your Faceboook News Feed, recognize faces in photos posted to the social network, or generate audio captions for photos so that the blind can understand what's in them. It could even help the company determine what parts of the world still need access to the Internet.

With Flow, Mehanna says, Facebook trains and tests about 300,000 machine learning models each month. Whereas it once rolled a new AI model onto its social network every 60 days or so, it can now release several new models each week.
The Next Frontier

The idea is far bigger than Facebook. It's common practice across the world of deep learning. Last year, Twitter acquired a startup, WhetLab, that specializes in this kind of thing, and recently, Microsoft described how its researchers use a system to test a sea of possible AI models. Microsoft researcher Jian Sun calls it "human-assisted search."

    Engineers even built their own 'automated machine learning engineer.'

Mehanna and Facebook want to accelerate this. The company plans to eventually open source Flow, sharing it with the world at large, and according to Mehanna, outfits like LinkedIn, Uber, and Twitter are already interested in using it. Mehanna and team have also built a tool called AutoML that can remove even more of the burden from human engineers. Running atop Flow, AutoML can automatically "clean" the data needed to train neural networks and other machine learning algorithms—prepare it for testing without any human intervention—and Mehanna envisions a version that could even gather the data on its own. But more intriguingly, AutoML uses artificial intelligence to help build artificial intelligence.

As Mehana says, Facebook trains and tests about 300,000 machine learning models each month. AutoML can then use the results of these tests to train another machine learning model that can optimize the training of machine learning models. Yes, that can be a hard thing to wrap your head around. Mehanna compares it to Inception. But it works. The system can automatically chooses algorithms and parameters that are likely to work. "It can almost predict the result before the training," Mehanna says.

Inside the Facebook ads team, engineers even built that automated machine learning engineer, and this too has spread to the rest of the company. It's called Asimo, and according to Facebook, there are cases where it can automatically generate enhanced and improved incarnations of existing models—models that human engineers can then instantly deploy to the net. "It cannot yet invent a new AI algorithm," Mehanna says. "But who knows, down the road..."

It's an intriguing idea—indeed, one that has captivated science fiction writers for decades: an intelligent machine that builds itself. No, Asimo isn't quite as advanced—or as frightening—as Skynet. But it's a step toward a world where so many others, not just the field's sharpest minds, will build new AI. Some of those others won't even be human.


____________________
https://rocm.github.io/
____________________
https://course.elementsofai.com/
____________________
- Cognitive Processing (including NLP, Computer Vision, Speech Recog.)  
___________________________
OpenCV Python Tutorial - Find Lanes for Self-Driving Cars (Computer Vision Basics Tutorial)
- https://www.youtube.com/watch?v=eLTLtUVuuy4
______________________________
https://www.infoq.com/news/2019/01/amazon-sustainability-datasets?utm_source=notification_email&utm_campaign=notifications&utm_medium=link&utm_content=content_in_followed_topic&utm_term=daily
__________________
https://www.infoq.com/news/2019/01/exploring-quantum-neural-nets
__________________
https://www.infoq.com/presentations/ml-research-production?utm_source=notification_email&utm_campaign=notifications&utm_medium=link&utm_content=content_in_followed_topic&utm_term=daily
__________________
https://github.com/jpmorganchase/swblocks-decisiontree
____________________
https://opensource.com/article/19/2/mycroft-voice-assistant

_______________________
Tensorflow tagged by votes questions in https://datascience.stackexchange.com/

https://datascience.stackexchange.com/questions/tagged/tensorflow?sort=votes&pageSize=15

-->
</html>
