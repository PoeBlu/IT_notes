<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>DeepLearning_theory_map(alpha) <!-- ignore --></title>
<a href="https://ml-cheatsheet.readthedocs.io">https://ml-cheatsheet.readthedocs.io</a>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body>
<br/>
 Mathematical Foundations
<table>
<tr>
<td>
 Linear Algebra
<pre xxxsmall zoom>
</pre>
</td>

<td>
 Lagrange Optimization
<pre xxxsmall zoom>
</pre>
</td>

<td>
 Probability Theory
<pre xxxsmall zoom>
</pre>
</td>

<td>
 Gaussian Derivatives and Integrals
<pre xxxsmall zoom>
</pre>
</td>

<td>
 Hypothesis Testing
<pre xxxsmall zoom>
</pre>
</td>

<td>
 Information Theory
<pre xxxsmall zoom>
</pre>
</td>

<td>
 Computational Complexity
<pre xxxsmall zoom>
</pre>
</td>

</tr>
</table>
<br/> <br/>

<table>
<tr>
<td>
  DECISION THEORY
<pre xxxsmall zoom bgorange>
There is an overall single cost associated with our decision,
and our true task is to make a decision rule (i.e., set a decision boundary)
so as to minimize such a cost.

This is the central task of decision theory of which pattern
classification is (perhaps) the most important subfiled.

Classification is, at base, the task of recovering the model that generated
the patterns.

Different classification techniques are useful depending on the
type of candidate models themselfs.

 Statistical pattern recognition models focus on the statistical
   properties of the patterns. When not enought (training) data is available,
 knowledge of the problem domain is also used.

 Other models focus on the logical rules like syntactic pattern recognition,
  where rules or grammars describe our decision. 

 Patter classification differs from classical statistical *hypothesis testing*,
wherein the sensed data are used to decide whether or not to reject a *null hypothesis*
in favor of some alternative hypothesis. In this case, if the null-Hypothesis falls
below a "significance" threshold, we reject the null-hypothesis in favor of the
alternative.


 Because perfect classification performance is often impossible, a more general
task is to determine the probability for each of the possible categories.

sensing -> segmentation -> feature extraction -> classification -> post-processing
           ^^^^^^^^^^^^                                            (costs)        
           "one of the deepest                                                    
           problems in pattern
           recognition"

The design of a pattern recognition system usually entails the repetition of a
number of different activities: 
 - data collection: "large cost" of the development.
 - feature choice : Choice of distinguishing features.
 - model   choice : How to know whether a given model performs better
 - training       : Process of using data to determine the classifier
 - evaluation 

</pre>

  Vocabulary
<pre xxxsmall zoom>

Feature:  Or Property is the data output from a "feature extractor" process
         Values are passed to a classfier.

Model: Determines the set of features used for the classifier.
      It can be a single specific set of features.

Feature-vector: A given vector (ordered set of features). The "dimensions" of
    the vector are provided by the model.

Feature-space: N-dimensional space of  feature inputs.
      Classification can be seen as the split of the feature-space in
      regions where the decision-cost is minimized.

Classifier: Takes feature values inputs and evaluates the "evidence" to make
            a decision or classification, assigning the input feature-vector to
            an output classification.
          The central aim of designing a classifier is to suggest actions when
          presented with not-yet-seen patterns. This is the issue of generalization.

Segmentation: Part of the pre-processing where objects of interest are "extracted"
            from background.

Feature Extraction: Process that takes-in a pattern and produces feature values.
    Number of features is virtually always chosen to be fewer than the total 
    necessary to describe the complete taret of interest, and this leads to a loss
    in information. 

     In acts of associate-memory, the ssytem takes-in a pattern and emits another 
    pattern which is representative of a general group of patterns. It thus reduces
    the information somewhat, but rarely to the extent that pattern classification
    does. In short, because of the crucial role of a decision in pattern recognition
    information, it is fundamentally an information reduction process.

    The conceptual boundary between feature-extraction and classification is arbitrary.


Subset and SUperset problem: Formally part of *mereology*, the study of part/whole
    relationships. It appears as though the best classifiers try to incorporate
    as much of the input into the categorization as "makes sense" but not too much.

Risk: Total spected cost  of making a wrong classification/Decision.

stochastic: The property of having a random probability distribution or pattern
      that may be analysed statistically but may not be predicted precisely.
</pre>
 
  Regression , Interpolation , Density Estimation
<pre xxxsmall zoom>
Regression: We seek to find some functional description of data, often with the
    goal of predicting values for new input. 
    Linear regression - in which the function is linear in the input variables-
    is the most popular and well studied form or regression.
Interpolation: In this case we known or can easily deduce, the function for 
    certain ranges of input; the problem is then to infer the function for intermediate
    ranges of input. 

Density estimation: is the problem of estimating the density (or probability) that
    a memeber of a certain category will be found to have particular features.
</pre>

   Learning 
<pre xxxsmall zoom>
Learning: "Any method" that incorporates information from training samples in the
     design of a classifier.
     Formally, it refers to some form of algorithm for reducing the error on a 
     set of training data. 

Supervised-learning:  A teacher provides a category label or cost for each pattern
    in a training set, and seeks to reduce the sum of the cost for thes patterns.

UnSupervised-learning (Clustering): There is no explicit teacher, and the system
    forms clusters or "natural groupings" of the input patterns.
    "Natural" is always defined explicitly or implicitly in the clustering system
    itself.
    Different clusting algorithms lead to different Clusters.

Reinformcement Learning:
    The most typical way to train a classifer is to present an input, 
     compute its tentative category label, and use th known target categorylabel to
    inprove
    might be an images of a character, the actual output of the classifier the 
    category label "R" and the desired otuput a "B". In the reinforcement learning
    or learning-with-a-critic, no desired category signal is given; instead, the
    only teaching feedback is that the tentative category is right or wrong.

gradient-descent: A range of *gradient-descent* algorithms that alter a 
     classifier's parameters in order to reduce an error measure now permeate the
     field of statistical pattern recognition.

</pre>
<br/><br/>

  Who-is-Who
<pre xxxsmall zoom>
  - <a href="https://en.wikipedia.org/wiki/Richard_O._Duda">Richard O. Duda</a>: Author of "Pattern Classification" Book
    <a href="https://dl.acm.org/author_page.cfm?id=81332496778">ACM Digital Library Refs</a>

  - <a href="https://en.wikipedia.org/wiki/Peter_E._Hart"  >Peter E. Hart</a>  : Author of "Pattern Classification" Book
    <a href="https://dl.acm.org/author_page.cfm?id=81100122968">ACM Digital Library Refs</a>

  - David G. Stork : Author of "Pattern Classification" Book
    <a href="https://dl.acm.org/author_page.cfm?id=81100152072">ACM Digital Library refs</a>
</pre>
</td>

<td>
  Bayesina Decision Theory
<pre xxxsmall zoom>
- Ideal case in which the probability structure underlying the categories is known perfectly.

  <b>While not very realistic, it permits us to determine the optimal (Bayes) classifier
  against which we can compare all other classifiers.</b>

</pre>
</td>
<td>
  Maxium-likelihood and Bayesian Parameter Estimation
<pre xxxsmall zoom>
- We address the case when the full probability structure underlying the
  categories is not known, but the general forms of their distributions are.
  Thus the uncertainty about a probability distribuition is represented by 
  the values of someh unkown parameter, and we seek to deteermine these parameters
  to attain the best categorization.
</pre>
</td>
<td>
  NonParametric Techniches
<pre xxxsmall zoom>
- We have no prior parameterized knowledge about the underlying probability
  structure;
  Our classification will be based on information provided by training samples alone.
</pre>
</td>
<td>
  Linear Discriminant Functions
<pre xxxsmall zoom>
- General approach of parameter estimation. We shall assume that the so-called
  "discriminat functions" are of a very particular form - namely linear-
  in order to derive a class of incremental training rules.
</pre>
</td>
</tr>
</table>
<table>
<tr>
<td>
  Multilayer Neural Networks
<pre xxxsmall zoom>
- We extend the "linear discrimant functions" to a class of very powerful
  algorithms for training multilayer neural networks;
</pre>
</td>

<td>
  Stochastic Methods
<pre xxxsmall zoom>
- We discuss simulate annealing, the Boltmann learning algorithm and other
  sotchastic methods which can avoid some the estimation problems that plague
  other neural methods.
</pre>
</td>

<td>
  Nonmetric Methods
<pre xxxsmall zoom>
- We move from statistical to logical rules.
  - tree-based algorithms (CARTS)
  - syntactic-based methods based on grammars.
</pre>
</td>

<td>
  Algorithm-independent Machine Learning
<pre xxxsmall zoom>
- <b>The "most" important and difficult chapter of the book</b>.
- Some of the results described here - those related to bias and variance,
 degress of freedom, the desire for "simple" classifers, and computatinal
 complexity- are subtle and crucial both theorically and practically.
  The rest of the concepts can ONLY be fully understood (or used) in light
 of the results presented here.
</pre>
</td>

<td>
  Unsupervised Learnind and Clustering
<pre xxxsmall zoom>
- Address the case when input training patterns are not labeled, and
  where our recognizer must determine the cluster structure. We also treat
  a related problem, that of learning with a critic, in which the teacher 
  provides only a single bit of information during the presentation of a
  training pattern - "yes", to indicate that the classification provided
  is correct or "not" otherwise.
</pre>
</td>

</tr>
</table>

</body>

<!--
REF: https://media.consensys.net/using-machine-learning-to-understand-the-ethereum-blockchain-1778485d603a
Using Machine Learning to Understand the Ethereum Blockchain

A hotbed field of study in data science analysis at the moment is machine learning, a form of AI that uses algorithms to study large sets of data. It’s used for everything from sequencing DNA to studying financial markets and brain-machine interfaces. There are many different kinds of machine learning, with differing data requirements and objectives. In the past year, ConsenSys has made a push to develop its analytics and data science capabilities with projects like Alethio, an analytics platform helps users visualize, interpret, and react to blockchain data in real time.

The immutable, public records and decentralized nature of blockchain networks provide an exciting sandbox for data scientists, offering whole new world of data to analyze and patterns to recognize. To begin understanding how we go about pulling meaning out of this seemingly chaotic data environment, we’ll begin by describing two main categories of machine learning that are being developed by data scientists at Consensys, and give a few examples of how each can be applied in practice.
Supervised vs. Unsupervised Learning

Unsupervised Learning involves finding patterns in a large data sets and using them to extract meaning. Unsupervised learning models are not predictive in nature — though they could play a role in a larger predictive modeling system. Rather, unsupervised learning seeks to reduce a large and complex dataset to simpler high-level patterns or themes. These themes can then be used as a reference to characterize individual data points and put them into a useful context.

Anomaly and novelty detection systems are examples of unsupervised learning models. By reducing a large dataset into a small number of common themes, one can learn what it means for a particular transaction or account point to be “normal.” By comparing any given transaction or account to this learned definition of normal, we can determine the extent to which they are anomalous compared to the global average (anomaly detection), or compared to a recent historical average (novelty detection). These anomaly detection systems can then be used to alert users whether anything unusual is happening on the whole blockchain, or within a particular subset of interesting accounts or transactions. Alethio currently offers an anomaly detection system for transactions, blocks, and accounts.

Other kinds of analysis offered by Alethio that could arguably be considered unsupervised learning including ranking algorithms, or influence analysis like page rank. While these are not commonly referred to as machine learning algorithms at all (rather, just algorithms), they do serve the same purpose of finding overall patterns in a dataset and using them to add context.

Supervised learning seeks to take a set of observations with known features, and uses them to estimate the corresponding value of some other variable (a response or label) for each observation. This could be broken down into two common categories: prediction and classification. Trying to use historical data to estimate the future value of variable (a response) is known as prediction. Trying to use existing data about an entity to determine whether that entity belongs to a certain category (assigning a “label”) is known as classification.

Generally speaking, the “knowns” on the blockchain consist of raw, protocol-level data that is available on-chain, such as transaction data. This raw data can be used to extract features for accounts, such as their total balance, average transaction frequency, average age of currency held, etc. Recent efforts by Alethio to augment protocol-level data with semantic lifting have expanded the set of “knowns” beyond the protocol layer to include application-level data, such as whether a contract is a token, and to which standard it complies. All of these known quantities can be used as the basis for features in a supervised learning model.

On the other hand, the unknown quantity (the label or response) is by definition not a piece of currently-available on-chain data; otherwise it would already be known and captured by our data pipelines. The unknown quantity might be the future value of some on-chain data, such as the balance of an account on some future date. More commonly, the unknown quantity is some value that is never available on-chain at all. If you are trying to predict whether the account belongs to some category, such as being a decentralized exchange, a DOS account, or a Ponzi scheme, you will need to look off-chain for this data.

The Importance of Datasets

This is where the data requirements for unsupervised learning on the blockchain become an important problem (read: opportunity!). In order to train and calibrate a supervised learning model, there must be some large initial set of data for which the value of the labels or responses is known. This calibrates the model so that the predicted and actual response are as close as possible. This means that when a new observation comes in where the response is unknown, the prediction will be close to the true value, assuming the new observation is being generated by a similar process that generated the original dataset. Once the training phase is complete and the model calibrated, it can then be applied to new observations where the response is unknown.

In the case of price prediction, this means having a large database of historical prices. In the case of classification of accounts, this means having an initial set of accounts that are already labeled as being a decentralized exchange, a DOS account, or a Ponzi.

In these classification examples, the labels in the dataset used for training are often only available through significant effort. One possibility would be to pull data from websites like coinmarketcap or etherscan, building ETLs to import interesting data from other blockchain businesses, or through the painstaking effort of trained research assistants who gather data about on-chain accounts by surfing the web and analyzing source code.

The realization of the importance of gathering external data about accounts (metadata) for the purposes of machine learning was the motivation for creating a new spoke at ConsenSys called Rakr. Through collaboration with Alethio and other spokes and services within the mesh, Rakr hopes to provide a platform for gathering and sharing this valuable metadata. While the implications of integrating blockchain metadata with raw on-chain data go far beyond machine learning, the applicability of this metadata for supervised machine learning will continue to be a primary use case for the Rakr platform. By combining Alethio’s powerful analytics platform with the valuable metadata provided by Rakr, the applications of data science at ConsenSys will be limited only by the imagination.
In Practice

The first example of a supervised learning model produced at ConsenSys was the Ponzi model developed by Alethio, which will be described in more detail during the sequel to this article. The development of this model lays the groundwork for many future analytics possibilities for Alethio. Alethio hopes to expand this model to a more general fraud model in the near term.

More generally, the feature extraction pipelines built during this model development effort can be reused to classify any account according to one of the labels in the Rakr database, including whether an account/contract is an exchange, an art DAO, a casino, a DOS-related account, and much more. As the set of interesting metadata provided by Rakr continues to grow, more new models will become possible. And as the analytics capabilities of Alethio grow and more useful features are created, these models will become more powerful and versatile.

Being able to know whether a given account is a fraud or related to a DOS attack is crucial for managing financial and network risk on the Ethereum network. If we want to productionize models that provide actionable insights about new accounts and very recent behavioral data, they must satisfy special requirements. For example, we must make sure that they are being updated in real time, and that the features being used for classification and prediction are reliable and complete at the time the model is run. This means that certain features that can be used for classification of “old” accounts, such as “whether a contract eventually self-destructed,” cannot be applied to accounts in real-time. Since the value of the feature may change in the future, it’s true value is not really known at the time the model is run.

Real-time machine learning models present unique challenges and opportunities that go beyond those of historical modeling techniques. With that said, the ability to classify accounts as frauds goes beyond real-time risk management; classification models can still be valuable even if they are applied “in the past”. Being able to accurately classify historical frauds is useful for research purposes, even if those accounts are no longer active. More generally, attaching tags to accounts on the blockchain allows users to define semantically interesting subsets of accounts on the blockchain (such as “ICOs” or “exchanges”), rendering the blockchain searchable based on criteria that humans care about.

Creating a database of empirical human knowledge about on-chain entities is already a valuable and challenging task, and a necessary foundation for many other products and services. But with over 30,000,000 Ethereum accounts and contracts to date and roughly 100,000 new accounts created every day, it is simply impossible for humans to tag the entire history of ethereum accounts, most of which have no useful information (such as contract source, a website, or any other identifying information) that could be used by humans to classify or tag them. This is why the machine learning models are crucial: because they are infinitely scalable, and can be used to classify accounts using only the raw data characterizing their on-chain behavior.

By augmenting human knowledge about the blockchain with powerful analytics and machine learning, we envision a blockchain where every account and entity is enriched with useful classifications and properties, whether empirical and created by humans, or predicted and created by statistical models. This will be a major step forward for the transparency and accessibility of knowledge on the blockchain, which are an essential aspects required for blockchain technology to flourish.

Keep an eye out for the next article by Paul Lintilhac, which will give an exposition of one of Alethio’s recent data science initiatives: the Ponzi Model.
___________________________

Unsupervised: 

Supervised:
___________
https://en.wikipedia.org/wiki/Kalman_filter
____________
https://github.com/wireservice/agate

agate is a Python data analysis library that is optimized for humans instead of machines. It is an alternative to numpy and pandas that solves real-world problems with readable code.

_________________
https://datanalytics.com/libro_r/
_________________
http://www.elmomentodecisivo.com/
________________
https://www.datanalytics.com/
_________________
https://www.infoq.com/presentations/algorithms-counting-reddit?utm_source=notification_email&utm_campaign=notifications&utm_medium=link&utm_content=content_in_followed_topic&utm_term=daily
____________________
https://www.infoq.com/presentations/data-ml-pipelines-stitchfix?utm_source=notification_email&utm_campaign=notifications&utm_medium=link&utm_content=content_in_followed_topic&utm_term=daily
_______________
https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html
_______________
https://ai.googleblog.com/
_______________
https://ai.google/
___________________
REF: https://dzone.com/articles/consensus-clustering-via-apache-spark
    In this article, we will discuss a technique called Consensus Clustering to assess the stability
    of clusters generated by a clustering algorithm with respect to small perturbations in the data
    set. We will review a sample application built using the Apache Spark machine learning library 
    to show how consensus clustering can be used with K-means, Bisecting K-means, and Gaussian 
    Mixture, three distinct clustering algorithms

___________________
Ontology (Aristoteles) : http://classics.mit.edu/Aristotle/categories.1.1.html
_________________________________________
https://www.serverwatch.com/server-news/nvidia-accelerates-server-workloads-with-rapids-gpu-advances.html
!!!!  
_____________________
https://www.linux.com/blog/holberton/2018/10/sourced-engine-simple-elegant-way-analyze-your-code

source{d} Engine: A Simple, Elegant Way to Analyze your Code
With the recent advances in machine learning technology, it is only a matter of time before developers can expect to run full diagnostics and information retrieval on their own source code. This can include autocompletion, auto-generated user tests, more robust linters, automated code reviews and more. I recently reviewed a new product in this sphere -- the source{d} Engine.
source{d} offers a suite of applications that uses machine learning on code to complete source code analysis and assisted code reviews. Chief among them is the source{d} Engine, now in public beta; it uses a suite of open source tools (such as Gitbase, Babelfish, and Enry) to enable large-scale source code analysis. Some key uses of the source{d} Engine include language identification, parsing code into abstract syntax trees, and performing SQL Queries on your source code such as:

    What are the top repositories in a codebase based on number of commits?

    What is the most recent commit message in a given repository?

    Who are the most prolific contributors in a repository
___________________________
https://www.eleconomista.es/empresas-finanzas/noticias/9449298/10/18/COMUNICADO-Huawei-lanza-una-plataforma-de-desarrollo-de-IA-con-ciclo-de-vida-completo-mas-rapida.html
"ModelArts es una plataforma de desarrollo de inteligencia artificial más rápida e inclusiva que cualquier otra plataforma de desarrollo de IA del mercado", dijo Zheng Yelai, vicepresidente de Huawei y presidente de la unidad de negocio Huawei Cloud. "Creemos que los desarrolladores de IA sabrán apreciar lo rápido que se inicia, completa entrenamientos e implanta modelos".

El etiquetado y la preparación de datos es un proceso largo en el desarrollo de la inteligencia artificial, y representa casi el 50% del tiempo necesario. ModelArts tiene un marco de gobernanza de datos integrado para el etiquetado y la preparación de datos durante el desarrollo de IA. El marco implementa un entrenamiento iterativo para reducir el volumen de datos que tienen que ser etiquetados manualmente, lo que aumenta por 100 la eficiencia del etiquetado y la preparación de datos.

Además, ModelArts integra diversas tecnologías de optimización, especialmente el sistema de paralelo híbrido con cascada para reducir a la mitad el entrenamiento requerido en un determinado modelo, conjunto de datos o conjunto de recursos de hardware.

La implantación de modelos de IA es un proceso complejo. Con ModelArts, los modelos de entrenamiento pueden moverse a dispositivos, la periferia y la nube con solo un clic. Los trabajos de inferencia en línea o por lotes se proporcionan a través de la nube para cumplir con los diferentes requisitos de las aplicaciones, como la implantación simultánea o distribuida.

ModelArts también incorpora varias tecnologías de IA, como el aprendizaje automático, el diseño de modelos y la configuración de parámetros para acelerar el desarrollo de la inteligencia artificial.

En términos de gestión del ciclo de vida del desarrollo de IA, ModelArts abarca la recogida de datos sin procesar, el etiquetado de datos, la creación de trabajos de entrenamiento, la selección de algoritmos, la creación de modelos y la creación de servicios de inferencia. ModelArts permite a los desarrolladores de IA compartir datos, modelos y API de inteligencia artificial.
Visión de IA

Por otra parte, HiLens consta de una plataforma de desarrollo de aplicaciones de visión con IA y de un dispositivo visual potenciado con capacidades de IA. HiLens cuenta con Skill, un nuevo concepto de desarrollo de IA. Skill consiste en un código de control y modelos entrenados en ModelArts. HiLens también es compatible con modelos entrenados en otros marcos convencionales. Las capacidades desarrolladas en HiLens pueden implantarse en cualquier dispositivo que tenga integrados los chips Ascend de IA.

El dispositivo visual HiLens se compone de una cámara inteligente compatible con inferencias. Los desarrolladores pueden usar el dispositivo HiLens para crear aplicaciones de visión e implantarlas en dispositivos y en la nube. El dispositivo visual HiLens integra el chip Ascend 310, que puede procesar 100 fotogramas por segundo y detectar caras en milisegundos. Además, los livianos contenedores integrados minimizan el uso de recursos y de ancho de banda de red, y pueden descargarse e iniciarse de forma rápida
_________________________________________
https://www.infoq.com/news/2018/11/PyTorch-Developer-Preview
_________________________________________
IA Classification:
  • Cognitive Processing (including Natural Language Processing, Computer Vision, Speech Recognition)
  • Conversasional Systems and Virtual Assistants (Question&Answering, ChatBots)
  • Machine Learning & Deep Learning
  • Reference frameworks (as IBM Watson, Microsoft Cognitive, Cognitive Services in AWS, and Google and others)
_____________________________
https://www.infoq.com/news/2018/11/Google-AI-Voice
_____________________________
Bibliography:

-->

</html>
