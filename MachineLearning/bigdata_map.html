<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>BigData map <!-- ignore --></title>
<head>
<script src="../map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="../map_v1.css" />
</head>

<body>

<table>
<tr>
<td>
  External Links:
  <ul xxxsmall zoom>
  <li><a href="http...."></a></li>
  <li><a href="http...."></a></li>
  </ul>
</td>
</tr>
</table>

</body>
<!--
TODO_START: {
_________________
Big Data Companies:
                  | OFFICES   | START | DESCRIPTION
__________________+___________+_______+____________________________________________________________________________
Synergic Partners | Madrid    | 2007  | - owned by Telefónica (2015) 
                  | Barcelona |       | - proyectos de arquitectura de datos e inteligencia aplicada en campos como
                  | Zúrich    |       |   - seguridad ciudadana (prediciendo patrones de crimen en ciudades como Nueva York)
                  |           |       |   - banca (definiendo la estrategia tecnológica global para EVO Banco)
                  |           |       |   - aviación comercial (trabajando con Iberia Express para conseguir una visión unificada de sus clientes)
__________________+___________+_______+_____________________________________________________________________________
CARTO (CartoDB)   | Madrid    | 2011  | - sistemas de información geográfica y mapeo web.
                  |           |       | - software, de código abierto, permite crear mapas y
                  |           |       |   representaciones visuales de un sinfín de materias,
                  |           |       |   pudiendo interrelacionar variables de negocio con la 
                  |           |       |   localización geográfica de activos o recursos.
                  |           |       |   Ej: Visualizar dónde/cómo están trabajando las máquinas
                  |           |       |       quitanieves durante una tormenta, obtener una representación
                  |           |       |       sencilla de entender de todas las transacciones financieras 
                  |           |       |       que se producen en un determinado evento u obtener un mapa
                  |           |       |       con las mejores zonas para comprar o alquilar un piso en 
                  |           |       |       función de un sinfín de parámetros y fuentes de datos distintas.
__________________+___________+_______+______________________________________________________________________________
Imanth Research   | Zaragoza  | 2012  | - ha creado una aplicación matemática que permite realizar
                  |           |       |   de forma sencilla todo tipo de computación, cálculos  y simulaciones utilizando
                  |           |       |   grandes cantidades de datos sobre un servicio cloud.
                  |           |       | - La firma promete, de este modo, encontrar correlaciones y patrones, realizar
                  |           |       |   simulaciones y desarrollar modelos predictivos que ayuden a mejorar las decisiones
                  |           |       |   estratégicas de las pymes que, no en vano, constituyen el 99,9%
                  |           |       |   del tejido empresarial de nuestro país.
__________________+___________+_______+_______________________________________________________________________________
QMENTA            | Barcelona | 2013  | Focalizada en llevar el Big Data a la salud.
(antigua MintLabs)| Boston    |       | - Se dedica al almacenamiento, procesamiento y visualización
                  |           |       |   de imágenes médicas (principalmente en el análisis de datos
                  |           |       |   cerebrales mediante MRI y datos clínicos relacionados).
                  |           |       | - Procesamiento de imágenes para acelerar el desarrollo de nuevas 
                  |           |       |   terapias para enfermedades neurológicas. 
__________________+___________+_______+________________________________________________________________________________
Stratio           | Madrid    | 2014  | Consolidación y gestión de grandes volúmenes de datos en la era "post-Hadoop"
                  | EEUU      |       | 
                  | Bogotá    |       | - Su tecnología permite crear aplicaciones y desarrollar plataformas
                  | São Paulo |       |   que procesen información de muy distinto tipo en tiempo real o por lotes, 
                  |           |       |   con importantes casos de uso en industrias como la banca, telecomunicaciones, retail...
__________________+___________+_______+_________________________________________________________________________________
Treelogic         | Asturias  |  1996 |  Cubre segmentos tan dispersos del Big Data,  
                  |           |       | monitorización de personas en misiones humanitarias, 
                  |           |       | análisis de los patrones que llevan al sedentarismo, 
                  |           |       | historia clínica electrónica, interrelación de medios de
                  |           |       |  transporte o mejorar la capacidad de entender
                  |           |       | y procesar lenguaje natural de las máquinas.
__________________+___________+_______+__________________________________________________________________________________
Geoblink          | Madrid    |  2015 |  Big Data + geolocalización: modelos estadísticos a partir de 
                  |           |       | múltiples fuentes para entender qué localizaciones son mejores para 
                  |           |       | construir una casa o abrir una nueva tienda, colocar una determinada
                  |           |       | publicidad o rotar un producto o promoción en función del tipo 
                  |           |       | de consumidor, renta o hábitos de vida en la zona.
__________________+___________+_______+___________________________________________________________________________________
Bitext            | Madrid    |  2007 | motores de análisis de texto multilingüe más preciso del mercado
                  |           |       | 50 lenguajes de África, Asia, Europa y Oriente Medio.
                  |           |       | Usado en chatbots conversacionales ... 
                  |           |       | Puede usarse también como servicio de categorización de texto,
                  |           |       | basado en una codificación automatizada que se puede usar para analizar 
                  |           |       | rápidamente preguntas abiertas en encuestas y escribir informes teniendo
                  |           |       | en cuenta solo la información relevante a partir de un sinfín de 
                  |           |       | datos en bruto.
__________________+___________+_______+____________________________________________________________________________________
Datary            | Madrid    | 2014  | Desarrollo de marketplace de datos en el que poder
                  |           |       | consolidar información de múltiples fuentes públicas 
                  |           |       | (INE, CIS, Eurostat, CNMC, OCDE, portal de transparencia del gobierno, ...) 
                  |           |       | en forma de API abierta.
__________________+___________+_______+_____________________________________________________________________________________


<td>
  Hadoop:
  Hadoop "vs" Spark
  <a href="https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html">REF</a>
<pre xxxsmall zoom>
Hadoop is essentially a distributed data infrastructure: 
 -It distributes massive data collections across multiple nodes 
  within a cluster of commodity servers
 -It also indexes and keeps track of that data, enabling
  big-data processing and analytics far more effectively
  than was possible previously. 
Spark, on the other hand, is a data-processing tool that operates on those 
distributed data collections; it doesn't do distributed storage.

You can use one without the other: 
  - Hadoop includes not just a storage component, known as the 
  Hadoop Distributed File System, but also a processing component called 
  MapReduce, so you don't need Spark to get your processing done.
  - Conversely, you can also use Spark without Hadoop. Spark does not come with
  its own file management system, though, so it needs to be integrated with one
  - if not HDFS, then another cloud-based data platform. Spark was designed for
  Hadoop, however, so many agree they're better together.

Spark is generally a lot faster than MapReduce because of the way it processes 
data. While MapReduce operates in steps, Spark operates on the whole data set 
in one fell swoop:
   "The MapReduce workflow looks like this: read data from the cluster, perform
    an operation, write results to the cluster, read updated data from the 
    cluster, perform next operation, write next results to the cluster, etc.," 
    explained Kirk Borne, principal data scientist at Booz Allen Hamilton. 
    Spark, on the other hand, completes the full data analytics operations 
    in-memory and in near real-time: 
    "Read data from the cluster, perform all of the requisite analytic 
    operations, write results to the cluster, done," Borne said.
Spark can be as much as 10 times faster than MapReduce for batch processing and 
p to 100 times faster for in-memory analytics, he said.
  You may not need Spark's speed. MapReduce's processing style can be just fine 
if your data operations and reporting requirements are mostly static and you 
can wait for batch-mode processing. But if you need to do analytics on 
streaming data, like from sensors on a factory floor, or have applications that
require multiple operations, you probably want to go with Spark.
 Most machine-learning algorithms, for example, require multiple operations. 

Recovery: different, but still good. 
Hadoop is naturally resilient to system faults or failures since data 
are written to disk after every operation, but Spark has similar built-in
resiliency by virtue of the fact that its data objects are stored in something 
called resilient distributed datasets distributed across the data cluster. 
"These data objects can be stored in memory or on disks, and RDD provides full 
recovery from faults or failures," Borne pointed out.
</pre>
</td>

<td>
  <a href="http://spark.apache.org/">Apache Spark</a><br/>
  Cluster computing platform
  <ul xxxsmall zoom>
  <li>
  </li>
  </ul>
<pre xxxsmall zoom { >
  general framework for large-scale data processing that supports lots of 
different programming languages and concepts such as MapReduce, in-memory
processing, stream processing, graph processing, and Machine Learning. This can
also be used on top of Hadoop. Data can be ingested from many sources like 
Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex 
algorithms expressed with high-level functions like map, reduce, join and window.

Common applications for Spark include real-time marketing campaigns, online 
product recommendations, cybersecurity analytics and machine log monitoring.
</pre } >
<br/>
Kafka vs Spark Streaming: <a href="https://dzone.com/articles/spark-streaming-vs-kafka-stream-1">REF</a>
<p xxxsmall zoom { >
""" If event time is very relevant and latencies in the seconds range are
completely unacceptable, Kafka should be your first choice. Otherwise, 
Spark works just fine. <br/>
...
Apache Spark can be used with Kafka to stream the data, but if you are 
deploying a Spark cluster for the sole purpose of this new application, that is
definitely a big complexity hit.<br/>
...
Conclusion
I believe that Kafka Streams is still best used in a "Kafka > Kafka" context, 
while Spark Streaming could be used for a "Kafka > Database" or 
"Kafka > Data science model" type of context.
</p } >
</td>

______________________
Big Data related technologies: Spark, Flink, Kafka, Phoenix, Kylin, Presto, Drill, Tensor Flow, SparkR, Sparklyr, H2O, etc., etc., etc
__________________________________
Tools BigData:

                   | LOCAL                    | GOOGLE                  | AWS               | AZURE
---------------------------------------------------------------------------------------------------
Data Cleaning      | Trifacta Wrangler        | DataPrep                | Glue              | MLStudio
---------------------------------------------------------------------------------------------------
SandBox/Notebook   | Jupiter,...              | DataLab                 | JupiterHub        | MLSutdio
---------------------------------------------------------------------------------------------------
Hadoop grid        | Cloudera/HortonWorks,... | DataProc                | EMR               | HDinsights
---------------------------------------------------------------------------------------------------
Ingest             | Kafka                    | Pub/Sub                 | Kinesys           | Event Hub
---------------------------------------------------------------------------------------------------
Stream Processing  | Apache Flume/ETL         | DataFlow                | Data Pipeline     | Data Factory
---------------------------------------------------------------------------------------------------
Neural Networks    | TensorFlow               | Mach.Learning Eng+APIs  | Tensorflow on AWS | MLStudio
---------------------------------------------------------------------------------------------------
DwH                | Teradata Sybase IQ       | BigQuery                | Redshift          | AzureDB DWH
---------------------------------------------------------------------------------------------------

Others             | MongoDB, Cassandra,...   | BigQuery, Bigtable      | Redshift          | CosmosDB

}
_____________________________
Safran Data Architect H/F:

- Mise en place de solution : Hadoop, HDFS, Yarn, solutions in-memory.
- Echanges et traitement des données : EDI, API, ETL, MapReduce, Hive, Pig, Spark, Storm, NoSQL type MongoDB, SQL.
- Algorithmique : évaluation de la complexité, structure de données, parcours de graphe, calculs parallèles.
- Outils de visualisation des données.
- Technologies EDI (ETL, protocoles d'échanges standards)
__________________________
https://en.wikipedia.org/wiki/Stan_(software)
__________________________
https://code.fb.com/core-data/using-apache-spark-for-large-scale-language-model-training/
__________________________
https://opensource.com/article/18/9/top-3-python-libraries-data-science
__________________________
https://www.youtube.com/watch?v=6puwaUHNRIU
-->

</html>
