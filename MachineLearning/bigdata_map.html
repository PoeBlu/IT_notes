<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>BigData map <!-- ignore --></title>
<head>
<script src="../map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="../map_v1.css" />
</head>

<body>

<pre zoom labels="datascience" bgorange>
<span xsmall bgorange>Orange</span>
@[https://orange.biolab.si/screenshots/]
Features:
- Interactive Data Visualization
- Visual Programming
- Student's friendly.
- Add-ons
- Python Anaconda Friendly
  $ conda config --add channels conda-forge
  $ conda install orange3
  $ conda install -c defaults pyqt=5 qt
- Python pip  Friendly
  $ pip install orange3
</pre>
</body>
</html>
<!--
TODO_START: {
_________________
Kafka vs Spark Streaming: <a href="https://dzone.com/articles/spark-streaming-vs-kafka-stream-1">REF</a>
""" If event time is very relevant and latencies in the seconds range are
completely unacceptable, Kafka should be your first choice. Otherwise, 
Spark works just fine. 
...
Apache Spark can be used with Kafka to stream the data, but if you are 
deploying a Spark cluster for the sole purpose of this new application, that is
definitely a big complexity hit.
...
Conclusion
I believe that Kafka Streams is still best used in a "Kafka > Kafka" context, 
while Spark Streaming could be used for a "Kafka > Database" or 
"Kafka > Data science model" type of context.

______________________
Big Data related technologies: Spark, Flink, Kafka, Phoenix, Kylin, Presto, Drill, Tensor Flow, SparkR, Sparklyr, H2O, etc., etc., etc
__________________________________
Tools BigData:
                   | LOCAL                    | GOOGLE                  | AWS               | AZURE
---------------------------------------------------------------------------------------------------
Data Cleaning      | Trifacta Wrangler        | DataPrep                | Glue              | MLStudio
---------------------------------------------------------------------------------------------------
SandBox/Notebook   | Jupiter,...              | DataLab                 | JupiterHub        | MLSutdio
---------------------------------------------------------------------------------------------------
Hadoop grid        | Cloudera/HortonWorks,... | DataProc                | EMR               | HDinsights
---------------------------------------------------------------------------------------------------
Ingest             | Kafka                    | Pub/Sub                 | Kinesys           | Event Hub
---------------------------------------------------------------------------------------------------
Stream Processing  | Apache Flume/ETL         | DataFlow                | Data Pipeline     | Data Factory
---------------------------------------------------------------------------------------------------
Neural Networks    | TensorFlow               | Mach.Learning Eng+APIs  | Tensorflow on AWS | MLStudio
---------------------------------------------------------------------------------------------------
DwH                | Teradata Sybase IQ       | BigQuery                | Redshift          | AzureDB DWH
---------------------------------------------------------------------------------------------------

Others             | MongoDB, Cassandra,...   | BigQuery, Bigtable      | Redshift          | CosmosDB

}
_____________________________
Safran Data Architect H/F:

- Mise en place de solution : Hadoop, HDFS, Yarn, solutions in-memory.
- Echanges et traitement des données : EDI, API, ETL, MapReduce, Hive, Pig, Spark, Storm, NoSQL type MongoDB, SQL.
- Algorithmique : évaluation de la complexité, structure de données, parcours de graphe, calculs parallèles.
- Outils de visualisation des données.
- Technologies EDI (ETL, protocoles d'échanges standards)
__________________________
https://en.wikipedia.org/wiki/Stan_(software)
__________________________
https://code.fb.com/core-data/using-apache-spark-for-large-scale-language-model-training/
__________________________
https://opensource.com/article/18/9/top-3-python-libraries-data-science
__________________________
https://www.youtube.com/watch?v=6puwaUHNRIU
_____________________
https://www.systutorials.com/3202/colossus-successor-to-google-file-system-gfs/
______________________
Apache Beam
Apache Beam provides an advanced unified programming model, allowing you to implement batch and streaming data processing jobs that can run on any execution engine.
Allows to execute pipelines on multiple environments such as Apache Apex, Apache Flink, Apache Spark among others.
__________________________
Apache Ignite
Apache Ignite is a high-performance, integrated and distributed in-memory platform for computing and transacting on large-scale data sets in real-time, orders of magnitude faster than possible with traditional disk-based or flash technologies.
___________________________
Apache Hive 2.0

The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL.  

Since version 2.0 includes many new features about, performance (Hive on Spark), Security (HiveServer2 web UI), Stability (solved issues), and other new functionalities.

The big leap of this version is the use of Apache Spark as execution engine."
________________________
At first an overview about PrestoDB. PrestoDB is a distributed SQL query engine originally developed by Facebook. It allows to combine data from multiple sources (RDBMS, No-SQL, Hadoop) within a single query, and it has little to no performance degradation running. Being used and developed by big data giants like, among others,  Facebook, Twitter and Netflix, guarantees a bright future for this tool.
_________________
Also a report has been developed about StreamSets. StreamSets is a project that aims to simplify pipeline creation and dataflow visualization.  His configuration-oriented gui, makes it very easy to setup a new data pipeline in no-time, and it even has a cluster mode to run on top of Spark. Another characteristic is the ability to change data routes in hot mode, and the capability to run custom Python and Spark scripts to process the data.
____________________
At last, a benchmark about different Graph Visualization tools such as Gephi, Cytoscape, D3.js and Linkurious.
The graph visualization tools usually offer ways of representing graph structure and properties, interactive ways to manipulate those and reporting mechanisms to extract and visualize value from the information contained in them. This benchmark aims to offer an overview of all the available tools to visualize graphs, and to compare the most significant ones in terms of features and capabilities."
________________
https://www.zdnet.com/article/pictionary-provides-ai-with-common-sense/?ftag=TRE-03-10aaa6b&bhid=28374205867001011904732094012637
-->
