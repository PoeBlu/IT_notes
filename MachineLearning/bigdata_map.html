<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>BigData map <!-- ignore --></title>
<head>
<script src="../map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="../map_v1.css" />
</head>

<body>

<pre zoom labels="datascience" bgorange>
<span xsmall bgorange>Orange</span>
@[https://orange.biolab.si/screenshots/]
Features:
- Interactive Data Visualization
- Visual Programming
- Student's friendly.
- Add-ons
- Python Anaconda Friendly
  $ conda config --add channels conda-forge
  $ conda install orange3
  $ conda install -c defaults pyqt=5 qt
- Python pip  Friendly
  $ pip install orange3
</pre>
</body>
</html>
<!--
TODO_START: {
_________________
Big Data Companies:
                  | OFFICES   | START | DESCRIPTION
__________________+___________+_______+____________________________________________________________________________
Synergic Partners | Madrid    | 2007  | - owned by Telefónica (2015) 
                  | Barcelona |       | - proyectos de arquitectura de datos e inteligencia aplicada en campos como
                  | Zúrich    |       |   - seguridad ciudadana (prediciendo patrones de crimen en ciudades como Nueva York)
                  |           |       |   - banca (definiendo la estrategia tecnológica global para EVO Banco)
                  |           |       |   - aviación comercial (trabajando con Iberia Express para conseguir una visión unificada de sus clientes)
__________________+___________+_______+_____________________________________________________________________________
CARTO (CartoDB)   | Madrid    | 2011  | - sistemas de información geográfica y mapeo web.
                  |           |       | - software, de código abierto, permite crear mapas y
                  |           |       |   representaciones visuales de un sinfín de materias,
                  |           |       |   pudiendo interrelacionar variables de negocio con la 
                  |           |       |   localización geográfica de activos o recursos.
                  |           |       |   Ej: Visualizar dónde/cómo están trabajando las máquinas
                  |           |       |       quitanieves durante una tormenta, obtener una representación
                  |           |       |       sencilla de entender de todas las transacciones financieras 
                  |           |       |       que se producen en un determinado evento u obtener un mapa
                  |           |       |       con las mejores zonas para comprar o alquilar un piso en 
                  |           |       |       función de un sinfín de parámetros y fuentes de datos distintas.
__________________+___________+_______+______________________________________________________________________________
Imanth Research   | Zaragoza  | 2012  | - ha creado una aplicación matemática que permite realizar
                  |           |       |   de forma sencilla todo tipo de computación, cálculos  y simulaciones utilizando
                  |           |       |   grandes cantidades de datos sobre un servicio cloud.
                  |           |       | - La firma promete, de este modo, encontrar correlaciones y patrones, realizar
                  |           |       |   simulaciones y desarrollar modelos predictivos que ayuden a mejorar las decisiones
                  |           |       |   estratégicas de las pymes que, no en vano, constituyen el 99,9%
                  |           |       |   del tejido empresarial de nuestro país.
__________________+___________+_______+_______________________________________________________________________________
QMENTA            | Barcelona | 2013  | Focalizada en llevar el Big Data a la salud.
(antigua MintLabs)| Boston    |       | - Se dedica al almacenamiento, procesamiento y visualización
                  |           |       |   de imágenes médicas (principalmente en el análisis de datos
                  |           |       |   cerebrales mediante MRI y datos clínicos relacionados).
                  |           |       | - Procesamiento de imágenes para acelerar el desarrollo de nuevas 
                  |           |       |   terapias para enfermedades neurológicas. 
__________________+___________+_______+________________________________________________________________________________
Stratio           | Madrid    | 2014  | Consolidación y gestión de grandes volúmenes de datos en la era "post-Hadoop"
                  | EEUU      |       | 
                  | Bogotá    |       | - Su tecnología permite crear aplicaciones y desarrollar plataformas
                  | São Paulo |       |   que procesen información de muy distinto tipo en tiempo real o por lotes, 
                  |           |       |   con importantes casos de uso en industrias como la banca, telecomunicaciones, retail...
__________________+___________+_______+_________________________________________________________________________________
Treelogic         | Asturias  |  1996 |  Cubre segmentos tan dispersos del Big Data,  
                  |           |       | monitorización de personas en misiones humanitarias, 
                  |           |       | análisis de los patrones que llevan al sedentarismo, 
                  |           |       | historia clínica electrónica, interrelación de medios de
                  |           |       |  transporte o mejorar la capacidad de entender
                  |           |       | y procesar lenguaje natural de las máquinas.
__________________+___________+_______+__________________________________________________________________________________
Geoblink          | Madrid    |  2015 |  Big Data + geolocalización: modelos estadísticos a partir de 
                  |           |       | múltiples fuentes para entender qué localizaciones son mejores para 
                  |           |       | construir una casa o abrir una nueva tienda, colocar una determinada
                  |           |       | publicidad o rotar un producto o promoción en función del tipo 
                  |           |       | de consumidor, renta o hábitos de vida en la zona.
__________________+___________+_______+___________________________________________________________________________________
Bitext            | Madrid    |  2007 | motores de análisis de texto multilingüe más preciso del mercado
                  |           |       | 50 lenguajes de África, Asia, Europa y Oriente Medio.
                  |           |       | Usado en chatbots conversacionales ... 
                  |           |       | Puede usarse también como servicio de categorización de texto,
                  |           |       | basado en una codificación automatizada que se puede usar para analizar 
                  |           |       | rápidamente preguntas abiertas en encuestas y escribir informes teniendo
                  |           |       | en cuenta solo la información relevante a partir de un sinfín de 
                  |           |       | datos en bruto.
__________________+___________+_______+____________________________________________________________________________________
Datary            | Madrid    | 2014  | Desarrollo de marketplace de datos en el que poder
                  |           |       | consolidar información de múltiples fuentes públicas 
                  |           |       | (INE, CIS, Eurostat, CNMC, OCDE, portal de transparencia del gobierno, ...) 
                  |           |       | en forma de API abierta.
__________________+___________+_______+_____________________________________________________________________________________


<td>
  Hadoop:
  Hadoop "vs" Spark
  <a href="https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html">REF</a>
<pre zoom>
Hadoop is essentially a distributed data infrastructure: 
 -It distributes massive data collections across multiple nodes 
  within a cluster of commodity servers
 -It also indexes and keeps track of that data, enabling
  big-data processing and analytics far more effectively
  than was possible previously. 
Spark, on the other hand, is a data-processing tool that operates on those 
distributed data collections; it doesn't do distributed storage.

You can use one without the other: 
  - Hadoop includes not just a storage component, known as the 
  Hadoop Distributed File System, but also a processing component called 
  MapReduce, so you don't need Spark to get your processing done.
  - Conversely, you can also use Spark without Hadoop. Spark does not come with
  its own file management system, though, so it needs to be integrated with one
  - if not HDFS, then another cloud-based data platform. Spark was designed for
  Hadoop, however, so many agree they're better together.

Spark is generally a lot faster than MapReduce because of the way it processes 
data. While MapReduce operates in steps, Spark operates on the whole data set 
in one fell swoop:
   "The MapReduce workflow looks like this: read data from the cluster, perform
    an operation, write results to the cluster, read updated data from the 
    cluster, perform next operation, write next results to the cluster, etc.," 
    explained Kirk Borne, principal data scientist at Booz Allen Hamilton. 
    Spark, on the other hand, completes the full data analytics operations 
    in-memory and in near real-time: 
    "Read data from the cluster, perform all of the requisite analytic 
    operations, write results to the cluster, done," Borne said.
Spark can be as much as 10 times faster than MapReduce for batch processing and 
p to 100 times faster for in-memory analytics, he said.
  You may not need Spark's speed. MapReduce's processing style can be just fine 
if your data operations and reporting requirements are mostly static and you 
can wait for batch-mode processing. But if you need to do analytics on 
streaming data, like from sensors on a factory floor, or have applications that
require multiple operations, you probably want to go with Spark.
 Most machine-learning algorithms, for example, require multiple operations. 

Recovery: different, but still good. 
Hadoop is naturally resilient to system faults or failures since data 
are written to disk after every operation, but Spark has similar built-in
resiliency by virtue of the fact that its data objects are stored in something 
called resilient distributed datasets distributed across the data cluster. 
"These data objects can be stored in memory or on disks, and RDD provides full 
recovery from faults or failures," Borne pointed out.
</pre>
</td>

<td>
  <a href="http://spark.apache.org/">Apache Spark</a><br/>
  Cluster computing platform
<pre zoom { >
  general framework for large-scale data processing that supports lots of 
different programming languages and concepts such as MapReduce, in-memory
processing, stream processing, graph processing, and Machine Learning. This can
also be used on top of Hadoop. Data can be ingested from many sources like 
Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex 
algorithms expressed with high-level functions like map, reduce, join and window.

Common applications for Spark include real-time marketing campaigns, online 
product recommendations, cybersecurity analytics and machine log monitoring.
</pre } >
<br/>
Kafka vs Spark Streaming: <a href="https://dzone.com/articles/spark-streaming-vs-kafka-stream-1">REF</a>
<p zoom { >
""" If event time is very relevant and latencies in the seconds range are
completely unacceptable, Kafka should be your first choice. Otherwise, 
Spark works just fine. <br/>
...
Apache Spark can be used with Kafka to stream the data, but if you are 
deploying a Spark cluster for the sole purpose of this new application, that is
definitely a big complexity hit.<br/>
...
Conclusion
I believe that Kafka Streams is still best used in a "Kafka > Kafka" context, 
while Spark Streaming could be used for a "Kafka > Database" or 
"Kafka > Data science model" type of context.
</p } >
</td>

______________________
Big Data related technologies: Spark, Flink, Kafka, Phoenix, Kylin, Presto, Drill, Tensor Flow, SparkR, Sparklyr, H2O, etc., etc., etc
__________________________________
Tools BigData:

                   | LOCAL                    | GOOGLE                  | AWS               | AZURE
---------------------------------------------------------------------------------------------------
Data Cleaning      | Trifacta Wrangler        | DataPrep                | Glue              | MLStudio
---------------------------------------------------------------------------------------------------
SandBox/Notebook   | Jupiter,...              | DataLab                 | JupiterHub        | MLSutdio
---------------------------------------------------------------------------------------------------
Hadoop grid        | Cloudera/HortonWorks,... | DataProc                | EMR               | HDinsights
---------------------------------------------------------------------------------------------------
Ingest             | Kafka                    | Pub/Sub                 | Kinesys           | Event Hub
---------------------------------------------------------------------------------------------------
Stream Processing  | Apache Flume/ETL         | DataFlow                | Data Pipeline     | Data Factory
---------------------------------------------------------------------------------------------------
Neural Networks    | TensorFlow               | Mach.Learning Eng+APIs  | Tensorflow on AWS | MLStudio
---------------------------------------------------------------------------------------------------
DwH                | Teradata Sybase IQ       | BigQuery                | Redshift          | AzureDB DWH
---------------------------------------------------------------------------------------------------

Others             | MongoDB, Cassandra,...   | BigQuery, Bigtable      | Redshift          | CosmosDB

}
_____________________________
Safran Data Architect H/F:

- Mise en place de solution : Hadoop, HDFS, Yarn, solutions in-memory.
- Echanges et traitement des données : EDI, API, ETL, MapReduce, Hive, Pig, Spark, Storm, NoSQL type MongoDB, SQL.
- Algorithmique : évaluation de la complexité, structure de données, parcours de graphe, calculs parallèles.
- Outils de visualisation des données.
- Technologies EDI (ETL, protocoles d'échanges standards)
__________________________
https://en.wikipedia.org/wiki/Stan_(software)
__________________________
https://code.fb.com/core-data/using-apache-spark-for-large-scale-language-model-training/
__________________________
https://opensource.com/article/18/9/top-3-python-libraries-data-science
__________________________
https://www.youtube.com/watch?v=6puwaUHNRIU
_____________________
https://www.systutorials.com/3202/colossus-successor-to-google-file-system-gfs/
______________________
Apache Beam
Apache Beam provides an advanced unified programming model, allowing you to implement batch and streaming data processing jobs that can run on any execution engine.
Allows to execute pipelines on multiple environments such as Apache Apex, Apache Flink, Apache Spark among others.
__________________________
Apache Ignite
Apache Ignite is a high-performance, integrated and distributed in-memory platform for computing and transacting on large-scale data sets in real-time, orders of magnitude faster than possible with traditional disk-based or flash technologies.
___________________________
Apache Hive 2.0

The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL.  

Since version 2.0 includes many new features about, performance (Hive on Spark), Security (HiveServer2 web UI), Stability (solved issues), and other new functionalities.

The big leap of this version is the use of Apache Spark as execution engine."
________________________
At first an overview about PrestoDB. PrestoDB is a distributed SQL query engine originally developed by Facebook. It allows to combine data from multiple sources (RDBMS, No-SQL, Hadoop) within a single query, and it has little to no performance degradation running. Being used and developed by big data giants like, among others,  Facebook, Twitter and Netflix, guarantees a bright future for this tool.
_________________
Also a report has been developed about StreamSets. StreamSets is a project that aims to simplify pipeline creation and dataflow visualization.  His configuration-oriented gui, makes it very easy to setup a new data pipeline in no-time, and it even has a cluster mode to run on top of Spark. Another characteristic is the ability to change data routes in hot mode, and the capability to run custom Python and Spark scripts to process the data.
____________________
At last, a benchmark about different Graph Visualization tools such as Gephi, Cytoscape, D3.js and Linkurious.
The graph visualization tools usually offer ways of representing graph structure and properties, interactive ways to manipulate those and reporting mechanisms to extract and visualize value from the information contained in them. This benchmark aims to offer an overview of all the available tools to visualize graphs, and to compare the most significant ones in terms of features and capabilities."
________________
https://www.zdnet.com/article/pictionary-provides-ai-with-common-sense/?ftag=TRE-03-10aaa6b&bhid=28374205867001011904732094012637
-->
