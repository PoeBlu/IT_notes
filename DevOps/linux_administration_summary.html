<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>Linux Administration Map (beta)</title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body>
<span xsmall>
<a href="https://linux.die.net/man/">[Man Pages]</a>,
<a href="https://linux.die.net/Linux-CLI/">[Command-line Tools  Summary]</a>,
<a href="https://en.wikipedia.org/wiki/Linux_Standard_Base">[Linux Standard Base]</a>,
<a href="http://www.tldp.org/LDP/Linux-Filesystem-Hierarchy/html/etc.html">[/etc config directory]</a>
(<a href="www.tldp.org/LDP/Linux-Filesystem-Hierarchy">Full FS Hierarchy</a>)</li>
</span>
<hr xxxsmall/>
<table style='width:100%'>
<tbody>
<tr>
<td colsep>User&nbsp;Mng</td>
<td>
  Create new user
<pre xxxsmall zoom>
$ useradd [options] LOGIN : creates new user with default+specified values

$ useradd -D  #  display default values
(ex.output)
  GROUP=100
  HOME=/home
  INACTIVE=-1
  EXPIRE=
  SHELL=/bin/bash
  SKEL=/etc/skel
  CREATE_MAIL_SPOOL=yes

$ useradd -D [options] # update default values
----- next opts have defaults if not indicated ----------
  --base-dir BASE_DIR  :  (default to /home) Ignored if --home-dir set
  --expiredate EXPIRE_DATE
  --inactive INACTIVE  :  day # after pass.expiration before disabling
  --gid GROUP: existing group name or ID for initial group (when --no-user-group used)
  --shell SHELL
--------------------------------------------------------
  --groups group1,group2,... supplementary groups
  --skel SKEL_DIR :  skel. dir. to be copied in the user's home directory
  --key KEY=VALUE : Overrides /etc/login.defs defaults 
                    (UID_MIN, UID_MAX, UMASK, PASS_MAX_DAYS and others).
                    Example: -K PASS_MAX_DAYS=-1 can be used when creating 
                      system account to turn off password ageing, even though 
                      system account has no password at all.
  --no-log-init   : Do not add user to lastlog and faillog databases
  --create-home   : Create the user's home directory if it does not exist.
                    By default no home directories are created
  --no-create-home: Do not create the user's home directory if enabled in defaults
  --no-user-group : Do not create a group. Initial group indicated by --gid 
  --non-unique    : Allow duplicate (non-unique) existing UID in --uid
  --password PASS : (disabled by default)
  --system        : Create system account (no aging, uid chosen in
                     SYS_UID_MIN-SYS_UID_MAX range)
  --root CHROOTDIR: Apply changes in chrooted directory 
  --uid UID       : numerical value for user's ID.
  --user-group    : Create group with the same name as user, and use as initial group
  --selinux-user SEUSER : SELinux user for the user's login
</pre>

  basic user audit
<pre xxxsmall zoom>
$ who     ← Displays current users logged into the system and the  logged-in time
$ w       ← Displays who is logged into the system and <b>what they are doing</b> (procs. they are running). 
$ users   ← Displays only user names who are currently logged in
$ last    ← Displays records of users-logged-in time, <b>remote IP or PTTY</b>, reboot time, 
$ lastlog ← Displays list of users and what day/time they logged into the system.
$ whoami  ← Tells the user who they are currently logged in as
</pre>
  <hr xxxsmall />

  basic process audit/control
<pre xxxsmall zoom>
$ ps   ← shows list of the processes running. i
         Without options: processes belonging to current user&amp;with a controlling terminal
  Ex. options include:
   -a: all processes from all users
   -u: add user names, %cpu usage, and %mem usage,...
   -x: add also processes without controlling terminals
   -l: add information including UID and nice value
   --forest: show process hierarchy.

$ pstree ← show parent/children process tree (-p flag show pid)

$ top -n 1 ← Display top by cpu processes once and finish
$ top    ← real-time display processes ordered by memory/CPU/...(as in CPU usage)

  Z,B,E,e   Global: 'Z' colors; 'B' bold; 'E'/'e' summary/task memory scale
  l,t,m     Toggle Summary: 'l' load avg; 't' task/cpu stats; 'm' memory info
  0,1,2,3,I Toggle: '0' zeros; '1/2/3' cpus or numa node views; 'I' Irix mode
  f,F,X     Fields: 'f'/'F' add/remove/order/sort; 'X' increase fixed-width

  L,&,<,> . Locate: 'L'/'&' find/again; Move sort column: '<'/'>' left/right
  R,H,V,J . Toggle: 'R' Sort; 'H' Threads; 'V' Forest view; 'J' Num justify
  c,i,S,j . Toggle: 'c' Cmd name/line; 'i' Idle; 'S' Time; 'j' Str justify
  x,y     . Toggle highlights: 'x' sort field; 'y' running tasks
  z,b     . Toggle: 'z' color/mono; 'b' bold/reverse (only if 'x' or 'y')
  u,U,o,O . Filter by: 'u'/'U' effective/any user; 'o'/'O' other criteria
  n,#,^O  . Set: 'n'/'#' max tasks displayed; Show: Ctrl+'O' other filter(s)
  C,...   . Toggle scroll coordinates msg for: up,down,left,right,home,end

  k,r       Manipulate tasks: 'k' kill; 'r' renice
  d or s    Set update interval
  W,Y       Write configuration file 'W'; Inspect other output 'Y'
  q         Quit


$ kill -l   ← Display existing signals (Default to SIGTERM that most of the times
              will just terminate the process "cleanely")
→  1) SIGHUP   2) SIGINT   3) SIGQUIT  4) SIGILL   5) SIGTRAP
→  6) SIGABRT  7) SIGBUS   8) SIGFPE   9) SIGKILL 10) SIGUSR1
→ 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM
→ 16) SIGSTKFLT   17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP
→ 21) SIGTTIN 22) SIGTTOU 23) SIGURG  24) SIGXCPU 25) SIGXFSZ
→ 26) SIGVTALRM   27) SIGPROF 28) SIGWINCH    29) SIGIO   30) SIGPWR
→ 31) SIGSYS  34) SIGRTMIN    35) SIGRTMIN+1  36) SIGRTMIN+2  37) SIGRTMIN+3
→ 38) SIGRTMIN+4  39) SIGRTMIN+5  40) SIGRTMIN+6  41) SIGRTMIN+7  42) SIGRTMIN+8
→ 43) SIGRTMIN+9  44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13
→ 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12
→ 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9  56) SIGRTMAX-8  57) SIGRTMAX-7
→ 58) SIGRTMAX-6  59) SIGRTMAX-5  60) SIGRTMAX-4  61) SIGRTMAX-3  62) SIGRTMAX-2
→ 63) SIGRTMAX-1  64) SIGRTMAX    
 
$ kill [ -s (signal name)] 'process_id' ← Send signal to process. kill -9 kills unconditionally
$ killall "process_name"  ← send signal to all processes matching full name
$ pkill "process_name"    ← send signal to all processes matching part of the name
$ skill  ← send a particular signal to command/username/tty.
        -L --- list the various signals that can be sent
        -u --- specify a username;
        -p --- process id (followed by the process id)
        -c --- command name (this is the same as killall)
        -t --- (tty number)
        -v --- verbose mode
        -i --- interactive mode.

PAUSE AND CONTINUE A PROCESS:
$ kill -STOP "pid" # Pauses
$ kill -CONT "pid" # Continues

$ nice -20 make ← Sets make priority to -20 
                  -20 is maximum priority   (negative only allowed to root)
                   20 is the minimum priority.
$ renice 10 "pid" ← Changes priority of running process.
</pre>
</td>
<td colsep></td>
<td colsep>Network</td>
<td>
  basic network audit/control
<pre xxxsmall zoom>
<b>Socket Statistics (ss)</b>
USAGE EXAMPLES
$ sudo ss -t -a -p
        Display all (-a) TCP (-t) sockets and processes using them (-p)
→ STATE       ... ADDRESS:PORT              PEER ADDRESS:PORT                
→ ESTABLISHED ... 127.0.0.1:postgres        127.0.0.1:46404        users:(("postgres",pid=64032,fd=11))
→ ESTABLISHED ... 10.0.0.5:idonix-metanet   81.61.178.46:51003     users:(("sshd",pid=61411,fd=3),("sshd",pid=61407,fd=3))
→ ESTABLISHED ... 127.0.0.1:37200           127.0.0.1:50004        users:(("sshd",pid=61411,fd=10))
→ ESTABLISHED ... 127.0.0.1:postgres        127.0.0.1:45086        users:(("postgres",pid=43553,fd=11))
→ TIME_WAIT   ...
$ ss -o state established '( dport = :ssh or sport = :ssh )'
        Display all established ssh connections.
$ ss -x src /tmp/.X11-unix/*
        Find all local processes connected to X server.
$ ss -o state fin-wait-2 '( sport =  :http  or  sport  =  :https  )'  dst 193.233.7/24
        List  all  the tcp sockets in state FIN-WAIT-1 for our apache to
        network 193.233.7/24 and look at their timers.

    ss man summary
    (MAIN) OPTIONS
    -n: Do not try to resolve service names.
    -a: Display both listening and non-listening  (for  TCP  this  means
        established connections) sockets
    -l: Display only listening sockets (omitted by default)
    -e: Show detailed extended socket information
    <b>-m: Show socket memory usage</b>
    <b blue>-p: Show process using socket</b>
    -i: Show internal TCP information.
    -K: <b TODO>Attempts to forcibly close sockets</b>. This option displays sockets
        that are successfully closed and silently skips sockets that the
        kernel does not support closing. IPv4/6 sockets only
    <b>-s: Print summary statistics, useful when
        amount of sockets is  so  huge  that  parsing  /proc/net/tcp  is
        painful.</b>
    -N NSNAME: Switch to the specified network namespace name.
    -b: Show socket BPF filters (only admins).
    -0: Display PACKET sockets   -w: Display RAW sockets
    <b blue>-t: Display TCP sockets</b>      -x: Display Unix domain sockets
    <b>-u: Display UDP sockets</b>      -S: Display SCTP sockets.
    -d: Display DCCP sockets     --vsock: Display vsock sockets
    ...

<b>DISPLAY IP Routing Table</b>
$ ip route list
→ <b>default via 10.0.0.1 dev eth0</b>
→ 10.0.0.0/24     dev eth0    proto kernel scope link src 10.0.0.5 
→ 169.254.0.0/16  dev eth0                 scope link metric 1002 
→ 172.17.0.0/16   dev docker0 proto kernel scope link src 172.17.0.1 
→ 168.63.129.16   via 10.0.0.1 dev eth0 proto static 
→ 169.254.169.254 via 10.0.0.1 dev eth0 proto static 
→ ...

<b>DISPLAY IP Routing Table</b>
1: lo:     <LOOPBACK,           UP,LOWER_UP> ...  state UNKNOWN mode DEFAULT ...
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0:   <BROADCAST,MULTICAST,UP,LOWER_UP> ...  state UP      mode DEFAULT ...
    link/ether    00:0d:3a:26:bb:2b brd ff:ff:ff:ff:ff:ff
...

<b>SNIFF network traffic</b>
$ sudo tcpdump -i eth0 port 8090  -n -A  ←  "snif" IP traffic  in network interface eth0
                                        with IP packets to/from port 8090 
                                        -A: Show in text/ASCII format
                                        -n: Do not convert IPs to host-names
                                            (Avoiding slow dns reverse lookups)

<b>Show IP route from source to destination ("hop" path)</b>
$ sudo traceroute "destination_IP_or_host" ← (attempts to ) show the route of a packet. 

<b>Examine open ports in remote machine</b>
$ sudo nmap -nt remote_machine    ← (try to) query remote machine for open ports 
</pre>

basic network traffic shaping:
<pre xxxsmall zoom>
<b> Network interface shaping with <a href="https://github.com/magnific0/wondershaper/">Wondershaper</a></b>
$ sudo ./wondershaper -a eth0 -u 4096 -d 8192 ← limit upload: 4Mbps, download:8Mbs

<b>Process network shaping with <a href="https://www.pcsuggest.com/bandwidth-traffic-shaping-in-linux-with-firejail/">Firejail</a></b>
$ firejail  --net=enp2s0 firefox [/bash] 
$ firejail --list | grep 'firefox' | awk -F: '{print$1}'
$ firejail  --bandwidth=PID set interface-name down-speed up-speed
</pre>

  Internet Utility commands
<pre xxxsmall zoom>
$ <b>host (ip_address|domain_name)</b>  ← Performs lookup of an internet address (using the Domain Name System, DNS). Simply type:
or
<b></b>
$ <b>dig</b>    www.amazon.com  ←        query to DNS
$ dig -x 10.10.10.10     ← revers query to DNS 
(check man page for more options)

$ <b>wget</b> www.myDomain.com/myPage  ← HTTP client
    Options: 
    -m: archive/"m"irrow single web-site
    -nc: (no clobber) avoid overwriting local files 

$   wget --spider \    ← parse bookmarks.html for links
    --force-html \
    -i bookmarks.html
(see man page for more info)

$ <b>curl</b>  ← Script oriented HTTP client.
          It can access dictionary servers (dict), 
          ldap servers, ftp, http, gopher, ...

$ curl -M : To access the full/huge manual 
$ curl -u username:password http://www.placetodownload/file
</pre>
</td>
<td colsep> </td>
<td colsep>System&nbsp;Info</td>
<td>
   Basic info
<pre xxxsmall zoom>
$ uptime  ← shows how long the computer has been "up" since last reboot. 
           number of users and the processor load
$ date    ← current date/time
$ cal     ← display calendar
$ uname   ← print information on the system such as OS type, kernel version...
    -a --- print all the available information
    -m --- print only information related to the machine itself
    -n --- print only the machine hostname
    -r --- print the release number of the current kernel
    -s --- print the operating system name
    -p --- print the processor type

$ cat /etc/*release* | sort | uniq ← Shows OS identification 
                                       (Distribution, major, minor,patch version, flavour, ...)
</pre>
   <code><a href="https://linux.die.net/man/8/vmstat">vmstat</a></code>(OS global stats)
<pre xxxsmall zoom bgorange>
display real-time stats of procs.,mem, paging, block IO, traps, disks and cpu activity.
Ussage:
  $ vmstat [options] [delay [count]]
The first report produced gives averages since the last reboot.
Additional reports give information on a sampling period of length delay.
The process and memory reports are instantaneous in either case.

 -a --active: Display active and inactive memory
              FROM:<a href="https://unix.stackexchange.com/questions/305606/linux-inactive-memory">REF</a>
              * active memory are pages which have been accessed "recently"
              * inactive memory are pages which have not been accessed "recently"
              """A high ratio of active to innactive memory can indicate
                 memory pressure, but that condition is usually accompanied by
                 pagin/swapping which is easier to understand"""
 -f --forks : display number of forks (fork,vfork,clone) since boot. This
 -m --slabs : Display <a href="https://en.wikipedia.org/wiki/Slab_allocation">slabinfo</a>
              (memory assigned to kernel objects)
 -s --stats : Displays a table of various event counters and memory statis‐
            tics. (without repeating)
 -d --disk  : Report disk statistics
 -D --disk-sum: Report some summary statistics about disk activity.
 -p --partition device: Detailed statistics about partition
 -S --unit  :=  1000 (k), 1024 (K), 1000000 (m), 1048576 (M) bytes.
            swap (si/so) and block (bi/bo) not affected
 -t --timestamp: Append timestamp to each line
 <b>-w --wide  : Wide output mode</b> (Recomended)

OUTPUT FIELD DESCRIPTION 
<b>VM MODE</b>                                         | <b>DISK MODE(--disk)</b>
Procs                                           | Reads
  r: # of runnable procs(running|waiting for)   |   total  :   Total reads completed successfully
  b: # of processes <b red>in uninterruptible sleep</b>    |   merged : grouped reads(resulting in 1 I/O)
                                                |   sectors: Sectors read successfully
Memory *1                                       |        ms: milliseconds spent reading 
(inactiv,cache,buff can be freed if needed)     |
  swpd  : virtual memory used                   | 
  free  : idle (ready to use) memory            | Writes
  buff  : memory used as disk buffers           |     total:   Total writes completed successfully
  cache : memory used as cache                  |    merged: grouped writes (resulting in 1 I/O)
  inact : inactive memory (-a option)           |   sectors: Sectors written successfully
          (still cached for possible reuse)     |        ms: milliseconds spent writing
  active: memory Used by processes              |
                                                | IO
 Swap                                           |   cur: I/O in progress
   si: Amount of memory swapped in from disk(/s)|     s: seconds spent for I/O
   so: Amount of memory swapped to disk(/s)     +--------------------------------------------------------

IO                                              | <b>DISK PARTITION MODE (--partition)</b>
  bi: Blocks received from block device         |           reads: Total # of reads issued to part.
  bo: Blocks     sent   to block device         |    read sectors: Total read sectors for partition
                                                |          writes: Total # of writes issued to part.
System                                          |requested writes: Total # of write requests made for part
  in: interrupts per second, including the clock+---------------------------------------------------------
  cs: <b red>context switches per second</b>
                                                | <b>SLAB MODE (--slabs)</b>
CPU (percentages of total CPU time)             |   cache: Cache name
  us: Time spent running non-kernel code(user  )|     num: # of currently active objects
  sy: Time spent running     kernel code(system)|   total: Total # of available objects
  id: Time spent idle                           |    size: Size of each object
  wa: <b red>Time spent waiting for IO</b>                 |   pages: # of pages with at least one active object

  st: Time stolen from a virtual machine        +---------------------------------------------------------

<b>All linux blocks are currently 1024 bytes.</b>

SEE ALSO
    free(1), iostat(1), mpstat(1), ps(1), sar(1), top(1)

* 1: <a href="https://stackoverflow.com/questions/6345020/what-is-the-difference-between-buffer-vs-cache-memory-in-linux">REF</a>
    - buffers are associated with a specific block device, 
      caching filesystem metadata(dir contents, file permissions),
      as tracking in-flight pages (what's being written
      from or read to for a particular block device). 
    - cache only contains parked file data (file content)
</pre>

  Accurate ps memory use
<pre xxxsmall zoom>
<a href="https://github.com/pixelb/ps_mem/">REF</a>

PIP INSTALL:
$ sudo pip install ps_mem

USSAGE:
ps_mem [-h|--help] [-p PID,...] [-s|--split-args] [-t|--total] [-w N]
       [-d|--discriminate-by-pid] [-S|--swap]

Ex 1:
  $ sudo ps_mem
  →  Private  +   Shared  =  RAM used       Program
  → 
  →  34.6 MiB +   1.0 MiB =  35.7 MiB       gnome-terminal
  → 139.8 MiB +   2.3 MiB = 142.1 MiB       firefox
  → 291.8 MiB +   2.5 MiB = 294.3 MiB       gnome-shell
  → 272.2 MiB +  43.9 MiB = 316.1 MiB       chrome (12)
  → 913.9 MiB +   3.2 MiB = 917.1 MiB       thunderbird
  → ---------------------------------
  →                           1.9 GiB
  → =================================
  → 

Ex 2: Show only ps_mem for current $USER:
  ~ sudo ps_mem -p $(pgrep -d, -u $USER)
  → ...

Ex 3: Summarize total RAM usage per user:

  for i in $(ps -e -o user= | sort | uniq); do
    printf '%-20s%10s\n' $i $(sudo ps_mem --total -p $(pgrep -d, -u $i))
  done
</pre>

  SysV shared memory
<pre xxxsmall zoom>
- SysV shared memory segments are accounted as a cache, though they
  do not represent any data on the disks.

To check the size of shared memory segments:
#  ipcs -m  # command and checking the bytes column.
</pre>
</td>
<td colsep></td>
<td>
  <b>BACKUPS</b> 
  <ul xxsmall zoom>
  <li><a bgorange href="https://github.com/earizon/easyup">EasyUp</a> KISS incremental remoto backup around rsync+ssh</a></li>
  <li><a href="http://rsnapshot.org/">Rsnapshot</a> filesystem snapshot utility on top of rsync.<br/>
     rsnapshot makes it easy to make periodic snapshots of local machines, and remote machines over ssh.
     The code makes extensive use of hard links whenever possible, to greatly reduce the disk space required 
     and rsync to save bandwidth (backup only changes) 
  </li>
  <li><a href="https://linuxhint.com/inotofy-rsync-bash-live-backups/">Live backups with inotify + rsync + bash</a>. Backup on "real-time changes"</li>
  <li><span cite>"""<a href="http://www.bacula.org/">Bacula</a> is a set of
    Open Source, computer programs that permit to manage backup, recovery, and 
    verification of computer data across a network of computers of different 
    kinds,  offering many advanced storage management features that make it 
    easy to find and recover lost or damaged files.</span>"""
<pre xxxsmall>
<a href="http://www.bacula.org/9.0.x-manuals/en/main/index.html">Manual v.9</a>
Director Daemon supervises all the backup, restore, verify and archive operations.
Sysadmin uses Director to schedule backups and to recover files..

Console service allows the administrator or user to communicate with the Director
(three versions: text-based, QT-based, wxWidgets)

File Daemon It's installed on the machine to be backed up and is responsible for 
providing the file attributes and data when requested by the Director
as well as for the file system dependent part of restoring the file attributes and data 
during a recovery operation.

Storage daemons are software programs in charge of storage and recovery of the 
file attributes and data to the physical backup media or volumes. In other words, it is 
responsible for reading and writing your tapes (or other storage media, e.g. files)

Catalog Services are responsible for maintaining the file indexes and 
volume databases for all files backed up allowing sysadmin or user to
quickly locate and restore any desired file. The Catalog services sets
Bacula apart from simple backup programs like tar and bru, because the catalog
maintains a record of all Volumes used, all Jobs run, and all Files saved, permitting
efficient restoration and Volume management. Bacula currently supports three different 
databases, MySQL, and PostgreSQL one of which must be chosen when building Bacula.

Monitor Service Allows the administrator or user to watch current status of Directors,
File Daemons and Bacula Storage Daemons. Currently, only a GTK+ version is available.
</pre>
  </li>
  <li>Symple remote backups with ssh
<pre xxxsmall>
$ tar cjf - myDirToBackup \       # local
  | ssh myUser@myRemoteMachine \  # ssh pipe
  "cd myBackupPath && tar -xjf -" # remote
</pre>
  </li>
  </ul>
</td>  
</tr>  
</table>  

<table>  
<tr>  
<td colsep>Text&nbsp;Mng</td>  
<td>  
   Text view tools
<pre xxxsmall zoom>
Displaying text:
$ head -n 20 /path/to/textFile # shows first 20 lines (-n 20). 10 lines by default if -n not provided.
$ tail -n 20 /path/to/textFile # shows last  20 lines (-n 20). 10 lines by default if -n not provided.
$ tail -f    /path/to/textFile # shows stream ("f"lush) of lines as they are appended to the file
$ less       /path/to/textFile # Views text. Add scroll control backwards and forwards. 
                               # embedded systems use "more", that just scroll forwards.
$ cat        /path/to/textFile # dump text content to standard output (STDOUT)
$ cat file1 file2 file3 ....   # concatenates files'content and dumps into STDOUT
$ tac file1 file2 file3 ....   # concatenates files'content and dumps into STDOUT in reverse order
</pre>
   Text info tools
<pre xxxsmall zoom>
$ wc /path/to/textFile  ← Display total count of words, lines and bytes
                          Options:
                            -w count only words
                            -l count only lines
                            -w count only bytes


$ diff file1 file2     ← Compares two text files and output a difference report indicating:
                         '&gt; line_in_file2_not_in_file1
                         '&lt; line_in_file1_not_in_file2
$ sdiff                ← Similar to diff but with report in colum mode (more human readable)
$ diff3                ← diff for three files
</pre>

   Text modification tools
<pre xxxsmall zoom>
By default we can use next tools like:
   $ tool inputFile   # Apply to given file
or
   $ command1 ... | tool1 | tool2 | tool3 # Use stdout from previous command (|) as input 

sorting content:
$ sort file1  ← Sorting alphabetically lines in file  (-r to reverse, -g for numerical sort)
j
$ sort file1 -t ':' -k 4 -k 1    ← Use ':' as separator, sort first by column 4, then column 1.

$ join file1 file2    ← Join two lines together assuming they share at least one common value
                        on the relevant line, skiping lines withouth common value.

Cut by column:
$ cut -d "," -f 1,3,7 file1.csv ← Use "," as column delimiter, show then columns 1,3,7
$ cut -c 1-50         file.txt  ← show characters 1 to 50 in each line
$ cut -5, 8, 20-      file.txt  ← show characters 1 to 5 and 8 and from 20 to the end

        This would display (“cut”) characters (columns) 1 to 5, 8 and from 20 to the end.

$ uniq file1     ←  Eliminates duplicate entries from a file
                    Commonly used with sort like:
                    $ cat file.txt | sort | uniq
                    Options: -c: display number of occurances of each duplicate
                             -u: list only unique entries
                             -d: list only duplicate entries

$ tr "u" "d" file1  ← translate all instances of characters in a text file
$  cat some_file | tr '[A-Z]' '[a-z]' > new_file  ← Convert all capital letters to lowercase

$ nl file1.txt  ← Display file1.txt to STDOUT prefixing with line numbers

$ sed "s/<b green>up</b>/<b orange>down</b>/<b blue>g</b>" file1.txt ← Replaces "<b green>up</b>" by "<b orange>down</b>". "<b blue>g</b>" flags indicates to replace all ocurrences. Otherwise only first is replaced.
sed stays for "Stream editor", and has a lot of powerful flags like searching for regular expresions ...
<span TODO>$ awk </span>
</pre>
</td>
<td>  
   Text view tools
<pre xxxsmall zoom>
Displaying text:
$ head -n 20 /path/to/textFile # shows first 20 lines (-n 20). 10 lines by default if -n not provided.
$ tail -n 20 /path/to/textFile # shows last  20 lines (-n 20). 10 lines by default if -n not provided.
$ tail -f    /path/to/textFile # shows stream ("f"lush) of lines as they are appended to the file
$ less       /path/to/textFile # Views text. Add scroll control backwards and forwards. 
                               # embedded systems use "more", that just scroll forwards.
$ cat        /path/to/textFile # dump text content to standard output (STDOUT)
$ cat file1 file2 file3 ....   # concatenates files'content and dumps into STDOUT
$ tac file1 file2 file3 ....   # concatenates files'content and dumps into STDOUT in reverse order
</pre>
   Text info tools
<pre xxxsmall zoom>
$ wc /path/to/textFile  ← Display total count of words, lines and bytes
                          Options:
                            -w count only words
                            -l count only lines
                            -w count only bytes


$ diff file1 file2     ← Compares two text files and output a difference report indicating:
                         '&gt; line_in_file2_not_in_file1
                         '&lt; line_in_file1_not_in_file2
$ sdiff                ← Similar to diff but with report in colum mode (more human readable)
$ diff3                ← diff for three files
</pre>

   Text modification tools
<pre xxxsmall zoom>
By default we can use next tools like:
   $ tool inputFile   # Apply to given file
or
   $ command1 ... | tool1 | tool2 | tool3 # Use stdout from previous command (|) as input 

sorting content:
$ sort file1  ← Sorting alphabetically lines in file  (-r to reverse, -g for numerical sort)
j
$ sort file1 -t ':' -k 4 -k 1    ← Use ':' as separator, sort first by column 4, then column 1.

$ join file1 file2    ← Join two lines together assuming they share at least one common value
                        on the relevant line, skiping lines withouth common value.

Cut by column:
$ cut -d "," -f 1,3,7 file1.csv ← Use "," as column delimiter, show then columns 1,3,7
$ cut -c 1-50         file.txt  ← show characters 1 to 50 in each line
$ cut -5, 8, 20-      file.txt  ← show characters 1 to 5 and 8 and from 20 to the end

        This would display (“cut”) characters (columns) 1 to 5, 8 and from 20 to the end.

$ uniq file1     ←  Eliminates duplicate entries from a file
                    Commonly used with sort like:
                    $ cat file.txt | sort | uniq
                    Options: -c: display number of occurances of each duplicate
                             -u: list only unique entries
                             -d: list only duplicate entries

$ tr "u" "d" file1  ← translate all instances of characters in a text file
$  cat some_file | tr '[A-Z]' '[a-z]' > new_file  ← Convert all capital letters to lowercase

$ nl file1.txt  ← Display file1.txt to STDOUT prefixing with line numbers

$ sed "s/<b green>up</b>/<b orange>down</b>/<b blue>g</b>" file1.txt ← Replaces "<b green>up</b>" by "<b orange>down</b>". "<b blue>g</b>" flags indicates to replace all ocurrences. Otherwise only first is replaced.
sed stays for "Stream editor", and has a lot of powerful flags like searching for regular expresions ...
<span TODO>$ awk </span>
</pre>
</td>
<td>
  <a href="http://freedesktop.org/wiki/Software/systemd/">SystemD</a>("Services")
<pre xxxsmall zoom>
<a href="https://www.freedesktop.org/software/systemd/man/systemd.service.html">man page</a>,
<a href="http://www.tecmint.com/create-new-service-units-in-systemd/">See also</a>
"service unit"                  "targets"
- createNew                       unit_collection
- run                             "wants"
- lifespan:daemon|run-once

Check unit_collections:          Check status of a service:
# systemctl --type=service       # systemctl status firewalld.service


# (sudo) systemctl daemon-reload
# (sudo) systemctl \
   enable|start|stop|restart|disable \
     firewalld.service

# sudo vim /etc/systemd/system/MyCustomScript.service\
  | [Unit]
  | Description = making network connection up
  | After = network.target
  | [Service]
  | ExecStart = /root/scripts/conup.sh
  | [Install]
  | WantedBy = multi-user.target

<b>
Systemd      |Systemd      |Systemd           | Systemd
Utilities    |Daemons      |Targets           | Core
</b>                                            
$ systemctl  |systemd      | bootmode         | manager
$ journalctl |journald     | basic            | systemd
$ notify     |networkd     | shutdown  
$ analyze    |logind       | reboot    
$ cgls       |user session |
$ cgtop                    | multiuser          
$ loginctl                 | dbus dlog, logind  
$ nspawn                   |                    
                           | graphical
                           | user-session
                           | display service

</pre>
FILE NAME EXTENSIONS FOR UNIT TYPES:
<pre xxxsmall zoom>
*<b orange>.target     </b>: define groups of units. They achieve little themselves and serve to call
 <b orange>            </b>  other units that are responsible for services, filesystems ...
 <b orange>            </b>  (equivalent to the classical SysV runlevels)
*<b orange>.service    </b>: handle services that SysV-init-based distributions will typically
 <b orange>            </b>  start or end using init scripts.
*<b orange>.(auto)mount</b>: mounting and unmounting filesystems
*<b orange>.path       </b>: allow systemd to monitor files and directories specified
 <b orange>            </b>  when an access happens in path, systemd will start the appropriate unit
*<b orange>.socket     </b>: create one or more sockets for socket activation.
 <b orange>            </b>  service unit associated will start the service when a connection request
 <b orange>            </b>  is received.
</pre>

CONFIG. FILE LAYOUT:
<pre xxxsmall zoom>
(NOTE: /etc takes precedence over /usr)
<b orange>Maintainer   </b>: /usr/lib/systemd/system              ( + $ systemctl daemon-reload)
<b orange>Administrator</b>: /etc/systemd/system/[name.type.d]/ ) ( + $ systemctl daemon-reload)
<b orange>runtime      </b>: /runtime/systemd/system
</pre>

<b>Log Filtering (Journalctl):</b>
<pre xxxsmall zoom>
# journalctl                      → all logs
# journalctl -b                   → Boot Messages
# journalctl -b -1                → Last Boot Messages
# journalctl --list-boots         → list system boots
# journalctl --since "3 hour ago" → Time range
                     "2 days ago"
    --until "2015-06-26 23:20:00"
# journalctl -u nginx.service     → by unit (can be specified multiple times)
# journalctl -f                   → Follow ("tail")
# journalctl -n 50                → most recent (50) entries
# journalctl -r                   → reverse chronological order
# journalctl -b -1  -p "crit"     → By priority: -b -1 : FROM emergency , -p "crit" : TO: Critical
# journalctl _UID=108             → By _UID
---------------------------------------------------------------------
Output Formats ( -o parameter )

   json: json one long-line
   json-pretty:
   verbose:
   cat:  very short form, without any date/time or source server names
   short: (default), syslog style
   short-monotonic: similar to short, but the time stamp second value is shown with precision
</pre>
</td>  
<td colsep></td>  
<td colsep>Soft&nbsp;install</td>  
<td>
<a href="https://dnf.readthedocs.io/en/latest/">DNF</a>(RedHat/Fedora/Centos)
<pre xxxsmall zoom>
$ dnf list *dnf*
$ dnf list <b>installed</b> *debuginfo 
$ dnf list <b>available</b> gtk*devel   
$ dnf list <b>extras   </b>            ← pkgs. installed NOT available in any known repository
$ dnf list <b>obsoletes</b>            ← obsoleted by packages in any accessible repository
$ dnf list <b>recent   </b>            ← packages recently added into accessible repositories
$ dnf list <b>upgrades </b>            ← available packages upgrading some installed packages

AVAILABLE COMMANDS:
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#autoremove-command-label">autoremove</a>
    Removes all "leaf" packages from the system that were originally 
    installed as dependencies of user-installed packages but which are no 
    longer required by any such package.
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#check-command-label">check</a>
    Checks the local packagedb and produces information on any problems it 
    finds. You can pass the check command the options “–dependencies”, “–
    duplicates”, “–obsoleted” or “–provides”, to limit the checking that is 
    performed (the default is “all” which does all).
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#check-update-command-label">check-update</a>
    Non-interactively checks if updates of the specified packages are 
    available. If no <package-specs> are given, checks whether any updates at 
    all are available for your system. DNF exit code will be 100 when there 
    are updates available and a list of the updates will be printed, 0 if not and 1 if an error occurs.

    Please note that having a specific newer version available for an 
    installed package (and reported by check-update) does not imply that 
    subsequent dnf upgrade will install it. The difference is that dnf 
    upgrade must also ensure the satisfiability of all dependencies and other restrictions.

    Output is affected by config option autocheck_running_kernel

<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#clean-command-label">clean</a>
    Performs cleanup of temporary files kept for repositories. This includes any 
    such data left behind from disabled or removed repositories as well as 
    for different distribution release 
    versions.
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#distro-sync-command-label">distro-sync</a>
    As necessary upgrades, downgrades or keeps selected installed packages to 
    match the latest version available from any enabled repository. If no 
    package is given, all installed packages are considered.
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#downgrade-command-label">downgrade</a>
    Downgrades the specified packages to the highest installable package of all 
    known lower versions if possible. When version is given and is lower than 
    version of installed package then it downgrades to target version.
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#group-command-label">group</a>
    Groups are virtual collections of packages. DNF keeps track of groups 
    that the user selected (“marked”) installed and can manipulate the 
    comprising packages with simple commands.  (See only manual for more info)
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#help-command-label">help</a>
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#history-command-label">history</a>
   The history command allows the user to view what has happened in past 
    transactions and act according to this information (assuming the history_record configuration option is set).
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#info-command-label">info</a>
     list description and summary information about installed and available packages.
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#install-command-label">install</a>
    DNF makes sure that the given packages and their dependencies are 
    installed on the system. Each <spec> can be either a <package-spec>, or a 
    @<group-spec>. See Install Examples. If a given package or provide cannot 
    be (and is not already) installed, the exit code will be non-zero.

    When <package-spec> that specify exact version of the package is given, 
    DNF will install the desired version, no matter which version of the 
    package is already installed. The former version of the package will be 
    removed in the case of non-installonly package.

    There are also a few specific install commands install-n, install-na and 
    install-nevra that allow the specification of an exact argument in NEVRA 
    format.

    dnf install tito
        Install package tito (tito is package name).
    dnf install ~/Downloads/tito-0.6.2-1.fc22.noarch.rpm
        Install local rpm file tito-0.6.2-1.fc22.noarch.rpm from ~/Downloads/ directory.
    dnf install tito-0.5.6-1.fc22
        Install package with specific version. If the package is already 
        installed it will automatically try to downgrade or upgrade to specific version.
    dnf --best install tito
        Install the latest available version of package. If the package is 
        already installed it will automatically try to upgrade to the latest 
        version. If the latest version of package cannot be installed, the installation fail.
    dnf install vim
        DNF will automatically recognize that vim is not a package name, but 
        provide, and install a package that provides vim with all required 
        dependencies. Note: Package name match has precedence over package provides match.
    dnf install https://kojipkgs.fedoraproject.org//packages/tito/0.6.0/1.fc22/noarch/tito-0.6.0-1.fc22.noarch.rpm
        Install package directly from URL.
    dnf install '@Web Server'
        Install environmental group ‘Web Server’
    dnf install /usr/bin/rpmsign
        Install a package that provides /usr/bin/rpmsign file.
    dnf -y install tito --setopt=install_weak_deps=False
        Install package tito (tito is package name) without weak deps. Weak 
        deps are not required for core functionality of the package, but they 
        enhance the original package (like extended documentation, plugins, 
        additional functions, …). 
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#list-command-label">list</a>
    Dumps lists of packages depending on the packages’ relation to the system
    . A package is installed if it is present in the RPMDB, and it is 
    available if it is not installed but it is present in a repository that 
    DNF knows about. The list command can also limit the displayed packages 
    according to other criteria, e.g. to only those that update an installed 
    package. The exclude option in configuration file (.conf) might influence 
    the result, but if the command line option --disableexcludes is used, it 
    ensure that all installed packages will be listed
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#makecache-command-label">makecache</a>
    dnf [options] makecache
        Downloads and caches in binary format metadata for all known repos. 
        Tries to avoid downloading whenever possible (e.g. when the local 
        metadata hasn’t expired yet or when the metadata timestamp hasn’t changed).
    dnf [options] makecache --timer
        Like plain makecache but instructs DNF to be more resource-aware, 
        meaning will not do anything if running on battery power and will 
        terminate immediately if it’s too soon after the last successful 
        makecache run (see dnf.conf(5), metadata_timer_sync). 
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#mark-command-label">mark</a>
    dnf mark install <package-specs>...
        Marks the specified packages as installed by user. This can be useful 
        if any package was installed as a dependency and is desired to stay on 
        the system when Auto Remove Command or Remove Command along with 
        clean_requirements_on_remove configuration option set to True is executed.
    dnf mark remove <package-specs>...
        Unmarks the specified packages as installed by user....
    dnf mark group <package-specs>...
        Marks the specified packages as installed by group...
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#module-command-label">module</a>
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#provides-command-label">provides</a>
    Finds the packages providing the given <provide-spec>. This is useful 
    when one knows a filename and wants to find what package (installed or not
    ) provides this file.
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#reinstall-command-label">reinstall</a>
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#remove-command-label">remove</a>
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#repoinfo-command-label">repoinfo</a>
<a bgorange href="https://dnf.readthedocs.io/en/latest/command_ref.html#repolist-command-label">repolist</a>
$ dnf repolist
→ Extra Packages for Enterprise Linux 7 - x86_64                                          24 MB/s |  15 MB     00:00    
→ CentOS-7 - openlogic packages for x86_64                                               174 kB/s |  38 kB     00:00    
→ Node.js Packages for Enterprise Linux 7 - x86_64                                       2.9 MB/s | 1.1 MB     00:00    
→ ...
→ Using metadata from Mon Sep 10 16:21:18 2018
→ repo id                                     repo name                                                            status
→ base                                        CentOS-7 - Base                                                       9,911
→ centos-openshift-origin                     CentOS OpenShift Origin                                                 232
→ centos-sclo-rh                              CentOS-7 - SCLo rh                                                    7,995
→ centos-sclo-sclo                            CentOS-7 - SCLo sclo                                                    773
→ code                                        Visual Studio Code                                                       43
→ docker-ce-stable                            Docker CE Stable - x86_64                                                20
→ *epel                                       Extra Packages for Enterprise Linux 7 - x86_64                       12,672
→ extras                                      CentOS-7 - Extras                                                       402
→ go-repo                                     go-repo - CentOS                                                        253
→ nodesource                                  Node.js Packages for Enterprise Linux 7 - x86_64                        144
→ openlogic                                   CentOS-7 - openlogic packages for x86_64                                113
→ pgdg94                                      PostgreSQL 9.4 7 - x86_64                                               603
→ updates                                     CentOS-7 - Updates                                                    1,333

<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#repoquery-command-label">repoquery</a>
    dnf [options] repoquery [<select-options>] [<query-options>] [<pkg-spec>]
        Searches the available DNF repositories for selected packages and 
        displays the requested information about them. It is an equivalent of rpm 
        -q for remote repositories.
    dnf [options] repoquery --querytags
        Provides list of recognized tags by repoquery option --queryformat
        There are also a few specific repoquery commands repoquery-n, 
        repoquery-na and repoquery-nevra that allow the specification of an exact 
        argument in NEVRA format (does not affect arguments of options like –
        whatprovides <arg>, …).
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#repository-packages-command-label">repository-packages</a>
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#search-command-label">search</a>
    Search package metadata for the keywords. Keywords are matched as case-
    insensitive substrings, globbing is supported. By default lists packages 
    that match all requested keys (AND operation). Keys are searched in 
    package names and summaries. If option “–all” is used, lists packages 
    that match at least one of keys (OR operation). And in addition keys are 
    searched in package descriptions and URLs. The result is sorted from the 
    most relevant results to the least.
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#shell-command-label">shell</a>
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#swap-command-label">swap</a>
    Remove spec and install spec in one transaction. Each "spec" can be 
    either a <package-spec>, which specifies a package directly, or a 
    @"group-spec", which specifies an (environment) group which contains it. 
    Automatic conflict solving is provided in DNF by –allowerasing option 
    that provides functionality of swap command automatically
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#updateinfo-command-label">updateinfo</a>
    Display information about update advisories.
    Depending on output type, DNF displays just counts of advisory types (
    omitted or --summary), list of advisories (--list) or detailed 
    information (--info). When --info with -v option is used, the information 
    is even more detailed
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#upgrade-command-label">upgrade</a>
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#upgrade-minimal-command-label">upgrade-minimal</a>
    Upgrade-minimal Command
    
    $ dnf [options] upgrade-minimal
        Updates each package to the latest version that provides bugfix, enhancement or fix for security issue (security)
    $ dnf [options] upgrade-minimal <package-installed-specs>...
        Updates each specified package to the latest available version that provides bugfix, enhancement or fix for security issue (security). Updates dependencies as necessary. 
<a href="https://dnf.readthedocs.io/en/latest/command_ref.html#upgrade-to-command-label">upgrade-to</a>

- ADDITIONAL INFORMATION:
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#options-label">Options</a>
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#specifying-packages-label">Specifying Packages</a>
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#specifying-packages-versions-label">Specifying Exact Versions of Packages</a>
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#specifying-provides-label">Specifying Provides</a>
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#specifying-groups-label">Specifying Groups</a>
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#specifying-transactions-label">Specifying Transactions</a>
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#metadata-synchronization-label">Metadata Synchronization</a>
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#configuration-files-replacement-policy-label">Configuration Files Replacement Policy</a>
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#files-label">Files</a>
    <a href="https://dnf.readthedocs.io/en/latest/command_ref.html#see-also-label">See Also</a>

  <a href="https://www.softwarecollections.org/en/">Software Collections</a>
Ex:
$ python --version
Python 2.7.5

$ scl enable rh-python35 bash
$ python --version
Python 3.5.1

  # <a href="https://wiki.centos.org/SpecialInterestGroup/SCLo">https://wiki.centos.org/SpecialInterestGroup/SCLo</a>
  # <a href="http://fedoraproject.org/wiki/EPEL">http://fedoraproject.org/wiki/EPEL</a>
  """ Extra Packages for Enterprise Linux (or EPEL) is a Fedora Special Interest
    Group that creates, maintains, and manages a high quality set of additional
    packages for Enterprise Linux, including, but not limited to, Red Hat 
    Enterprise Linux (RHEL), CentOS ... usually based on their Fedora counterparts
    and will never conflict with or replace packages in the base Enterprise
    Linux distributions....  """



<b orange># Install from given repository:</b>
$ sudo yum --disablerepo=\* --enablerepo=my-cool-repo install myPackage

Ex: Install Development Tools
...
<b>$ dnf groupinfo "Development Tools"</b>
→ Group: Development Tools
→  Description: A basic development environment.
→  Mandatory Packages:
→    autoconf
→    automake
→    binutils
→    ...
→  Default Packages:
→    byacc
→    cscope
→    ...
→  Optional Packages:
→    ElectricFence
→    ant
→    babel
     ...
<b>$ sudo dnf groupinstall "Development Tools"</b>
</pre>
</td>
<td>
  <a href="https://help.ubuntu.com/community/AptGet/Howto#Maintenance_commands">APT</a>(Debian/Ubuntu)
<pre xxxsmall zoom>

  <a TODO href="https://help.ubuntu.com/community/SourcesList">Package source list: /etc/apt/sources.list file</a>


$ sudo apt-get  install ubuntu-desktop  "package2" ... (-s to simulate)
$ sudo aptitude install ubuntu-desktop  ← Ncurses variant


$ sudo auto-apt run "./configure" ← runs "command_string", installing uninstalled packages when possible.
  ^ keeps databases up-to-date by calling:
    $ auto-apt update
    $ auto-apt updatedb
    $ auto-apt update-local

$ apt-get update ← Run periodically and after modifications to /etc/apt/*

$ apt-get upgrade ← upgrade all installed packages.

$ apt-get dist-upgrade ← same as upgrade, except add the "smart upgrade" checkbox.
                         It tells APT to use "smart" conflict resolution system,
                         and <b>it will attempt to upgrade the most important packages 
                         at the expense of less important ones if necessary.</b>
                         ¡¡¡does not upgrade from a previous version of Ubuntu!!!

<b>Diagnostics:</b>
$ apt-get check    ← update package lists and checks for broken dependencies

<b>Cleaning/Removal</b>
$ sudo apt-get autoclean  ← removes .deb files for packages that are no longer
                            installed on your system. (Can save space in /var/cache/apt/archives)
$ apt-get clean           ← same as autoclean, except it removes all packages from the package cache.
                            Not recomended with slow-connections.
                            ($ du -sh /var/cache/apt/archives)

$ sudo apt-get remove "package_name" ← keeps  configuration files 
$ sudo apt-get purge  "package_name" ← Remove configuration files

$ apt-get autoremove     ← removes packages no longer needed

<b>Troubleshooting</b>
$ sudo apt-get -f install  ← Fix Broken Packages (system complains about "unmet dependencies")

$ dpkg-reconfigure "package_name"


$ echo "'package_name' hold" | \  ← Put package_name on hold
  sudo dpkg --set-selections        may have the unintended side effect of preventing upgrades
                                    to packages that depend on updated versions of the pinned package.
                                    apt-get dist-upgrade will override this, but will warn you first.

<b>Search commands</b>
$ apt-cache search "search_term" ← lists packages matching in name or description

$ dpkg -l *"search_term"*  ← find packages whose names contain "search_term".
                             It also shows whether a package is installed on your system

$ dlocate "package_name"         ← "reverse lookup". It Determines which installed package owns "package_name".
                                    It shows files from installed packages that match "package_name",
                                    with the name of the package they came from. 
                                    Its equivalent to the slower (but installed by default):
                                    $ dpkg -S "filename_search_pattern"
$ sudo apt-file update && \       ← Similar to dlocate  but searches over all available packages. 
                                    """what package provides this file?"""

  sudo apt-file search \"filepath_pattern"
<b>Package Info</b>
$ apt-cache show "package_name"  ← shows the description of "package_name", version, size, dependencies and conflicts.
$ dpkg -L "package_name"         ← list files in package
$ dpkg -c foo.deb                ← lists files in the manually downloaded package "./foo.deb".

$ apt-cache pkgnames             ← provides a list of every package in the system



<b>Setting http-proxy</b>
- alt 1: Temporary proxy session:
  # export http_proxy=http://username:password@yourproxyaddress:proxyport

- alt 2: APT configuration file method 
  Add next line to /etc/apt/apt.conf 
  | Acquire::http::Proxy "http://yourproxyaddress:proxyport";


<b>Developer commands:</b>
$ apt-get build-dep <package_name> 
</pre>
</pre>


  <a href="https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management"><code>apk</code></a> (Alpine/"Docker")
<pre xxxsmall zoom>
Because Alpine Linux is designed to run from RAM, package management involves two phases:

- Installing / Upgrading / Deleting packages on a running system.
- Restoring a system to a previously configured state 
  (e.g. after reboot), including all previously installed packages 
  and locally modified configuration files. (RAM-Based Installs Only)

- apk is the tool used to install, upgrade, or delete software on a running sytem.
- lbu is the tool used to capture the data necessary to 
  restore a system to a previously configured state.


Contents

    1 Overview
    2 Packages and Repositories
        2.1 Repository pinning
        2.2 Commandline repository options
    3 Update the Package list
    4 Add a Package
    5 Add a local Package
    6 Remove a Package
    7 Upgrade a Running System
    8 Search for Packages
    9 Information on Packages
        9.1 apk info
            9.1.1 Listing installed packages
        9.2 apk policy
    10 Additional apk Commands
    11 Local Cache
    12 Enabling Local Cache
        12.1 To enable Local Cache on releases prior v2.3
        12.2 To manually enable Local Cache on HDD install
        12.3 To manually enable Local Cache on run-from-RAM installs
    13 Cache maintenance
        13.1 Delete old packages
        13.2 Download missing packages
        13.3 Delete and download in one step
        13.4 Automatically Cleaning Cache on Reboot
    14 Using the Local Cache with tmpfs volumes
    15 Advanced APK Usage
        15.1 Holding a specific package back
    16 Troubleshooting
        16.1 "apk-tools is old"

Overview

apk: 
  add      Add new packages to the running system
  del      Delete packages from the running system
  fix      Attempt to repair or upgrade an installed package
  update   Update the index of available packages
  info     Prints information about installed or available packages
  search   Search for packages or descriptions with wildcard patterns
  upgrade  Upgrade the currently installed packages
  cache    Maintenance operations for locally cached package repository
  version  Compare version differences between installed and available packages
  index    create a repository index from a list of packages
  fetch    download (but not install) packages
  audit    List changes to the file system from pristine package install state
  verify   Verify a package signature
  dot      Create a graphviz graph description for a given package
  policy   Display the repository that updates a given 
           package, plus repositories that also offer the package
  stats    Display statistics, including number of 
           packages installed and available, number of 
           directories and files, etc.
  manifest Display checksums for files contained in a given package


- Software packages for Alpine Linux are digitally signed 
  tar.gz archives with ".apk" extension (often called "a-packs")
  stored in one or more repositories (directory with a collection of *.apk files
  and a APKINDEX.tar.gz index )

- The list of repositories to check is stored in /etc/apk/repositories
  (one repo per line)
  Ex:
  /media/sda1/apks
  http://nl.alpinelinux.org/alpine/v3.7/community
  <b blue>@edge</b> http://nl.alpinelinux.org/alpine/edge/main
  <b blue>@edgecommunity</b> http://nl.alpinelinux.org/alpine/edge/community
  <b blue>@testing</b> http://nl.alpinelinux.org/alpine/edge/testing
  ^ "tagged" repo
    will be used like
    # apk add stableapp newapp@<b blue>edge</b> bleedingapp@<b blue>testing</b>
    by default only untagged repositories are used

Update the Package list
  # apk update (Fill catch locally the latest APKINDEX.tar.gz from remote repos)

Add Packages (transitive dependencies is automatic):
  # apk add openssh openntp vim

Add a packager from dev. repository (dangerous):
  # apk add cherokee --update-cache \
    --repository http://dl-3.alpinelinux.org/alpine/edge/testing/
    --allow-untrusted

Add local Package
  # apk add --allow-untrusted /path/to/file.apk


Remove a Package

  # apk del openssh

Upgrade a Running System

  # apk update
  # apk upgrade

To upgrade only a few packages, use the add command with the -u or --upgrade option:

apk update
apk add --upgrade busybox 
Note: Remember that when you reboot your machine, the remote repository will not be available until after networking is started. This means packages newer than your local boot media will likely not be installed after a reboot. To make an "upgrade" persist over a reboot, use a local cache.
Search for Packages


Search:
 # apk search -v        ← list all packages along with descriptions
 # apk search -v 'acf*' ← list all packages part of the ACF system
 # apk search -v --description 'NTP' ← list all packages that list NTP as 
                                       part of their description, 

Information on Packages
  # apk info -a zlib ← -w: show just webpage info
                       -a: show          all info
</pre>

  <a TODO href="https://www.flatpak.org/">FlatPak</a>
<pre xxxsmall zoom>
  contanerized multi-distro software package manager
</pre>
</td>
</tr>
</table>
<br/>
Storage
<table>
<td colsep>Basic&nbsp;FS.Mng</td>
<td>
  Moving around FS
<pre xxxsmall zoom>
<b>Display current working directory</b>
$ pwd  # pwd: "P"rint "W"orking "D"irectory
<b>Change working directory</b>
$ cd /home/myUser/Dowload
$ pushd . # Store current dir. in stack
$ cd      # With no args move to $HOME directory
$ cd ~    # ~ is alias for $HOME
$ cd ..   # Change to parent directory
$ popd    # Move to last dir. in stack (and remove from stack)

<b>(ls "file or directory") List "file" or "files in directory"</b>
With no file or directory list info in current working directory
ls -l :  ("long"), show permissions, file size, modification date, ownership
ls -a :  ("all" ), shows also hidden files (files/dirs starting with ".") 
ls -d :  Show only directory entry, do NOT display directory contents 
ls -F :  ("format") append helper graphical symbol like '*' for executable files
ls -S :  sort by size output in descending order
ls -R :  ("recursive") list recursively children directories
</pre>

  handling files/dirs
<pre xxxsmall zoom>
<b>Create directory</b>
mkdir -p /home/myUser/work/project01 # -p: will create any non-existing directory up to project01
                                     #     Withouth -p mkdir will fail if /home/myUser/work does not exists

<b>Moving files</b>
$ mv /my/path01/myFile1  /my/path02/   # move myFile1 from path01 to path02 directory
<b>Rename file</b>
$ mv myOriginalName  myFinalName   # use the mv (move) command to rename
<b>copying files</b>
$ cp    /tmp/fileToCopy       /home/myUser/ # copy to destination directory
$ cp -R /tmp/directoryToCopy  /home/myUser/ # copy recursively (-R) directory to destination directory


<b>Remove files/directories</b>
$ rm -f file       # force (-f) file deletion.  The -f option is useful for automated scripts

                   # Be very careful with recursive deletions. Specially as root user

<b>Securely remove files by overwriting first</b>
$ shred -n 2 -z -v /tmp/myImportantSecret  # -n 2: Overwrite twice with random data
                                           # finally write over with zeroes (-z)
                                           # -v: Show progress

<a href="https://linux.die.net/man/1/shred">shred</a> prevent data from being recovered by software (and most probably hardware)
</pre>

  Hard/Soft Links
<pre xxxsmall zoom>
In unix an "inode" is the low-level structure that stores the physical location in disk of
a given file. This "inode" is not visible to the user. 
The user visible entities are the file system paths, that point to the real inodes.

When a new file is created we have an structure similar to:

   /my/file/path  (points to)  → inode   (points to) →  physical_block_on_disk
   ^visible in              ^invisible to               ^managed by hard-disk
    user shells,             users, managed              internal circuitry
    GUI explorers,...        by the OS kernel            (or networks NAS)

* We can create <b>symbolic links</b> to a file_path, that are "sortcuts" to access
  a file_path from another file_path like (ln -s):
  
     $ ln -s /my/file/path   /my/newLocation/file/path/symLink
     
      This will leave and structure similar to:
     
        /my/file/path   →  inode   →  physical_block_on_disk
         ↑
         │
        /my/newLocation/file/path/symLink
     
     If the original /my/file/path is deleted or moved the links is broken.
  
  
* A hard-link can also be created pointing direcly to the inode, like (ln):
     $ ln /my/file/path   /my/newLocation/file/path2

      This will leave and structure similar to:
     
        /my/file/path   
                                    →  inode   →  physical_block_on_disk
        /my/newLocation/file/path2

The underlying inode will increase the number of references. The file data can be
accesses independently by the original /my/file/path or the new /my/newLocation/file/path2.
The inode (file) wil exists until all hardlinks or references are deleted.

</pre>

  Searching files/data
<pre xxxsmall zoom>
<b><a href="https://linux.die.net/man/1/find">find</a> files whose name ends in "html"</b>
Ex 1: "complex" find 
Command             |   Human explanation
(split into lines   |
 for readability)   |
------------------- +--------------------
$ find              |   let's display find results
   /var/lib         | ←     starting in this base directory and subdirectories
   -type f          | ← AND of type file (ignore directories, links, ...)
   -iname "*html"   | ← AND whose name matches *html (iname:ignore case, name: case sensitive)
   -mmin  -30       | ← AND whose modification time is 30 or less (-30) minutes (-mmin)
   -msize +20k      | ← AND whose size (msize) is 20kilobytes or more (+20)

Ex 2: find files and <b orange>for each found file execute</b> a <b blue>given command (grep ...)</b>
$ find 
   /var/lib
   -type f
   -iname "*html"
   <b orange>-exec</b> <b blue>grep "myStyleSheet.css"</b> <b orange>{} \;</b>
</pre>

   Basic file-audit<br/>
   <span xsmall>(also last execution for executable files)</span>
<pre xxxsmall zoom>
$ stat myFile.txt   
    File: myFile.txt
    Size: 5557            
  Blocks: 16
IO Block: 4096   regular file
  Device: 800h/2048d      
   Inode: 1554743
   Links: 1
  Access: (0644/-rw-r--r--)
     Uid: ( 1000/ earizon)
     Gid: ( 1000/ earizon)
  <b>Access: 2018-09-12 14:31:06.882431359 +0000</b>
  <b>Modify: 2018-08-29 16:15:31.000000000 +0000</b>
  <b>Change: 2018-09-12 14:31:06.882431359 +0000</b>
 Birth: -

For an executable access will show when it was last executed
</pre>

</td>
<td>
Check disk free/used space:
<pre xxxsmall zoom>
<a href="https://linux.die.net/man/0/df">man 1 df</a> 
$ df -kh  # Check free space

<a href="https://linux.die.net/man/1/du">man 1 du</a>
$ du -sch dir1 dir2  # check "d"isk "u"ssage of dir1 dir2
</pre>

Monitor files open:
<pre xxxsmall zoom>
<a href="https://linux.die.net/man/8/lsof">man 8 lsof</a>
# list files open by process "pID"
$ sudo lsof -p <b orange>511</b> #
(output can be similar ...)
COMMAND   <b orange>PID</b>  USER   FD      TYPE     DEVICE SIZE/OFF    NODE NAME
avahi-dae <b orange>511</b> avahi  cwd       DIR        8,1       67 1274283 /etc/avahi
avahi-dae <b orange>511</b> avahi  rtd       DIR        8,1       67 1274283 /etc/avahi
avahi-dae <b orange>511</b> avahi  txt       REG        8,1   136264 2568376 /usr/sbin/avahi-daemon
avahi-dae <b orange>511</b> avahi  DEL       REG        8,1          1713236 /usr/lib64/libnss_sss.so.2;5ae2fcc0
avahi-dae <b orange>511</b> avahi  DEL       REG        8,1          1390813 /usr/lib64/...
avahi-dae <b orange>511</b> avahi    0r      CHR        1,3      0t0    1028 /dev/null
avahi-dae <b orange>511</b> avahi    1u     unix 0xff222c00      0t0   20500 socket
avahi-dae <b orange>511</b> avahi    3u     unix 0xffb84400      0t0   18699 /var/run/avahi-daemon/socket
avahi-dae <b orange>511</b> avahi    7w     FIFO        0,8      0t0   20324 pipe
avahi-dae <b orange>511</b> avahi   11r  a_inode        0,9        0    7017 inotify
avahi-dae <b orange>511</b> avahi   12u     IPv4      21553      0t0     UDP *:mdns
avahi-dae <b orange>511</b> avahi   13u     IPv4      21554      0t0     UDP *:44720
avahi-dae <b orange>511</b> avahi   14u  netlink                 0t0   21555 ROUTE
...

# list processes using any file in etc
$ sudo lsof <b orange>/etc/*</b>
(output can be similar ...)
COMMAND     PID      USER   FD   TYPE DEVICE SIZE/OFF      NODE NAME
avahi-dae   511     avahi  cwd    DIR    8,1       67 101274283 <b orange>/etc/</b>avahi
avahi-dae   511     avahi  rtd    DIR    8,1       67 101274283 <b orange>/etc/</b>avahi
java      41043 azureuser  296r   REG    8,1      393       154 <b orange>/etc/</b>os-release
java      41043 azureuser  297r   REG    8,1      393       154 <b orange>/etc/</b>os-release
</pre>
Monitor file/dir. access
<pre xxxsmall zoom>
<a href="https://linux.die.net/man/1/inotifywait">man 1 inotifywait</a>: Wait for changes, then execute someCommand
while  true ; do
    inotifywait -q -e modify fileToMonitor1 fileToMonitor2 ... ;  someCommandToExecute ; done
done

<a TODO href="https://linux.die.net/man/1/inotifywatch">man 1 inotifywatch</a>: gather filesystem access statistics
</pre>

List block devices
<pre xxxsmall zoom>
[root@localhost earizon]# lsblk
NAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda               8:0    0 223.6G  0 disk
├─sda1            8:1    0     1G  0 part /boot
└─sda2            8:2    0 222.6G  0 part
  ├─fedora-root 253:0    0    50G  0 lvm  /
  ├─fedora-swap 253:1    0   7.8G  0 lvm  [SWAP]
  └─fedora-home 253:2    0 164.8G  0 lvm  /home
sdb               8:16   0 931.5G  0 disk
└─sdb1            8:17   0 931.5G  0 part /mount/disk01
sdd               8:48   0 465.7G  0 disk
└─sdd1            8:49   0 465.7G  0 part
mmcblk0         179:0    0  29.7G  0 disk
└─mmcblk0p1     179:1    0  29.7G  0 part /run/media/myUser/C852-6915
</pre>

</td>
<td colsep></td>
<td colsep>Block&nbsp;Storage</td>
<td>
<a href="https://en.wikipedia.org/wiki/Device_mapper">Device mapper</a>

<pre xxxsmall zoom>
device mapper: kernel framework mapping virtual block devices to 
               one (or more) physical block device (with the posibility
               to process&amp;filter in/out data)

<b>DEVICE-MAPPER ARCHITECTURE:</b>
<b>
 MAPPED DEVICE               │           MAPPING TABLE              │ TARGET DEVICE</b>
 (LOGICAL DRIVE)             │                                      │ PLUGIN (INSTANCE/s)
                             │                                      │
 logical device provided by  │ entry1:                              │ - filters
 device-mapper driver.       │ mapped-device1    ←→ target-device1  │ - access physical devices
 It provides an interface to │ └─ start address   └┬─ start address │ 
 operate on.                 │                     └─ sector-length │ 
                             │ entry2:                              │ Example plugins:
 Ex:                         │ mapped-device2    ←→ target-device2  │ - mirror for RAID
 - LVM2 logical volumes      │ └─ start address   └┬─ start address │ - linear   for LVM2 
 - dm-multipath pseudo-disks │                     └─ sector-length │ - stripped for LVM2
 - "docker images"           │ entry3:                ^^^^^^^^^^^^^ │ - snapshot for LVM2
                             │ ....                   1sector = 512 │ - dm-multipath
                             │                                 bytes│
                             │  NOTE: 1 sector = 512 bytes          │

<b>DATA FLOW:</b>
                                         
App → (Data) → MAPPED DEVICE →          DEVICE MAPPER               →  TARGET-DEVICE     → Physical 
                                        Route to target                PLUGIN instance     Block Device
                                        based on:       
                                        - MAPPED-DEVICE 
                                        - MAPPING-TABLE                

- Data can be also modified in transition, which is performed, for example,
  in the case of device mapper providing disk encryption or simulation of
  unreliable hardware behavior.


<b>Features</b>
- The device mapper "touch" various layers of the Linux kernel's storage stack.

- Functions provided by the device mapper include linear, striped and error mappings,
  as well as crypt and multipath targets.

Exs.:
  - Two disks may be concatenated into one logical volume with a pair
    of linear mappings, one for each disk.

  - crypt target encrypts the data passing through the specified device,
    by using the Linux kernel's Crypto API.

<b>available mapping targets</b>
 - cache    : allows creation of hybrid volumes, by using solid-state drives 
              (SSDs) as caches for hard disk drives (HDDs)
 - crypt    : provides data encryption, by using kernel Crypto API
 - delay    : delays reads and/or writes to different devices (testing)
 - era      : behaves in a way similar to the linear target, while it keeps 
              track of blocks that were written to within a user-defined
              period of time
 - error    : simulates I/O errors for all mapped blocks      (testing)
 - flakey   : simulates periodic unreliable behaviour         (testing)
 - linear   : maps a continuous range of blocks onto another block device
 - mirror   : maps a mirrored logical device, while providing data redundancy
 - multipath: supports the mapping of multipathed devices, through usage of 
              their path groups
 - raid     : offers an interface to Linux kernel's software RAID driver (md)
 - snapshot : (and snapshot-origin) used for creation of LVM snapshots,
              as part of the underlying copy-on-write scheme
 - striped  : stripes the data across physical devices, with the number of
              stripes and the striping chunk size as parameters
 - thin     : allows creation of devices larger than the underlying 
              physical device, physical space is allocated only when
              written to
 - zero     : equivalent of /dev/zero, all reads return blocks of zeros,
              and writes are discarded

<b>kernel features and projects are built on top</b>
Note: user-space apps talk to the device mapper via <b>libdevmapper.so</b>
  which in turn issues ioctls to the /dev/mapper/control device node.

  - cryptsetup    : utility to setup disk encryption based on dm-crypt
  - dm-crypt/LUKS : mapping target providing volume encryption
  - dm-cache      : mapping target providing creation of hybrid volumes
  - dm-integrity  : mapping target providing data integrity, either
                    using checksumming or cryptographic verification,
                    also used with LUKS
  - dm-log-writes : mapping target that uses two devices, passing through
                    the first device and logging the write operations performed
                    to it on the second device
  - dm-verity     : validates the data blocks contained in a file system
                    against a list of cryptographic hash values, developed as 
                    part of the Chromium OS project
  - dmraid(8)     : provides access to "fake" RAID configurations via the 
                    device mapper
  - DM Multipath  : provides I/O failover and load-balancing of block devices
                    within the Linux kernel

                    - allows to configure multiple I/O paths between server nodes 
                      and storage arrays(separate cables|switches|controllers) 
                      into a single mapped/logical device.
                    
                    - Multipathing aggregates the I/O paths, creating a new device
                      that consists of the aggregated paths.


  - Docker        : uses device mapper to create copy-on-write storage for
                    software containers

  - DRBD          : Distributed Replicated Block Device

  - kpartx(8)     : utility called from hotplug upon device maps creation and deletion

  - LVM2          : logical volume manager for the Linux kernel

  - Linux version of TrueCrypt

  <a href="https://linux.die.net/man/8/dmsetup">man 8 dmsetup</a>

dmsetup — low level logical volume management

SYNOPSIS
  dmsetup clear device_name
  dmsetup create device_name [-u|--uuid uuid] [--addnodeoncreate|--addnodeonresume] [-n|--notable|--table {table|table_file}] [--readahead
           {[+]sectors|auto|none}]
  dmsetup deps [-o options] [device_name]
  dmsetup help [-c|-C|--columns]
  dmsetup info [device_name]
  dmsetup info -c|-C|--columns [--count count] [--interval seconds] [--nameprefixes] [--noheadings] 
               [-o fields] [-O|--sort sort_fields] [--sepa‐rator separator] [device_name]
  dmsetup load device_name [--table {table|table_file}]
  dmsetup ls [--target target_type] [--exec command] [--tree] [-o options]
  dmsetup mangle [device_name]
  dmsetup message device_name sector message
  dmsetup mknodes [device_name]
  dmsetup reload device_name [--table {table|table_file}]
  dmsetup remove [-f|--force] [--retry] [--deferred] device_name
  dmsetup remove_all [-f|--force] [--deferred]
  dmsetup rename device_name new_name
  dmsetup rename device_name --setuuid uuid
  dmsetup resume device_name [--addnodeoncreate|--addnodeonresume] [--noflush] [--nolockfs] [--readahead {[+]sectors|auto|none}]
  dmsetup setgeometry device_name cyl head sect start
  dmsetup splitname device_name [subsystem]
  dmsetup stats command [options]
  dmsetup status [--target target_type] [--noflush] [device_name]
  dmsetup suspend [--nolockfs] [--noflush] device_name
  dmsetup table [--target target_type] [--showkeys] [device_name]
  dmsetup targets
  dmsetup udevcomplete cookie
  dmsetup udevcomplete_all [age_in_minutes]
  dmsetup udevcookie
  dmsetup udevcreatecookie
  dmsetup udevflags cookie
  dmsetup udevreleasecookie [cookie]
  dmsetup version
  dmsetup wait [--noflush] device_name [event_nr]
  dmsetup wipe_table device_name [-f|--force] [--noflush] [--nolockfs]

  devmap_name major minor
  devmap_name major:minor
</pre>

</td>
<td>
  <b bgorange>Logical Volume Manager</b>
<pre xxxsmall zoom>
WARN: You need LVM tools installed. Mount alone is not able to mount LVM volumes

<a href="https://opensource.com/article/18/11/manage-storage-lvm">REF</a>

<b orange>
 physical drive 1                                     logical-volume 1
 physical drive 2    N <-> 1    Volume-Group 1 <-> M  logical-volume 2
 physical drive 3                                     logical-volume 3
 ...                                                  ...
                     ^^^^^^^                 ^^^^^^^ 
                     The Vol.Group           The Vol.Group
                     acts as a logical       allows to dynamically
                     pool of physical        create logical volumes
                     devices                 isolated from the physical
                                             resources
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 
                     The Volume-Group acts as a relation
                     N-to-M between physical resources
                     and logical partitions
</b>

Quick Sumary:
    CREATE       → ADD Physical drives → Add logical-volumes
    VOLUME-GROUP   to VOLUME-GROUP       to VOLUME-GROUP.
                                         ^^^^^^^^^^^^^^^^^^
                                         logical-volumes can map to:
                                         - company deparments
                                         - test/pre/pro enviroment
                                         - ...


<b>PRE-SETUP:</b>
Format a <b brown>physical drive /dev/sdx</b> to be included on the pool
# dd if=/dev/zero of=<b orange>/dev/sdx</b> count=8196
# parted /dev/sdx print | grep Disk
Disk /dev/sdx: 100GB
# parted /dev/sdx mklabel gpt
# parted /dev/sdx mkpart primary 1s 100%

Note: Volume-group is synonym of "physical-storage pool"

<b>STEP 1: Create an LVM pool (and add a first physical disk to it)</b>
NOTE: Usually, you don't have to set up LVM at all since most distros 
     defaults to creating a virtual "pool" of storage and adding your
     machine's hard drive(s) to that pool.

# vgcreate volgroup01 /dev/sdx1  # ← create new storage pool and
                                      aggregate disk-partition 


<b>STEP 2: Create logical-volumes</b>
# lvcreate volgroup01 49G --name vol0  # ← create logical-volume /dev/volgroup01/vol0
# lvcreate volgroup01 49G --name vol1  # ← create logical-volume /dev/volgroup01/vol1


<b>STEP 4: Switch volume-group online</b>
# vgchange --activate y volgroup01

<b>STEP 5: make the file systems</b>
# mkfs.ext4 -L finance    /dev/volgroup01/vol0
# mkfs.ext4 -L production /dev/volgroup01/vol1
            ^^^^^^^^^^^^^ 
            label the drive
            In this case a logical-volume is used by department

<b>STEP 6: Mount the volumes</b>
# mount /dev/volgroup01/vol0 /mnt/vol0
# mount /dev/volgroup01/vol1 /mnt/vol1

<b>STEP 7: Adding space to the volume-group</b>
# part /dev/sdy mkpart primary 1s 100%  # ← create partition on new physical-disk
# vgextend volgroup01 /dev/sdy1         # ← aggregate to volgroup01 
# lvextend -L +49G /dev/volgroup01/vol0 # ← Extend the already-existing logical-volume
</pre>

  <span xsmall>Show LVM layout</span>
<pre xxxsmall zoom>
<b>vgdisplay</b> shows info about volume groups: 
# vgdisplay
  --- Volume group ---
  VG Name               volgroup01
  System ID            
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  4
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                3
  Open LV               3
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               <237.47 GiB
  PE Size               4.00 MiB
  Total PE              60792
  Alloc PE / Size       60792 / <237.47 GiB
  Free  PE / Size       0 / 0  
  VG UUID               j5RlhN-Co4Q-7d99-eM3K-G77R-eDJO-nMR9Yg

<b>lvdisplay</b> shows info about logical volumes: 

# lvdisplay
  --- Logical volume ---
  LV Path                /dev/volgroup01/finance
  LV Name                finance
  VG Name                volgroup01
  LV UUID                qPgRhr-s0rS-YJHK-0Cl3-5MME-87OJ-vjjYRT
  LV Write Access        read/write
  LV Creation host, time localhost, 2018-12-16 07:31:01 +1300
  LV Status              available
  # open                 1
  LV Size                149.68 GiB
  Current LE             46511
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:3

[...]
</pre>

<span xsmall>Show PVs,VGs,LVs</span>
<pre xxxsmall zoom>
<b># pvs # ← Physical Volumes</b>
  PV         VG     Fmt  Attr PSize    PFree
  <b orange>/dev/sda2</b>  fedora lvm2 a--  <222.57g    0



<b># vgs # ← Volume Groups </b>
  VG     #PV #LV #SN Attr   VSize    VFree
  <b orange>fedora</b>   1   3   0 wz--n- <222.57g    0
<b># lvs # ← Logical Volumes</b> 
  LV   VG     Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  <b orange>home</b> fedora -wi-ao---- <164.82g
  <b orange>root</b> fedora -wi-ao----   50.00g
  <b orange>swap</b> fedora -wi-ao----    7.75g
</pre>

  <span xsmall>Mount in rescue-mode</span>
<pre xxxsmall zoom>
PRE-SETUP:
- the LVM toolchain must ready for use in the rescue-mode environment.
   (/usr/sbin directory mounted or similar)

STEPS:
# vgchange --activate y
(output will be similar to)
→ 2 logical volume(s) in volume group "volgroup01" now active
# mkdir /mnt/finance
# mount /dev/volgroup01/finance /mnt/finance
</pre>


  <span xsmall>LVM + LUKS encryption</span>
<pre xxxsmall zoom>
- LUKS stands for Linux-Unified-Key-Setup encryption toolchain.
- LVM  integrates nicely with disk encryption

LUKS encrypts full-partitions (vs files  in GnuPG, ...)

NOTICE/WARN: LUKS will prompts for a password during boot.
             (server-autoboot will fail)


<b>STEP 1: format the partition with the "cryptsetup" command</b>
# cryptsetup luksFormat /dev/sdx1
→ LUKS will warn that it's going to erase your drive: (Accept to continue)
→ A prompt will ask for a passphrase: (Enter it to continue)

The partition is encrypted at this point but no filesystem is in yet:
- In order to partition it you must un-lock it.

# cryptsetup luksOpen /dev/sdx1 mySafeDrive # ← Unlock before formating it.
                                ^^^^^^^^^^^ 
                                human-friendly name
                                will create a symlink
                                /dev/mapper/mySafeDrive
                                to auto-generated designator

→ LUKS will ask for the passphrase to un-lock the drive: (Enter it to continue)

- Check the volume is "OK":
# ls -ld /dev/mapper/mySafeDrive
→ lrwxrwxrwx. 1 root root 7 Oct 24 03:58 /dev/mapper/mySafeDrive -> ../dm-4

<b>STEP 2: format with standard-filesystem (ext4,...)</b>
# mkfs.ext4 -o Linux -L mySafeExt4Drive /dev/mapper/mySafeDrive

<b>STEP 3: Mount the unit</b>
# mount /dev/mapper/mySafeExt4Drive /mnt/hd
</pre>

  <span xsmall>Increase LV Size</span>
<pre xxxsmall zoom>
<b>STEP 1: Mark physical disk partition as LVM:</b>
# fdisk -cu /dev/sdd
→ new partition ("n")
  → primary partition ("p")
    → Enter partition number (1-4)
      → Change type ("t") to Linux LVM  ("8e")
        → Check status ("p")
          → Write changes ("c")


<b>STEP 2: create new PV (Physical Volume)</b>
# pvcreate /dev/sdd1
  → Verify the pv:
    # pvs

<b>STEP 3: Extending Volume-Group</b>
# vgextend vg_tecmint /dev/sdd1
  → Verify it:
  # vgs


<b>STEP 4: Increar Logical Volume Size</b>
4.1: Check available free space in volume-group:
# vgdisplay | grep -i "Free"
 → Free  PE / Size       4607 / 18GB  
                         ^^^^^^^^^^^  
                         max size a logical volume
                         can be extended to.

4.2: Extend the volume:
# lvextend -l +4607 /dev/vg_tecmint/LogVol01
              ^
              Use '+' to add the more space.

4.3: Check changes:
# lvdisplay

<b>STEP 5: re-size the file-system</b>
# resize2fs /dev/vg_tecmint/LogVol01
</pre>
</pre>


</td>
<td colsep></td>
<td colsep>File&nbsp;Systems</td>
<td>
  EXT4
<pre xxxsmall zoom>
Features:
- metadata and journal checksums.
- timestamps intervals down to nanoseconds.
- EXT4 extents: described by its starting and ending place on the hard drive.
  EXT4-Extents make possible to describe very long, physically contiguous files in 
  a single inode pointer entry significantly reducing the number of pointers in large files.
- New anti-fragmentation algorithms.
</pre>

Checks error|fragmentation:
<pre xxxsmall zoom>
(ussually it must be really low in EXT"N" filesystems):
WARN: Be sure to use the -n flag, preventing fsck to take any action on the file-system

# fsck -fn /dev/sda
→ ...
→ ...
/dev/sda: 613676/3040000 files (<b>0.3% non-contiguous</b>), 6838740/12451840 blocks
</pre>

  Journal
<pre xxxsmall zoom>
- for performance reasons, we do not want to write or sync every change to ext4.
- If the system crashes meanwhile, the changes that are not written to ext4 will
  be lost if Journal is not enabled.
- Every write/sync operation is written to Journal first (not to ext4 first)
  and it is finalized later (written to ext4 later). If the system crashes,
  during recovery, probably on the next boot, Journal is replied back to ext4
  so changes are applied and not lost.

Journal can be used in three different modes (mount option):

  - journal:   All data (both metadata and actual data) is written to Journal first, so the safest.
  - ordered:   This is the default mode. All data is sent to ext4, metadata is sent to Journal also.
               No protection for data but metadata is protected for crash.
  - writeback: Data can be written to ext4 before or after being written to Journal. 
               On a crash, new data may get lost.

The information / blocks is written to Journal following next sequence:

 1.- A Descriptor Block is written, containing the information about the
     final locations of this operation.
 2.- A Data Block is written.(real data or meta data)
 3.- A Commit Block is written. After this, the data can be sent to ext4.
     (Alternatively a Revocation Block will cancel)
     If commit-block is not found, when a replay happens (crash-recovery, ...),
     data will not be written to ext4.
</pre>

 Tunning performance
<pre xxxsmall zoom>
<a href="https://www.kernel.org/doc/Documentation/filesystems/ext4.txt">REF: Kernel.org doc</a>
   Contains full list of mount opts, /proc&amp;/sys entries

Mount options:
journal_async_commit    Commit block can be written to disk without waiting
            for descriptor blocks. If enabled older kernels cannot
            mount the device. This will enable 'journal_checksum'
            internally.


commit=nrsec    (*) Ext4 can be told to sync all its data and metadata
            every 'nrsec' seconds. The default value is 5 seconds.
            This means that if you lose your power, you will lose
            as much as the latest 5 seconds of work (your
            filesystem will not be damaged though, thanks to the
            journaling).  This default value (or any low value)
            will hurt performance, but it's good for data-safety.
            Setting it to 0 will have the same effect as leaving
            it at the default (5 seconds).
            Setting it to very large values will improve
            performance.

inode_readahead_blks=n  This tuning parameter controls the maximum
            number of inode table blocks that ext4's inode
            table readahead algorithm will pre-read into
            the buffer cache.  The default value is 32 blocks.

stripe=n    Number of filesystem blocks that mballoc will try
            to use for allocation size and alignment. For RAID5/6
            systems this should be the number of data
            disks *  RAID chunk size in file system blocks.

min_batch_time=usec This parameter sets the commit time (as
            described above) to be at least min_batch_time.
            It defaults to zero microseconds.  Increasing
            this parameter may improve the throughput of
            multi-threaded, synchronous workloads on very
            fast disks, at the cost of increasing latency.



</pre>


</td>

<td TODO>
  XFS
</td>

<td>
  <a TODO href="https://opensource.com/article/18/4/stratis-lessons-learned">Stratis (by RedHat)</a>
  <a xxsmall href="https://stratis-storage.github.io/StratisSoftwareDesign.pdf">REF Design(PDF)</a>
  <br/>
</td>
<td colsep></td>
<td colsep>Encrypt.</td>
<td>
  Storage encryption 
  <ul xxxsmall>
  <li>can be performed at the file system level or the block level.</li>
  <li>Linux file system encryption options include eCryptfs and EncFS, while 
    FreeBSD uses PEFS.</li>
  <li>Block level or full disk encryption options include dm-crypt + LUKS on 
    Linux and GEOM modules geli and gbde on FreeBSD.</li>
  </ul>
  <hr xxxsmall />
  <a href="https://www.veracrypt.fr/en/Home.html">VeraCrypt</a>
<pre xxxsmall zoom>
  free open source disk encryption software for Windows/OSX/Linux.
  <a href="https://www.idrix.fr">Developed by IDRIX</a>, based on
  TrueCrypt 7.1a. main features:
  - Creates virtual encrypted disk within a file and mounts
    it as a real disk.
  - Encrypts entire partition or storage device
  - Encrypts a partition or drive where Windows is installed
    (pre-boot authentication).
  - Encryption is automatic, real-time(on-the-fly) and transparent.
  - Parallelization and pipelining allow data to be read
    and written as fast as if the drive was not encrypted.
  - Encryption can be hardware-accelerated on modern processors.
  - Provides plausible deniability, in case an adversary forces 
    you to reveal the password: Hidden volume (steganography)
    and hidden operating system.
</pre>
</td>

</tr>
</table>
<br/><br/>
<table>
<tr>
<td colsep>kernel&nbsp;monit</td>
<td>
  Monitoring Diagram
<pre xxxsmall zoom bgorange>
  ↑        ┌───────────────────────────────────────────────────────────────────┐
  │        │                    APPLICATIONS                                   |
  │        ├───────────────────────────────────────────────────────────────────┤
  │        │[ltrace][ldd]      System Libraries [gethostlatency]         [perf]│
  │        ├───────────────────────────────────────────────────────────────────┤
  │        │[strace][sydgid]     System Call Interface [*3]              [perf]|     CPU
[perf]   ↑ ├─────────────────┬───────┬──────────────┬───────┬──────────────────┤Interconnect  ┌──────────┐
[dtrace] │ │ VFS             │       │SOCKETS  [ss] │       │SCHEDULER   [perf]├──────────────┤CPU1[perf]├───┐
[stap]  L K├─────────────────┤       │──────────────┤       │[perf][latencytop] ←───[top]     └──────────┘   │
[lttnp] I E│ FILE SYSTEMS    │       │TCP/UPD  [*2] │       │[mpstat]          │  ╱[ps]      Memory│         │
[ktap]  N R├─────────────────┤       │──────────────┤       ├──────────────────┤ ╱ [pidstat]    BUS│  [perf] │
  │     U N│ VOLUME MANAGERS │       │IP            │       │VIRTUAL MEMORY    │╱                  │         │
  │     X E├─────────────────┤       │[ip]          │       │[vmstat]          ←                 ┌───┐       │
  │       L│ BLOCK DEVICE    │       │[route]       │       │[slabtop]         ├────────────────→│RAM│       │
  │      │ │ Interface       │       │[iptables]    │       │[free]            │                 └───┘       │
  │      │ │ [*1] [pidstat]  │       │              │       │[memleak]         │                 [numastat]  │
  │      │ │                 │       │              │       │[comkill]         │                 [lstopop]   │
  │      │ │                 │       │              │       │[slabratetop]     │                             │
  │      │ │                 │       │──────────────│       ├──────────────────┤                             │
  │      │ │                 │       │Ethernet [ss] │       │CLOCKSOURCE       │                             │
  │      │ │                 │       │[tcpdump]     │       │[/sys/...]        │                             │
  │      │ ├─────────────────┴───────┴──────────────┴───────┴──────────────────┤                             │
  │      │ │                       Device Drivers                              │                             │
  ↓      ↓ └───────────────────────────────────────────────────────────────────┘             I/O [perf]      │
                           Expander-Interconnect                           ┌──────────┐     BUS  [tiptop]    │
                   ─┬────────────────────────────────────────┬─────────────┤I/O Bridge├──────────────────────┘
                    │                                        │             └──────────┘
                    │                                        │            
              ┌─────┴───────────┐                    ┌───────┴───────────┐[nicstat]     
              │I/O Controller *1│                    │ Network Controller│[ss]
              └─────────────────┘                    └───────────────────┘[ip]
             ┬──────┴───────┬                         ┬──────┴────┬      
             │              │                         │           │
            Disk[*1]       Swap [swapon]             Port        Port 
                                                     [ping] [traceroute]
                                                     [ethtool] [snmpget]
                                                     [lldptool] 
           
*1: [iostat] [iotop] [blktrace]
*2: [tcptop] [tcplife] [tcpconnect] [tcpaccept] [tcpconnlat] [tcpretrans]
*3: [opensnoop] [statsnoop] [syncsnoop]

OTHERS: [sar] [dstat] [/proc]

 ┌───┐[sar -m FAN]       ┌────────────┐[ipmitool]
 │FAN│                   │POWER SUPPLY│[dmidecode]
 └───┘                   └────────────┘


</pre>

<a TODO href="http://www.brendangregg.com/blog/2015-07-08/choosing-a-linux-tracer.html">Tracer comparative</a>
</td>
<td>
  <span xsmall>ltrace: Library Call Tracer</span>
<pre xxxsmall zoom>
<a href="https://linux.die.net/man/1/ltrace">man 1 ltrace</a>
Summary:<b>
ltrace                           | ltrace -c  # ← Count time and calls for each library call
                                                   and report a summary on program exit.</b>
  [-e filter|-L]                 |   [-e filter|-L] 
  [-l|--library=library_pattern] |   [-l|--library=library_pattern] 
  [-x filter]                    |   [-x filter] 
  [-S]                           |   [-S] 
  [-b|--no-signals]              |   
  [-i] [-w|--where=nr]           |   
  [-r|-t|-tt|-ttt]               |   
  [-T]                           |   
  [-F pathlist]                  |
  [-A maxelts]                   |
  [-s strsize]                   |
  [-C|--demangle]                |
  [-a|--align column]            |
  [-n|--indent nr]               |
  [-o|--output filename]         |   [-o|--output filename] 
  [-D|--debug mask]              |
  [-u username]                  |
  [-f]                           |   [-f] 
  [-p pid]                       |   [-p pid] 
  [[--] command [arg ...]]       |   [[--] command [arg ...]]



runs the specified command until it exits, 
intercepting/recording:
   + dynamic library calls  by process
     - Display functions and funct.parameters.
     - external prototype libraries is needed 
       for human-readable output.
       (ltrace.conf(5), section PROTOTYPE LIBRARY DISCOVERY ) 

   + signals which received by process

   + system calls           by process



</pre>

  <span xsmall>strace: System call tracer</span>
<pre xxxsmall zoom>
<a href="https://linux.die.net/man/1/strace">man 1 strace</a>

SUMMARY
strace                    | strace -c  ← -c: Count time, calls, and errors 
                          |                  for each system call and report summary on exit.
                          |                  -f aggregate over all forked processes
  [ -dDffhiqrtttTvVxx ]   |   [ -D ] 
  [ -acolumn ]            |   
  [ -eexpr ] ...          |   [ -eexpr ] ... 
                          |   [ -Ooverhead ] 
  [ -ofile ]              |   
  [ -ppid ] ...           |   
  [ -sstrsize ]           |   
  [ -uusername ]          |
  [ -Evar=val ] ...       |
  [ -Evar ] ...           |   [ -Ssortby ] 
                          |   [ -Ssortby ] 
  [ command [ arg ... ] ] |   [ command [ arg ... ] ]


strace runs specified command until it exits intercepting:
  +  system calls called by a process
     - system-call-name + arguments + return-value is printed to STDERR (or -o file)
       Ex output:
       open("/dev/null", O_RDONLY) = 3
       open("/foo/bar", O_RDONLY) = -1 ENOENT (No such file or directory)
       
  +  signals    received by a process 
       Ex output:
       $ strace sleep 111
       → ...
       → sigsuspend([] <unfinished ...>
       → --- SIGINT (Interrupt) ---     ← Signal received
       → +++ killed by SIGINT +++

If a system call is being executed and meanwhile another one is being called 
from a different thread/process then strace will try to preserve the order
of those events and mark the ongoing call as being unfinished. 
When the call returns it will be marked as resumed. Ex. output:
  → [pid 28772] select(4, [3], NULL, NULL, NULL &lt;<b>unfinished ...&gt;</b>
  → [pid 28779] clock_gettime(CLOCK_REALTIME, {1130322148, 939977000}) = 0
  → [pid 28772] <b>&lt;... select resumed&gt;</b> )      = 1 (in [3])

Interruption of a (restartable) system call by a signal delivery is
processed differently as kernel terminates the system call and also 
arranges its immediate reexecution after the signal handler completes.

read(0, 0x7ffff72cf5cf, 1)              = ? <b>ERESTARTSYS (To be restarted)</b>
--- SIGALRM (Alarm clock) @ 0 (0) ---
rt_sigreturn(0xe)                       = 0
read(0, ""..., 1)                       = 0

</pre>


  <span xsmall>mpstat<br/>CPU stats <br/>(interr., hypervisor...)</span>
<pre xxxsmall zoom>
<a href="https://linux.die.net/man/1/mpstat">mpstat</a>
Sumary
mpstat 
  [ -A ]                        ==  -I ALL -u -P ALL 
  [ -I { SUM | CPU | ALL } ]    ==  Report interrupts statistics
  [ -u ]                            Reports cpu utilization (default)
  [ -P { cpu [,...] | ON | ALL } ]  Indicates the processor number
  [ -V ] 
  [ secs_interval [ count ] ]
    secs_interval = 0 => Report from times system startup (boot)

mpstat writes to standard output activities for each available processor, 
Global average activities among all processors are also reported. 


- CPU output columns: 
%usr   :  executing at the user level (application).
%nice  :  executing at the user level with nice priority.
%sys   :  executing at the system level (kernel).
          It does NOT include time spent servicing hardware 
          and software interrupts.
%iowait:  idle during which the system had an outstanding disk I/O request.
%irq   :  time spent by the CPU or CPUs to service hardware interrupts.
%soft  :  time spent by the CPU or CPUs to service software interrupts.<b>
%steal :  time spent in involuntary wait by the virtual CPU or CPUs
           while the hypervisor was servicing another virtual processor.</b>
%guest : time spent by the CPU or CPUs to run a virtual processor.
%idle  : time that the CPU or CPUs were idle and the system did not have
         an outstanding disk I/O request.
</pre>


  <span xsmall>pidstat: Linux task stats</span>
<pre xxxsmall zoom>
<a href="https://linux.die.net/man/1/pidstat">man 1 pidstat</a>
Summary

pidstat 
  [ -C comm ]       ← Display only tasks whose command name includes the string comm
  [ -d ]            ← Report I/O statistics 
  [ -h ] 
  [ -I ] 
  [ -l ] 
  [ -p { pid [,...] | SELF | ALL } ] 
  [ -r ]            ← Report page-faults and memory ussage
  [ -t ]            ← Also display stats for associated threads  
  [ -T { TASK | CHILD | ALL } ] 
  [ -u ] 
  [ -V ] 
  [ -w ] 
  [ secs_interval [ count ] ]

- monitor individual tasks.
- Dumps to STDOUT activities for every task selected    (-p) 
   or for every        task managed by the Linux kernel (-p ALL)
   or for every active task managed by the Linux kernel ("no -p")
</pre>
</td>
<td>
  <span xsmall>iostat<br/>
  stats on CPU, device I/O, FSs</span>
<pre xxxsmall zoom TODO>
<a href="https://linux.die.net/man/1/iostat">man 1 iostat<a>

</pre>

  <span>iotop<br/>
  Simple top-like I/O monitor</span>
<pre xxxsmall zoom TODO>
<a href="https://linux.die.net/man/1/iotop">man 1 iotop<a>
</pre>

  <span xsmall>blktrace<br/>
  traces on block I/O traffic</span>
<pre xxxsmall zoom TODO>
<a href="https://linux.die.net/man/8/blktrace">man 8 blktrace</a>
</pre>
</td>
<td colsep> </td>
<td>
  <span bgorange>perf</span>
<pre xxxsmall zoom>
Performance counters for Linux are a new kernel-based subsystem
that provide a framework for all things performance analysis.

 It covers hardware level (CPU/PMU, Performance Monitoring Unit) 
features and software features (software counters, tracepoints) as well. 
</pre>

  <span xsmall bgorange>perf stat<br/>
  gather perf-counter stats</span>
<pre xxxsmall zoom>
<a href="https://linux.die.net/man/1/perf-stat">man 1 perf-stat</a>
SUMMARY

perf stat 
     [--event=EVENT]  ← PMU event in the form:
                        - symbolic event name (perf list to list)
                        - raw PMU event (eventsel+umask) with format 
                          rNNN where NNN is an hexadecimal event descriptor 
     [--no-inherit]   ← child tasks do not inherit counters 
     [--all-cpus]
     [--pid=<pid>]    ← comma separated list of existing processes
     [--tid=<tid>]    ← comma separated list of existing thread id 
     [--scale]        ← scale/normalize counter values 
     [--repeat=<n>]   ← repeat command, print average + stddev (max: 100) 
     [--big-num]      ← print large numbers with thousands-local-separator
     [--cpu=]         ← comma separated list of cpus (All if not provided)
     [--no-aggr]      ← Do not aggregate counts across all monitored CPUs
                        in system-wide mode (-a). Only valid in system-wide mode. 
     [--null]         ← don't start any counters 
     [--verbose]      ← show counter open errors, etc,...
     [--field-separator SEP]
     [--cgroup name]  ← monitor only the container (cgroup) called "name".
                        - Only available in per-cpu mode. The cgroup filesystem
                        must be mounted. All threads belonging to container "name"
                        are monitored when they run on the monitored CPUs. 
                        - Multiple cgroups can be provided. Each cgroup is applied 
                        to the corresponding event, i.e., first cgroup to first event, 
                        - It is possible to provide an empty cgroup 
                          (monitor all the time) using, e.g., -G foo,,bar.
                        - Cgroups must have corresponding events, i.e., they always
                          refer to events defined earlier on the command line. 
     [--output file]
     [--append]
     [--log-fd]      ← append to given fd instead of stderr.
     (-)
     <command> [<options>]
     ^^^^^^^^^ 
     Any command you can specify in a shell. 

<b>example:</b></b>
$ perf stat - make -j
    Performance counter stats for 'make -j':

    8117.370256  task clock ticks     #      11.281 CPU utilization factor
            678  context switches     #       0.000 M/sec
            133  CPU migrations       #       0.000 M/sec
         235724  pagefaults           #       0.029 M/sec (page faults)
    24821162526  CPU cycles           #    3057.784 M/sec
    18687303457  instructions         #    2302.138 M/sec
      172158895  cache references     #      21.209 M/sec
       27075259  cache misses         #       3.335 M/sec

    Wall-clock time elapsed:   719.554352 msecs
</pre>

  <span xsmall>perf top<br/>
  real-time system profiling</span>
<pre TODO xxxsmall zoom>
<a href="https://linux.die.net/man/1/perf-top">man 1 perf-top</a>
</pre>
</td>
<td>
  <span xsmall>perf record<br/>
  record command's profile into perf.data</span>
<pre TODO xxxsmall zoom>
<a href="https://linux.die.net/man/1/perf-record">man 1 perf-record</a>
</pre>

  <span xsmall>perf report<br/>
  display recorded perf.data</span>
<pre TODO xxxsmall zoom>
<a href="https://linux.die.net/man/1/perf-report">man 1 perf-report</a>
</pre>

  <span xsmall>perf list<br/>
  List (sym.) event types</span>
<pre TODO xxxsmall zoom>
<a href="https://linux.die.net/man/1/perf-list">man 1 perf-list</a>
</pre>
</td>
<td>
  <span xsmall>perf bench<br/>
  General framework for bench.suites</span>
<pre TODO xxxsmall zoom>
<a href="https://linux.die.net/man/1/perf-bench">man 1 perf-bench</a>
</pre>

  <span xsmall bgorange>perf lock<br/>
  Analyze lock events</span>
<pre TODO xxxsmall zoom>
<a href="https://linux.die.net/man/1/perf-lock">man 1 perf-lock</a>
</pre>
</td>
<td colsep> </td>
<td>
  <span TODO>dtrace</span>
<pre xxxsmall zoom>
</pre>

  <span TODO>stap</span>
<pre xxxsmall zoom>
</pre>
</td>

</tr>
</table>


</body>
<!--

____________________________________________


______________________
TODO: HHRR
Ioannis Koustoudis: LFCS­Linux sysadmin from Kavala, Greece. He works for the ministry of education and supports almost 200 school units in their infrastructure. If he is not in front of a computer screen, he plays music (he is a multi­-instrumentalist) or take care of his two lovely kids.
_______________________
https://lists.gnu.org/archive/html/bug-coreutils/2008-09/msg00085.html
Re: Bug in date?
From:   James Youngman
Subject:    Re: Bug in date?
Date:   Tue, 9 Sep 2008 19:52:53 +0100

On Tue, Sep 9, 2008 at 1:47 PM, Enrique Arizón Benito
<address@hidden> wrote:

> Upps, I forgot it.
>
>  #date -s "1970-01-01 00:00:01"
>  date: cannot set date: Invalid argument

It looks like date is simply reporting an error that it received from
the operating system.   The strace utility (or some platform-specific
replacement if you are not using Linux) should be able to confirm
this.

>  Thu Jan 1 00:00:01 CET 1970
>
> Curiosly after the Invalid argument error, date properly prints the new hour
> but doesn't change it.

This is almost certainly because the date program understands the date
you refer to, but failed in its attempt to set the system clock to
that value.

> That's what makes me think it's really a bug.

More likely it's because CET is 1h ahead of UTC and therefore the time
you specified is before the Epoch.    But Eric already said that, so
perhaps you ruled out that possibility but did not say so.

James


_________________________

Turning on Linux ACLs:
   https://wiki.archlinux.org/index.php/Access_Control_Lists
_____________________
$ ps -efZ | grep sshd
system_u:system_r:sshd_t:s0-s0:c0.c1023 root 80388   1  0 Mar14 ?        00:00:00 /usr/sbin/sshd -D
system_u:system_r:sshd_t:s0-s0:c0.c1023 root 93132 80388  0 14:11 ?      00:00:00 sshd: azureuser [priv]
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 azureus+ 93137 93132  0 14:11 ? 00:00:00 sshd: azureuser@notty
_________________
https://upload.wikimedia.org/wikipedia/commons/3/30/IO_stack_of_the_Linux_kernel.svg
______________________
https://www.infoq.com/news/2015/02/under-hood-containers
_____________
https://en.wikipedia.org/wiki/Executable_and_Linkable_Format
___________
How to configure and Install Config Server Firewall & Login failure Daemon
https://techarena51.com/blog/how-to-configure-and-install-config-server-firewall-login-failure-daemon/?utm_source=devopswiki
_____________
https://grsecurity.net/
Grsecurity® is an extensive security enhancement to the Linux kernel that defends against a wide range of security threats through intelligent access control, memory corruption-based exploit prevention, and a host of other system hardening that generally require no configuration.

It has been actively developed and maintained for the past 17 years. Commercial support for grsecurity is available through Open Source Security, Inc


Comparation grsecurity SELinux AppArmor KSPP
   https://www.grsecurity.net/compare.php

https://en.wikipedia.org/wiki/Grsecurity#PaX
   """A major component bundled with grsecurity is PaX. Among other features, 
  the patch flags data memory, the stack, for example, as non-executable and 
  program memory as non-writable. The aim is to prevent memory from being 
  overwritten, which can help to prevent many types of security vulnerabilities,
  such as buffer overflows. PaX also provides address space layout randomization
  (ASLR), which randomizes important memory addresses to reduce the probability
  of attacks that rely on easily predicted memory addresses."""

Hardening Debian for the Desktop Using Grsecurity
   https://micahflee.com/2016/01/debian-grsecurity/
_________________
  "Real time" backup with rsync and bash:
  https://github.com/Leo-G/backup-bash
________
  Important files in /etc:
/etc/hosts
/etc/...
___________________
http://clusterlabs.org/
The ClusterLabs stack unifies a large group of Open Source projects related to High Availability into a cluster offering suitable for both small and large deployments. Together, Corosync, Pacemaker, DRBD, ScanCore, and many other projects have been enabling detection and recovery of machine and application-level failures in production clusters since 1999. The ClusterLabs stack supports practically any redundancy configuration imaginable.
__________________
https://www.linbit.com/en/products-and-services/drbd-sds/
High Availability for software-defined storage

The revolutionary Block Storage Replication tool. Companies implement 
oftware-defined storage (SDS) in order to easily provision and manage data 
storage, independent of the underlying hardware. LINBIT’s groundbreaking SDS 
product ties together data availability, performance, and scale, making it 
perfect for your cloud storage environment.
____________________________________________________
https://www.tutorialspoint.com/operating_system/pdf/os_linux.pdf
____________________________________________________
How to Use Systemd Timers as a Cron Replacement
https://www.maketecheasier.com/use-systemd-timers-as-cron-replacement/
____________________________________________________
GNU Findutils: Finding files in FS
https://www.gnu.org/software/findutils/manual/html_mono/find.html

Ex: https://serverfault.com/questions/295929/how-do-i-find-and-report-on-broken-symbolic-links-automatically/433273
______________________________________

http://www.enterprisestorageforum.com/storage-technology/open-source-storage-64-applications.html

       Open Source Storage: 64 Applications for Data Storage
       19-24 minutes
       
       As data storage needs continue to grow and many organizations move toward software-defined infrastructure, more enterprises are using open source software to meet some of their storage needs. Projects like Hadoop, Ceph, Gluster and others have become very common at large enterprises.
       
       Home users and small businesses can also benefit from open source storage software. These applications can make it possible to set up your own NAS or SAN device using industry-standard hardware without paying the high prices vendors charge for dedicated storage appliances. Open source software also offers users the option to set up a cloud storage solution where they have control over security and privacy, and it can also offer affordable options for backup and recovery.
       
       The list below features 64 open source storage projects that are among the best options available for enterprises, SMBs and individual users. Please note that this is not a ranking. Entries are organized into categories and then alphabetized within the categories.
       undefined 100%
       
       If you know of additional open source storage applications that you believe should be on our list, feel free to note them in the Comments section below.
       NAS/SAN Software
       
       1. Ceph
       Red Hat's Ceph offers unified object and block storage capabilities. It's a distributed storage solution that boasts excellent performance, scalability and reliability. Well-known users include Cisco, DreamHost, CERN, Bloomberg, and Deutsche Telekom. Operating System: Linux
       
       2. CryptoNAS
       This project aims to simplify the process of setting up an encrypted fileserver. It comes in a live-CD package or a server package that adds a web front-end. Operating System: Linux
       
       3. ESOS
       Short for Enterprise Storage OS, ESOS is a Linux distribution for setting up a storage array on your own hardware. Commercial support is available. Operating System: Linux
       
       4. FreeNAS
       FreeNAS claims to be the "the World's #1 storage OS with over 10+ million downloads." It counts the United Nations, the University of Florida, the Salvation Army, Reuters, Michigan State University, the Department of Homeland Security and many other organizations among its users. It can be installed on nearly any hardware to turn it into a network attached storage (NAS) device. Paid, supported enterprise solutions based on the same technology are available under the TrueNAS brand name. Operating System: FreeBSD
       
       5. NAS4Free
       Another option for do-it-yourself NAS, NAS4Free claims to be "the simplest and fastest way to create a centralized and easily-accessible server for all kinds of data." Key features include the ZFS file system, software RAID (levels 0, 1 or 5) and disk encryption. Operating System: FreeBSD
       
       6. Openfiler
       As a unified storage solution, Openfiler aims to combine the best features of NAS storage with the best features on SAN storage. Key features include high availability/failover, block replication and Web-based management. Its users include Motorola, Pratt & Whitney, Bill Me Later and the London Metropolitan Police. A paid commercial edition is available. Operating System: Linux
       
       7. OpenMediaVault
       Based on Debian Linux, OpenMediaVault describes itself as a "next-generation network attached storage (NAS) solution." It was designed to provide NAS for home users or small offices. It offers a Web-based administration console and includes software RAID capabilities. Operating System: Linux
       
       8. Turnkey Linux File Server
       The Turnkey Linux project offers images for setting up Linux-based servers for a variety of purposes, including an image for creating a simple NAS device. It includes support for SMB, SFTP, NFS, WebDAV and rsync file transfer protocols. Operating System: Linux
       Filesystems
       
       9. Btrfs
       A joint project supported by Facebook, Fujitsu, Intel, the Linux Foundation, Netgear, Novell, Oracle, Red Hat and others, Btrfs is a copy on write (CoW) filesystem for Linux. It focuses on "fault tolerance, repair and easy administration." Operating System: Linux
       
       10. Ext4
       Included in most popular Linux distributions, Ext4 supports file systems up to 1 EB in size with up to 16 TB per file. Other key features include extents, multiblock allocation, delayed allocation, Fast fsck, journal checksumming, "no journaling" mode, online defragmentation and more. Operating System: Linux
       
       11. GlusterFS
       A Red Hat project, GlusterFS is a highly scalable file system built for applications like media streaming and big data analytics. Professional support is available through third-party vendors. It has a large and active user community, and the website includes links to many Gluster-related blogs. Operating System: Linux
       
       12. Lustre
       Built to handle the needs of high-performance computing (HPC) environments, Lustre is a scalable parallel file system. It was first developed at Carnegie Mellon University, and its first users included various U.S. Department of Energy National Laboratories. The latest version, released in April, supports Data on MDT, file-level redundancy, lock ahead and more. Operating System: Linux
       
       13. ZFS
       Also incorporated into many Linux distributions, including Arch, Debian, Fedora, Ubuntu and others, ZFS is another highly scalable file system. It includes compression, protection against data corruption, snapshots, RAID support and more. Operating System: Linux, macOS, FreeBSD
       RAID
       
       14. DRBD
       DRBD is an open source solution for building high-availability storage clusters based on RAID-1. Commercial products, including software-defined storage, disaster recovery and high availability solutions based on the same technology, are available through project sponsor Linbit. Operating System: Linux
       
       15. Mdadm
       Built into the Linux kernel, mdadm makes it easy to create, manage and monitor storage arrays. It can also move spares between RAID arrays. More information is also available on the Linux RAID wiki. Operating System: Linux
       
       16. Raider
       Raider is a bash shell script that allows users convert any Linux disk into a RAID array with software RAID 1, 4, 5, 6 or 10. It works with many of the most popular Linux distributions, including Ubuntu, Debian, OpenSUSE, Fedora, Linux Mint and others. Operating System: Linux
       
       17. RaidEye
       RaidEye isn't so much a tool for creating RAID devices as a utility for monitoring RAID volumes. It works with the RAID capabilities built into macOS and notifies users of problems via a sound alarm, popup dialog and email. Operating System: macOS
       
       18. SnapRAID
       SnapRAID arrays can recover from up to six disk failures. The tool was built for home media servers or other environments with a lot of big files that rarely change. Key features include data hashing, the ability to recover deleted files and no lock-in. Operating System: Windows, Linux, macOS
       Backup and Synchronization
       
       19. AMANDA
       The Advanced Maryland Automatic Network Disk Archiver, or AMANDA, is a popular network backup solution that can save data from Linux, Unix or Windows systems to hard drives, tape or optical media. It was last updated in December 2017. Zmanda, which sponsors the project, offers commercial products based on the same technology. Operating System: Windows, Linux, macOS.
       
       20. Areca Backup
       Designed for personal use, Area is a simple but versatile backup solution. Key features include delta backup, compression, encryption, filters, as-of-date recovery and more. Operating System: Windows, Linux
       
       21. BackupPC
       Robust enough for enterprise use, BackupPC backs up data from Linux and Windows systems to disk. Noteworthy features include a unique pooling scheme, optional compression, a web interface and support for mobile devices. It claims to be highly configurable and easy to install and maintain. Operating System: Windows, Linux
       
       22. Bacula
       Another option for enterprises, Bacula is a network backup solution that aims to be easy to use and very efficient. It claims to be the most popular open source backup program. Commercial support and services for the solution are available through Bacula Systems. Operating System: Windows, Linux, macOS
       
       23. Bareos
       Forked from Bacula, Bareos is a popular open source backup option that is under very active development with the latest version released in February 2018. The Bareos.com website offers paid support and services for the tool. Operating System: Windows, Linux, macOS
       
       24. Box Backup
       This "completely automatic" backup solution creates backups continuously and can also create snapshots when desired. It includes encryption and optional RAID capabilities, and old file versions and deleted files remain available. Operating System: Windows, Linux
       
       25. BURP
       Short for "BackUp And Restore Program," BURP is a network backup solution. It offers a choice of two different protocols: one based on librsync (see below) and one that uses variable length chunking for inline deduplication. It is designed to be easier to configure than some other open source solutions, and it can do delta backups. Operating System: Windows, Linux
       
       26. Clonezilla
       Designed to replace Acronis True Image or Norton Ghost, Clonezilla is useful for both system deployment and backup and recovery. It comes in two flavors: live for standalone systems and SE for network backup or cloning multiple systems at once. The SE version can clone 40 or more systems at once. Operating System: Linux
       
       27. Create Synchronicity
       Powerful but lightweight, this backup tool takes up only 220KB of space on your hard drive. It supports multiple languages, has an intuitive interface and includes a scheduler. It is also helpful for syncing photos, music and other files across devices. Operating System: Windows
       
       28. DAR
       Disk Archive, a.k.a. DAR, is an older command-line tool for backup, but it is still being updated regularly with the most recent version released in April 2018. For those who prefer a GUI, one is available through the DarGUI project. Operating System: Windows, Linux, macOS
       
       29. DirSync Pro
       This "small but powerful," utility offers incremental backup, filtering and scheduling capabilities. It also boasts an intuitive interface, and it offers the ability to analyze two sets of files or folders and detect the changes between them. It also includes a helpful real-time synchronization option. Operating System: Windows
       
       30. Duplicati
       Duplicati works with cloud storage services like AWS S3, Microsoft OneDrive, Google Drive and Box to create backups with AES-256 encryption. It does a full backup on first use and incremental backups after that; it also offers data deduplication capabilities. Operating System: Windows, Linux, macOS
       
       31. FOG
       FOG offers cross-platform cloning and imaging capabilities plus remote management for networks of any size. It offers support through forums and a wiki. Operating System: Linux, Windows, macOS.
       
       32. FreeFileSync
       A tool for standalone systems, FreeFileSync aims to save users time when setting up and running backups. It is cross-platform and includes 64-bit support. Tutorials and a manual are available on the website. Operating System: Linux, Windows, macOS
       
       33. FullSync
       Although it was designed to help web developers push updates to their sites, FullSync can also be used by anyone to create backups. Key features include multiple modes, flexible rules, buffered filesystems, support for multiple file transfer protocols and more. Note that development on this project has slowed somewhat as it hasn’t been updated since April 2016. Operating System: Linux, Windows, macOS
       
       34. Grsync
       Grsync takes the older rsync synchronization tool and adds an easy-to-use GUI. Noteworthy features include unlimited sessions, highlighted errors, batch capabilities, simulations, support for multiple languages and more. Operating System: Linux, Windows, macOS
       
       35. Mondo Rescue
       For Linux and FreeBSD only, Mondo Rescue is a disaster recovery solution that supports tape, disk, network or optical media backups. According to its website, its users include "Lockheed-Martin, Nortel Networks, Siemens, HP, IBM, and dozens of smaller companies." The most recent update was released in April 2016. Operating System: Linux, Free BSD
       
       36. Partimage
       This tool saves partitions of drives as image files, making it useful for backup or installing the same image on multiple systems. It can run across networks or on a standalone PC. It can also be used to create a SystemRescueCD. Operating System: Linux
       
       37. Redo
       Redo boasts that its bare-metal restore capabilities can get a crashed system back up and running in as little as 10 minutes. It's very easy to use and can also recover deleted images and files. Operating System: Windows, Linux
       
       38. Rsync
       Rsync is a Unix-based file-transfer utility with synchronization capabilities that make it suitable for creating backups or mirroring. It's a useful tool but is best used by advanced users. The most recent version was released in January 2018. Operating System: Linux, Windows, macOS
       
       39. Synkron
       While this app is focused primarily on synchronization, it can be used for creating backups as well. Key features include analysis capabilities, blacklisting, restores and cross-platform support. Documentation is available in both German and English. Operating System: Windows, Linux, macOS
       
       40. Unison
       Like Synkron, Unison is a file synchronization tool. It can copy files between any two systems connected to the internet, and it has features in common with source code management tools as well as with backup utilities. Its advantage over some other synchronization tools is that it can combine two sets of files where both sets have undergone changes. Operating System: Windows, Unix
       
       41. UrBackup
       This client-server backup solution does both image and file backups. It promises "both data safety and a fast restoration time." It makes backups while the system is in use without interrupting normal operation. Operating System: Windows, Linux
       
       42. Weex
       The Weex developers intended it primarily as a tool for pushing content to websites, but it can also be used to synchronize or backup files. It supports FTP file transfer and uses caching to speed data transfer. Operating System: Windows, Linux
       Online/Cloud Data Storage
       
       43. CloudStack
       This Apache Foundation project is open source cloud computing platform that includes cloud storage capabilities. Noteworthy features include compute orchestration, network as a service capabilities, user and account management, resource accounting and support for multiple hypervisors. Operating System: Windows, Linux
       
       44. CloudStore
       This Dropbox alternative synchronizes data between a system and online storage. It promises strong encryption, password-less authentication, flexible synchronization, fast setup and auto-resumes for interrupted data transfers. Operating System: Linux
       
       45. Cozy
       Cozy is both an open source project for storing personal data online and a free service for managing and securing sensitive data. Note that the free hosting is for up to 5GB of data; additional storage will require a fee. Operating System: Linux
       
       46. FTPbox
       Want to set up your own cloud storage server? FTPbox makes it easy to be your own cloud provider, with all files transferred via FTP. Operating System: Windows
       
       47. OpenStack
       Probably the best-known open source cloud computing platform, OpenStack offers a complete operating system for controlling compute, networking and, of course, storage in the cloud. It incorporates three sub-projects related to storage: Cinder, Swift and Manila. Operating System: Windows
       
       48. Perkeep
       Formerly known as Camlistore, Perkeep describes itself as "a set of open source formats, protocols, and software for modeling, storing, searching, sharing and synchronizing data in the post-PC era." It's still under very active development and will require some technical knowhow to use. Operating System: Linux
       
       49. Pydio
       Downloaded more than a million times, Pydio counts the University of Cambridge, Seagate, Guitar Center, Washington State University and Nikon among its users. It offers cloud-based file management and sharing. A paid enterprise distribution is available. Operating System: Windows, Linux (Android and iOS clients available)
       
       50. Rockstor
       Rockstor makes it possible to create your own NAS or cloud storage solution based on Linux and BTRFS. It comes in both personal and SMB versions. Operating System: Linux
       
       51. SeaFile
       SeaFile describes itself as "an enterprise file hosting platform with high reliability and performance." You can download the code for free or use the paid pro edition that includes support. Operating System: Windows, Linux, macOS, Android, iOS
       
       52. SparkleShare
       SparkleShare creates a special folder on your system that is automatically synchronized with a host folder stored on your server or in the cloud. It includes encryption capabilities, and it is a good option for collaborating on documents that change frequently. Operating System: Windows, Linux, macOS
       
       53. StackSync
       Create your own scalable personal cloud with StackSync. It encrypts all data on the client side and works with cloud storage services or your own server. Operating System: Windows, Linux
       
       54. Syncthing
       Like many of the other projects in this category, Syncthing offers an alternative to Dropbox. It keeps data completely private with encryption and authentication requirements. Operating System: Windows, Linux, macOS
       Storage Management
       
       55. Libvirt Storage Management
       Libvirt is an API for creating storage pools and volumes on a host system. It supports a wide variety of storage pool types including directory, filesystem, network filesystem, logical volume, disk, iSCSI, SCSI, Gluster, ZFS and more. Operating System: Linux
       
       56. openAttic
       This tool offers management and monitoring capabilities for the Ceph distributed storage platform. It offers a dashboard, as well as tools for managing pools, block devices, iSCSI, NFS, Ceph Object Gateway and Ceph nodes. Operating System: Linux
       Distributed Storage/Big Data Tools
       
       57. Alluxio
       Alluxio (Formerly known as Tachyon) describes itself as "open-source memory-speed virtual distributed storage." It works with tools like Spark, Hadoop, Flink, Zeppelin and Presto to speed performance on big data queries. Operating System: Linux, macOS
       
       58. Hadoop
       Nearly synonymous with big data, Hadoop is a widely used open source distributed storage platform for processing data. It is an Apache Foundation project, and the organization also oversees dozens of related projects. Operating System: Windows, Linux, macOS
       
       59. HPCC
       This Hadoop alternative also offers distributed storage and massive scalability. Paid enterprise services are available. Operating System: Linux
       
       60. Sheepdog
       The Sheepdog website describes this project as "a distributed object storage system for volume and container services and manages the disks and nodes intelligently." It supports snapshotting, cloning and thin provisioning, and it is compatible with OpenStack Swift and Amazon S3. Operating System: Linux
       Compression
_______________________
https://docs.fedoraproject.org/f27/system-administrators-guide/

https://wiki.centos.org/AdditionalResources/Repositories/SCL
__________
https://en.wikipedia.org/wiki/Avahi_(software) ZeroConf Linux impl.
__________
https://www.2daygeek.com/zstandard-a-super-faster-data-compression-tool-for-linux/
Zstandard super-fast compression tool
__________
TODO: journalctrl
the journal is "synchronous". Eacth time someone tries to write it checks if
ther is space or something needs to be deleted. (vs remove each 24 day,...)
 $ journalctrl [tab] ...
______________
https://gluster.readthedocs.io
<a href="https://access.redhat.com/documentation/en-us/red_hat_gluster_storage/3.4/">Redhat Gluster</a>
____________
List and sort the versions available in
your repo:

YUM: sorting by version number:                    | APT:
                                                   | 
$ yum list docker-ce --showduplicates | sort -r    | apt-cache madison docker-ce
________________________
yum upgrade forces the removal of obsolete packages, while yum update may or may not also do this. The removal of obsolete packages can be risky, as it may remove packages that you use.

This makes yum update the safer option.

From man yum:

    update

    If run without any packages, update will update every currently installed package. If one or more packages or package globs are specified, Yum will only update the listed packages. While updating packages, yum will ensure that all dependencies are satisfied. (See Specifying package names for more information) If the packages or globs specified match to packages which are not currently installed then update will not install them. update operates on groups, files, provides and filelists just like the "install" command. If the main obsoletes configure option is true (default) or the --obsoletes flag is present yum will include package obsoletes in its calculations - this makes it better for distro-version changes, for example: upgrading from somelinux 8.0 to somelinux 9.

    upgrade

    Is the same as the update command with the --obsoletes flag set. See update for more details
_______________________
Linux filesystem administration
    """
    ...By the end of this live online course, you?ll understand:
    
    Linux filesystem concepts and tools
    How to find and search files
    How to use the inode table and file links
    The difference between a hard link and a soft link
    And you?ll be able to:
      - Modify file permissions and ACLs
      - Use file quotas
      - Create archive and Zip files
      - Create and mount disk partitions
      - Archive and back up files
      - Implement LVMs and RAID
    """
____________________
https://linux.die.net/EVMSUG/

____________________
https://linux.die.net/sag/
____________________
@ma:
https://linux.die.net/Linux-CLI/file-permissions.html
_____________________________________
https://opensource.com/article/18/7/sysadmin-guide-selinux
________________________________________
https://opensource.com/article/18/9/how-build-rpm-packages?sc_cid=701f2000000RRBZAA4
________________________________________
https://www.tecmint.com/remove-package-with-dependencies-using-yum/#
________________________
https://danwalsh.livejournal.com/
________________________
http://swift.siphos.be/linux_sea/
________________________
Add pstree view (and security related info)
________________________
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/
https://docs.fedoraproject.org/en-US/Fedora/24/html/Networking_Guide/index.html
https://wiki.debian.org/NetworkManager
________________________
https://lwn.net/Articles/531114/
________________________
/proc/sys/vm/
file-system-control-panel to kernel
________________________
Troubleshooting
<td>
boot
</td>
  Authentication issues (PAM)
<td>
</td>
<td>
Performance <br/>

memory
<pre xxxsmall zoom>
 "everything goes through memory"

top / htop  / atop
- if there is lot o buffer and cache even if there is not
  much availa. mem "there is no problem"
cache is a mechanism to speed up performance
     ... but it can be reused


less /proc/meminfo;
....
MemTotal: 
:                   
Pool of usable memory  after kernel load:
  Active   :       
  Inactive : Memory allocated by kernel and never used
Active (anon)ymos:      
Inactive (anon)ymous:   
Active (file)ymos:     
Inactive (file)ymous:    
cat /proc/meminfo; vmstat; iostat; bonnie++; dd ; nc (netcat)


</pre>

disk
<pre xxxsmall zoom>
Look for:
- wrong I/O scheduler
- wrong filesystem
- wrong journaling


sar: System activity reporting
 (Maybe first for local hard-disk) 
</pre>

network
<pre xxxsmall zoom>
</pre>

cpu
<pre xxxsmall zoom>
</pre>
</td>
<td>
hardware issues
</td>
_________________________
https://www.maketecheasier.com/use-zswap-improve-old-linux-performance/
_________________________
__________________________
https://asciinema.org/
Record and share your terminal sessions, the right way.
Forget screen recording apps and blurry video. Enjoy a lightweight, purely text-based approach to terminal recording. 

_______________________
<!-- TODO: https://opensource.com/article/18/11/partition-format-drive-linux

________________

__________________
https://www.dfrws.org/sites/default/files/session-files/paper-an_analysis_of_ext4_for_digital_forensics.pdf
_________________
https://www.kernel.org/doc/html/v4.12/crypto/index.html
__________________
https://www.oreilly.com/library/view/red-hat-certified/9780134723990/earning objectives 

2.2 Using man 
2.3 Finding the Right man Page 
2.7 Working on the Linux Shell 
2.8 Understanding Globbing and Wildcards 
2.9 Using Globbing and Wildcards 
2.10 Understanding I/O Redirection and Pipes 
2.11 Using I/O Redirection and Pipes 

Lesson 3: Essential File Management Tools
3.1 Understanding Linux File System Layout 
3.2 Essential File Management Tasks 
3.3 Finding Files 
3.4 Understanding Links 
3.5 Working with Links 
3.6 Working with tar 
3.7 Working with Compressed Files 


Lesson 4: Working with Text Files
4.1 Understanding Regular Expressions 
4.2 Using Common Text Tools 
4.3 Working with grep 
4.4 Understanding the Basics of sed and awk 

Lesson 5: Connecting to a RHEL Server
5.1 Opening Local Shells 
5.2 Using su to Work as Another User 
5.3 Using sudo to Perform Administrator Tasks 
5.4 Connecting to a Server with SSH 
5.5 Understanding SSH Keys 
5.6 Using SSH Keys 

Lesson 6: Managing Users and Groups
6.1 Understanding the Need for Users 
6.2 Creating and Managing Users 
6.3 Understanding User Properties 
6.4 Understanding User Configuration Files 
6.5 Creating and Managing Groups 
6.6 Understanding Group Membership 
6.7 Managing Password Properties 

Lesson 7: Managing Permissions
7.1 Understanding Ownership 
7.2 Changing File Ownership 
7.3 Understanding Basic Permissions 
7.4 Managing Basic Permissions 
7.5 Understanding umask 
7.6 Understanding Special Permissions 
7.7 Managing Special Permissions 
7.8 Understanding ACLs 
7.9 Managing ACLs 

Lesson 8: Configuring Networking
8.1 Understanding NIC Naming 
8.2 Managing runtime network Configuration with the ip Command 
8.3 Storing Network Configuration Persistently 
8.4 Managing Persistent Network Configuration with nmcli 
8.5 Managing Persistent Network Configuration with nmtui 
8.6 Verifying Network Configuration Files 
8.7 Understanding Routing and DNS 
8.8 Configuring Routing and DNS 
8.9 Understanding Network Analysis Tools 
8.10 Using Network Analysis Tools 

Lesson 9: Managing Processes
9.1 Understanding Jobs and Processes 
9.2 Managing Shell Jobs 
9.3 Getting Process Information with ps 
9.4 Understanding Memory Usage 
9.5 Understanding Performance Load 
9.6 Monitoring System Activity with top 
9.7 Sending Signals to Processes 
9.8 Understanding Priorities and Niceness 
9.9 Changing Process Nice Values 

Lesson 10: Managing Software
10.1 Understanding Meta Package Handlers 
10.2 Setting up Yum Repositories 
10.3 Using Repositories 
10.4 Managing Packages with yum 
10.5 Using Yum Groups 
10.6 Understanding yum and RPM Queries 
10.7 Using RPM Queries 

Lesson 11: Working with Virtual Machines
11.1 Introducing KVM Virtualization 
11.2 Managing Libvirt and KVM 
11.3 Installing a Virtual Machine 
11.4 Using virsh 
11.5 Using virt-manager 
11.6 Understanding KVM Networking 
11.7 Managing KVM Networking 
11.8 Importing OVF Virtual Machine Files 

Lesson 12: Scheduling Tasks
12.1 Using cron vs. at 
12.2 Understanding Cron Execution Times 
12.3 Scheduling with cron 
12.4 Understanding anacron 
12.5 Using at 

Lesson 13: Configuring Logging
13.1 Understanding rsyslogd and journald Logging 
13.2 Configuring rsyslog Logging 
13.3 Working with journald 
13.4 Configuring logrotate 

Lesson 14: Managing Partitions
14.1 Understanding Disk Layout 
14.2 Understanding GPT and MBR Partitions 
14.3 Creating MBR Partitions with fdisk 
14.4 Creating GPT Partitions with gdisk 
14.5 Understanding File System Differences 
14.6 Making the File System 
14.7 Mounting the File System Manually 
14.8 Mounting Partitions via /etc/fstab 
14.9 Creating a Swap Partition 

Lesson 15: Managing LVM Logical Volumes

15.4 Understanding Device Mapper and LVM Device Names 

______________
https://linux.die.net/man/1/dstat
Dstat is a versatile replacement for vmstat, iostat and ifstat. Dstat overcomes some of the limitations and adds some extra features.

Dstat allows you to view all of your system resources instantly, you can eg. compare disk usage in combination with interrupts from your IDE controller, or compare the network bandwidth numbers directly with the disk throughput (in the same interval). 
_______________
https://linux.die.net/man/1/explain


The explain command is used to decode an error return read from an strace(1) listing, or silimar. Because thisd is being deciphered in a different process than the orginal, the results will be less accurate than if the program itself were to use libexplain(3). 
_________________________
https://linux.die.net/man/1/latrace

latrace - LD_AUDIT 2.4+ libc frontend
Synopsis

latrace [-ltfsbcCpADaoyIiBdvTFELVh] command [arg ... ]
Description

latrace is able to run a command and display its dynamic library calls using a LD_AUDIT libc feature (available from libc version 2.4 onward - see the section called "DISCUSSION" ). It is also capable to measure and display various statistics of dynamic calls.

If the config file is provided, latrace will display symbol's arguments with detailed output for structures. The config file syntax is similar to the C language, with several exceptions (see the section called "CONFIG").

The latrace by default fully operates inside of the traced program. However another pipe mode is available, to move the main work to the latrace binary (see the section called "PIPE mode"). 

</html>
