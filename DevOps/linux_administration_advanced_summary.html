<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>Linux Advanced Administration Summary <!-- ignore --></title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body>

<table style='width:100%'{>
<tbody>

<tr {>
<td>
 <a xsmall TODO href="https://www.freeipa.org/page/Main_Page">Central Authentication</a>
<pre xxxsmall zoom>
- Manage Linux users and client hosts in your realm from one central location 
  with CLI, Web UI or RPC access. Enable Single Sign On authentication for all 
  your systems, services and applications.
- Policy
- Define Kerberos authentication and authorization policies for your 
  identities. Control services like DNS, SUDO, SELinux or autofs.
- Trusts
- Create mutual trust with other Identity Management systems like Microsoft 
  Active Directory.
</pre>
</td>

<td>
 <a xsmall TODO href="">Setup Time Services</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
 <a xsmall TODO href="https://www.maketecheasier.com/use-systemd-timers-as-cron-replacement/">Systemd Timers(Cron alt)</a>
<pre xxxsmall zoom>
</pre>
</td>


  
  
<td>
 <a xsmall TODO href="https://www.linux-kvm.org/">KVM/Qemu:</a>
<pre xxxsmall zoom>
</pre>
 <a xsmall TODO href="https://www.linux-kvm.org/page/Migration">KVM live migration</a>
<pre xxxsmall zoom>
</pre>

 <a xsmall TODO href="http://fedoraproject.org/wiki/Features/Spice">Spice (Virt.Desktops)</a>
<pre xxxsmall zoom>
Spice aims to provide a complete open source solution 
for interaction with virtualized desktops 
</pre>

</td>






</tr>
</table>
<br/>
  Storage
<table>
<tr>
<td>
 <a xsmall TODO href="https://alpss.at/">REF:Alpine Persistence/Storage Summit</a>
<pre xxxsmall zoom>
  https://alpss.at/ : Alpine Linux Persistence and Storage Summit
  08:30 - 09:00   Zoned Namespaces for NVMe   Matias Bjørling and Christoph Hellwig
  10:00 - 10:30   Copy Offload    Bart van Assche
  10:30 - 11:00   PCI 2P2 and computational storage   Stephen Bates
  11:00 - 11:30   Exciting new stuff in RDMA  Idan Burstein
  12:00 - 12:30   SPI-attached Flash Memory   Boris Brezillon and Miquel Raynal
  12:30 - 13:00   Advanced power-cut testing the MTD stack    Richard Weinberger
  14:00 - 19:00   BOFs and hallway track (or hiking on your own)  
  08:30 - 09:00   Qemu as a Memory emulation platform Damien Le Moal
  09:00 - 09:30   UMAP    Adam Manzanares
  10:00 - 10:30   Inode namespacing – COW inodes  Jeff Mahoney
  10:30 - 11:00   Adding filesystem authentication to UBIFS   David Gstir
  11:00 - 11:30   Reverse Engineering APFS (Apple File System)    Johannes Thumshirn
  09:00 - 09:30   Blk-mq and zoned devices    Bart van Assche and Damien Le Moal
  12:00 - 13:00   Blk-mq: timeout handling, power management, etc Bart van Assche

</pre>
</td>
<td>
block multi-queue (kernel 3.13+)
<pre xxxsmall zoom>
block layer "blk-mq" introduces block multi-queue support,
intended to meet the high IOPS requirements of SSDs:
Cite (Jens Axobe):
"""the classic request_fn based driver doesn't work well enough
(for big IOPS)
Axboe The design is centered around per-cpu queues for queueing IO,  
which then funnel down into x number of hardware submission queues
</pre>
</td>
<td>
 <a xsmall TODO href="http://fedoraproject.org/wiki/Features/Opensharedroot">sharedroot</a>
<pre xxxsmall zoom>
The open sharedroot project provides abilities to boot multiple linux systems 
with the same root filesystem providing a single system filesystem based 
cluster. (NFS, GFS,...)  
</pre>
</td>

<td>
 <a xsmall TODO href="">Loop device</a>
<pre xxxsmall zoom>
</pre>
</td>

<td>
 <a xsmall TODO href="https://linux.die.net/man/8/debugfs">man 8 debugfs</a>
<pre xxxsmall zoom>
  ext2/ext3/ext4 file system debugger:

- It can be used to examine and change
  the state of an ext2, ext3, or ext4 file system.
</pre>
</td>

</tr>
</table>
<br/>
Kernel
<table>
<tr>
<td>
  
 <a xsmall TODO href="">/proc/meminfo</a>
<pre xxxsmall zoom>
$ cat /proc/meminfo 
MemTotal:       16116792 kB
MemFree:         2042420 kB
MemAvailable:   10656344 kB
Buffers:         1637424 kB
Cached:          6513208 kB
SwapCached:          352 kB
Active:          8372356 kB
Inactive:        3940908 kB
Active(anon):    3755128 kB
Inactive(anon):   645496 kB
Active(file):    4617228 kB
Inactive(file):  3295412 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:       8126460 kB
SwapFree:        8124156 kB
Dirty:              1304 kB
Writeback:             0 kB
AnonPages:       4162388 kB
Mapped:           732652 kB
Shmem:            238000 kB
Slab:            1337700 kB
SReclaimable:    1029376 kB
SUnreclaim:       308324 kB
KernelStack:       15632 kB
PageTables:        31724 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    16184856 kB
Committed_AS:   11012532 kB
VmallocTotal:   34359738367 kB
VmallocUsed:           0 kB
VmallocChunk:          0 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
CmaTotal:              0 kB
CmaFree:               0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB
DirectMap4k:     1147072 kB
DirectMap2M:    15319040 kB
</pre>

</td>
<td>
 <a xsmall TODO href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format">ELF</a>
<pre xxxsmall zoom>
man readelf
</pre>
</td>
<td>
 <a xsmall TODO href="https://en.wikipedia.org/wiki/Ksplice">Ksplice</a>
<pre xxxsmall zoom>
See also:
 - kexec, a method for loading a whole new kernel from a running system
 - kGraft, kpatch and KernelCare, other Linux kernel live patching 
   technologies developed by SUSE, Red Hat and CloudLinux, respectively

<a TODO href="https://www.2daygeek.com/how-to-install-upgrade-unbreakable-enterprise-kernel-uek-in-oracle-linux/#">1</a>
<a TODO href="https://www.2daygeek.com/how-to-updateinstall-oracle-kernel-critical-security-updates-without-rebooting/">2</a>
<a TODO href="https://serverfault.com/questions/78406/is-ksplice-production-ready/">3</a>
</pre>
</td>
<td>  
<a TODO href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_MRG/1.3/html/Realtime_Tuning_Guide/sect-Realtime_Tuning_Guide-General_System_Tuning-Interrupt_and_Process_Binding.html">Attach Interrupt to CPU/Real-time processes</a>
</td>  
<td>
 <a xsmall TODO href="">automatic NUMA balancing</a>
<pre xxxsmall zoom>
 automatic non-uniform memory access (NUMA) balancing, Linux 3.13+
</pre>
</td>
</tr>
</table>

<br/>
 Containers
<table>
<tr>
<td>
 <a xsmall TODO href="">man 5 namespace.conf</a>
<pre xxxsmall zoom>
Linux Namespaces:
- https://en.wikipedia.org/wiki/Linux_namespaces
- http://man7.org/linux/man-pages/man1/nsenter.1.html
</pre>
</td>
<td>
 <a xsmall TODO href=""> Cgroups:</a>
<pre xxxsmall zoom>
</pre>
</td>
<td>
 <a xsmall TODO href="https://unix.stackexchange.com/questions/98808/how-to-assign-an-additional-ip-hostname-to-a-chrooted-environment">Assign new IP to chrooted env</a>
<pre xxxsmall zoom>
</pre>
</td>
<td>
  <span xsmall>Hidding processes to other Users:</span> 
<pre xxxsmall zoom>
<a href="http://www.cyberciti.biz/faq/linux-hide-processes-from-other-users/">REF</a>
- The hidepid option in the procfs defines how much info
  about processes we want to be available for non-owners. 

  hidepid=0 : (default old behavior), anybody may read
              all world-readable /proc/PID/* files</li>
  hidepid=1 : users may not access any /proc/ / directories, 
              but their own.  
              Sensitive files like cmdline, sched*, status 
              are NOW protected against other users

  hidepid=2 : equals to hidepid=1 plus :
              - /proc/PID/  will be invisible to other users.
                It complicates intruder's task of gathering info
                about running processes, whether some daemon
                runs with elevated privileges, whether another user
                runs some sensitive program, whether other users run any 
                program at all, etc.Linux kernel protection.

Ex:
# mount -o remount,rw,hidepid=2 /proc
Alternatively edit /etc/fstab:
proc  /proc  proc defaults,hidepid=2  0   0
</td>
</tr>
</table>

 Security Hardening
<table>
<tr>
<td>
 <a xsmall TODO href="https://grsecurity.net/">Grsecurity®</a>
<pre xxxsmall zoom>
Grsecurity: extensive security enhancement to the Linux kernel that defends 
  against a wide range of security threats through intelligent access control, 
  memory corruption-based exploit prevention, and a host of other system 
  hardening that generally require no configuration.

   It has been actively developed and maintained for the past 17 years.
  Commercial support for grsecurity is available through Open Source Security, Inc

<a href="https://www.grsecurity.net/compare.php">Comparation grsecurity SELinux AppArmor KSPP</a>

<a href="https://en.wikipedia.org/wiki/Grsecurity#PaX">REF</a>
 """A major component bundled with grsecurity is PaX. Among other features, 
the patch flags data memory, the stack, for example, as non-executable and 
program memory as non-writable. The aim is to prevent memory from being 
overwritten, which can help to prevent many types of security vulnerabilities,
such as buffer overflows. PaX also provides address space layout randomization
(ASLR), which randomizes important memory addresses to reduce the probability
of attacks that rely on easily predicted memory addresses."""
</pre>
</td>

<td>
 <a xsmall TODO href="https://sysdig.com/blog/selinux-seccomp-falco-technical-discussion/">SysDig Falco</a>
<pre xxxsmall zoom>
</pre>

<a xsmall TODO href="https://labs.mwrinfosecurity.com/blog/high-interaction-honeypots-with-sysdig-and-falco">HoneyPots with SysDig and Falco</a>
<pre xxxsmall zoom>
</pre>


</td>

<td>
 <a xsmall TODO href="https://github.com/trimstray/the-practical-linux-hardening-guide">practical linux hardening guide</a>

<pre xxxsmall zoom>
</pre>
</td>
</tr>
</table>




</body>

</html>
<!--
TODO
NON-CLASSIFIED: {{{
________________
https://blogs.gnome.org/hughsie/2008/11/06/devicekit-power-latency-control/
DeviceKit-power latency control
Use cases:
    I want my IM application to request 0.5s latency for messages
    I’m running an OpenGL simulation and want maximum performance, even on battery
    I’m running an SQL server for a credit card company, and want the server to request low latency CPU and network as any delay costs money
    I’m an admin, and want to change the power consumption vs. latency from cron scripts so it uses high latency during the night for maximum power saving, and low latency during business hours.
    I want high throughput when copying files, but want low throughput for downloading updates in the background.
    I want my power manager to set all latencies to lowest when on AC power
    I don’t want my users messing with latency settings
    I’m and admin and I want to be able to override all latency settings on my machines

________________________
https://upower.freedesktop.org/

 UPower is an abstraction for enumerating power devices, listening to device events and querying history and statistics. Any application or service on the system can access the org.freedesktop.UPower service via the system message bus. Some operations (such as suspending the system) are restricted using PolicyKit.

UPower was once called DeviceKit-power. UPower aims to make a large chunk of HAL redundant, as HAL is officially deprecated.

UPower is also useful to control the latency of different operations on your computer, which enables you to save significant amounts of power. Nothing much uses this interface yet, but this is a classic chicken and egg scenario, and I think it's important to encourage the egg to lay a chicken. Please report any problems to the Freedesktop bugzilla or send a mail to the DeviceKit mailing list for discussion. 
_______________________________
https://www.collabora.com/news-and-blog/blog/2018/11/21/gaining-ebpf-vision-tracing-linux-filesystem-disk-requests/ 
_______________________________
http://www.eweek.com/security/aws-boosts-serverless-security-with-firecracker-microvms
_______________________________
https://www.oreilly.com/library/view/red-hat-certified/9780134723990/
Lesson 20: Connecting to an LDAP Server
    20.1 Understanding LDAP, Kerberos, IPA, and AD 
    20.2 Setting up LDAP Authentication 
    20.3 Connecting to an IPA Server 
    20.4 Joining an AD Domain 

Lesson 21: Accessing Remote File Systems and Automount
    21.1 Accessing an NFS Server 
    21.2 Accessing a Samba Server 
    21.3 Understanding Automount 
    21.4 Configuring Automount 
    21.5 Configuring Automount for Home Directories 
______________________
https://www.thomas-krenn.com/en/wiki/Category:Storage
________________________________
<td>
 <a xsmall TODO href="http://clusterlabs.org/">ClusterLabs</a>
<pre xxxsmall zoom>
The ClusterLabs stack unifies a large group of Open Source projects related 
to High Availability into a cluster offering suitable for both small and 
large deployments. Together, Corosync, Pacemaker, DRBD, ScanCore, and many 
other projects have been enabling detection and recovery of machine and 
application-level failures in production clusters since 1999. The ClusterLabs 
stack supports practically any redundancy configuration imaginable.
</pre>
</td>
<td>
  KVM weekly backup, the easy way
<pre xxxsmall zoom>
 KVM weekly backup, the easy way
For the joy of all my readers here it comes the master-of-the-universe weekly script backup for KVM:

# Redirect stdout/stderr to custom log.

exec 1>> /var/log/custom_$(basename ${0}).$(whoami).$(date '+%Y%m%d').log 2>&1
pushd /media/backup/MyServer1/
# Keep copies of the VM for the last 4 weeks.
mv kvm.qemu.gz.1 kvm.qemu.gz.2
mv kvm.qemu.gz.0 kvm.qemu.gz.1
mv kvm.qemu.gz kvm.qemu.gz.0

# Stop KVM instance through telnet.
# KVM has to be started with the option:
# -monitor telnet:127.0.0.1:9942,server,nowait
echo "stop" | nc -q 10 127.0.0.1 9942 # Freeze KVM instance

#Here we do the real backup of the KVM instance
cat /VM-Images/kvm.qemu | gzip > /media/backup/MyServer1/kvm.qemu.gz
if [ $? != 0 ]; then
echo "WARN: KVM backup failed"
fi
echo "c" | nc -q 10 127.0.0.1 9942 # Continue KVM instance
popd

For the previous script to work correctly we have to start the KVM instance with the option -monitor telnet:127.0.0.1:9942.

In my particular system I start KVM instances at startup in /etc/rc.local. This is script is also quite interesting so I extracted all the kvm related stuff:

/etc/rc.local:
# Setting up the bridge (tip: apt-get install bridge-utils)
brctl addbr ofi1
brctl addif ofi1 eth1
ifconfig eth1 0.0.0.0 promisc up
ifconfig ofi1 192.168.2.100 netmask 255.255.0.0 up
ifconfig ofi1:1 172.16.1.3

...
kvm -net nic,macaddr=52:54:00:19:34:56 \
-net tap,script=/etc/qemu-ifup -hda /VM-Images/kvm.qemu \
-boot c -vnc :5 1>/var/log/custom_kvm.qemu.log \
-monitor telnet:127.0.0.1:9942,server,nowait 2>&1 &
...
# ionice/renice down our virtual machine
PID=$(sof /VM-Images/kvm.qemu | grep -v ^COMMAND | while read cmd pid staff; do echo $pid; done)
ionice -c 3 -p ${PID} &
renice 10 -p ${PID} &


/etc/qemu-ifup:
#!/bin/sh
# ofi1 is the choosen name in /etc/rc.local
/usr/sbin/brctl addif ofi1 $1 ;
ifconfig $1 0.0.0.0 up;
</pre>
</td>
<td>
  <a xsmall TODO href="https://medium.com/coinmonks/solidity-tutorial-returning-structs-from-public-functions-e78e48efb378">Solidity tutorial: returning structs from public functions</a>
<pre xxxsmall zoom>
In the current version of Solidity (0.4.13, as of this writing), it’s impossible to return a struct from a public function. For example, if you attempt to compile the following code, you will get two errors, one for each function that tries to do so:

You can open this Gist in browser-solidity with the following link: http://ethereum.github.io/browser-solidity/#gist=03f9e536d033f81fe2a2df7a74a17ddf&version=soljson-v0.4.13+commit.fb4cb1a.js

As you’ll see, the compiler complains about both functions, getBryn and getPerson:

TypeError: Internal type is not allowed for public or external functions.

Confirmed, it’s impossible to return structs.

However, if you’ve been coding in Solidity for a while, you might notice that the following works perfectly well:

Because the mapping is public, Solidity automatically generates a getter for this function. That’s odd, because the getter would have to return a Person struct, right? Let’s dig in a little bit deeper.

Let’s deploy the above contract and run the following Javascript code:

Okay, so what’s going on here?

Shouldn’t the compiler complain when we ask it to generate a getter for a mapping that returns a Person struct?

As it turns out, solc is smart enough to handle this case for us. Notice the return value from the invocation of project.people('0xdeadbeef'). It’s an array, which is how Solidity tuples are returned to Javascript land. Alright, so we know that the getter is not returning a struct, per se, but rather a tuple. This is a big clue!

When the compiler generates a getter for a mapping or an array where the element is a struct, it does something like the following:

As you can see, the getter simply breaks the struct down into a tuple. No internal types (like structs) are exposed. As a result, we get the functionality we desired with no compiler error. We simply have to be careful about the ordering of struct fields—if the order changes, we’ll have to change any code that interacts with this getter, since the order of the tuple values will change as well.

Knowing this, we can take a cue from Solidity’s implementation of this mapping getter when writing our own functions that return structs.
Returning arrays of structs

Sometimes we might need to return an array of structs to the caller. However, if we take the naïve approach, we’ll once again run into the same limitation—we can’t expose internal types.

We learned above that we can “destructure” a struct and return it as a tuple. To return an array of structs, we will do the same thing. Each value in the returned tuple will represent a field in the struct. But because we’re trying to return many structs (and therefore, many values for each field), each field in the tuple will be an array.

Check out the following code to get a better sense of how this might look:

Getting data back in this format is, admittedly, a pain to deal with on the frontend. Depending on your frontend data model, you may very well have to write functions to “rebuild” these structs from the data you receive. I’ve thrown together a quick example to show you how you might go about doing that:

Note that struct “destructuring” is (hopefully) a temporary workaround. Solidity’s behavior is expected to change at some point in the future, making this a bit more seamless.

Find me around the web:

    Twitter: https://twitter.com/brynbellomy
    Github: https://github.com/brynbellomy
    LinkedIn: https://www.linkedin.com/in/bryn-bellomy

</pre>


</td>
____________________________
https://www.reddit.com/r/linuxadmin/comments/aoj2bf/an_indepth_guide_on_everything_you_can_log_with/?utm_source=reddit-android
_____________________________
https://firecracker-microvm.github.io
Firecracker implements a virtual machine monitor (VMM) that uses the Linux Kernel-based Virtual Machine (KVM) to create and manage microVMs. Firecracker has a minimalist design. It excludes unnecessary devices and guest functionality to reduce the memory footprint and attack surface area of each microVM. This improves security, decreases the startup time, and increases hardware utilization. Firecracker currently supports Intel CPUs, with planned AMD and Arm support. Firecracker will also be integrated with popular container runtimes such as containerd. Our latest roadmap can be found here.
__________________________________

https://www.infoq.com/news/2019/03/vector-ebpf-container
Vector Performance Monitoring Tool Adds eBPF, Unified Host-Container Metrics Support 


Vector, the open source performance monitoring tool from Netflix, added support for eBPF based tools using a PCP daemon, a unified view of container and host metrics, and UI improvements.

Netflix had earlier released a performance monitoring tool called Vector as open source. Vector can "visualize and analyze system and application-level metrics in near real-time". These metrics include CPU, memory, disk and network, and application profiling using flamegraphs. Vector is build on top of Performance Co-Pilot (PCP), a performance analysis toolkit. PCP works in a distributed fashion with a daemon on each monitored host, which controls and routes metric requests to individual agents which collect the actual metrics. There are agents for most popular software, and custom application metrics can be collected by writing one's own agent. Client applications connect to the daemon.
________________________
https://opensourceforu.com/2013/12/things-know-futexes/
__________________________
TODO: linux bft filters kernwl 4.14+
_______________________
https://blog.packagecloud.io/eng/2016/04/05/the-definitive-guide-to-linux-system-calls/
____________________
https://www.ostechnix.com/improve-linux-systems-security-using-firejail/
__________________
https://en.wikipedia.org/wiki/Netlink
______________
https://events.static.linuxfound.org/images/stories/slides/jls09/jls09_ikeda.pdf
}}}
}
