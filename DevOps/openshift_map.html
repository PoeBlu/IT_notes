<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
<title>OpenShift map (beta)</title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body>

<table>
<tr>
<td>
<a href="https://github.com/openshift/origin/releases">OC Client releases</a>
<ul xsmall>
<li><a bgorange href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#core-concepts">Core Concepts</a></li>
<li><a bgorange href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/"           >Dev.Guide    </a></li>
</ul>
  External Links
<ul xxxsmall zoom>
<li> <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-new-app" >Create new applications</a> </li>
<li> <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-projects" >Monitor and configure projects</a> </li>
<li> <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-templates" >Generate configurations using templates</a> </li>
<li> <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-how-builds-work" >Manage builds, including build strategy options and webhooks</a> </li>
<li> <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-how-deployments-work" >Define deployments, including deployment strategies</a> </li>
<li> <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-routes" >Create and manage routes</a> </li>
<li> <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-secrets" >Create and configure secrets</a> </li>
<li> <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-integrating-external-services" >Integrate external services, such as databases and SaaS endpoints</a> </li>
<li> <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-application-health" >Check application health using probes</a> </li>
</ul>

  "ACRONYMS"
<pre xxxsmall zoom>
OPC:  OpenShift Container Platform
App:  (Application). In the context of OPC is the full set
      of processes/services used to have a functional application.
      For example an app can be:
        - 1 pod  running Postgresql
        - 3 pods running FrontEnd
        - 1 pod  running Cache
      In practice the App is described by the DeploymentConfig (dc/) 
      object created during the 'oc new-app' phase.
S2I: Source to Image: One of the possible Build Strategies

  bc/  == Build Configuration
  dc/  == Deployment configuration
  pod/ == (Running?) pod
</pre>

  API Objects "INPUTS&amp;OUTPUTS"
<pre xxxsmall zoom>
API objects are designed such that there are portions of
the object which specify the desired state of the system,
and other portions which reflect the status or current
state of the system.
<b>This can be thought of as inputs and outputs.</b>
 The input portions, when expressed in JSON or YAML,
are items that fit naturally as source control managed
(SCM) artifacts. (App. configuration as code)

WARN: API object specifications should be captured with 
'oc export':
 This operation removes environment specific data from the
object definitions (e.g., current namespace or assigned
IP addresses), allowing them to be recreated in different
environments (unlike oc get operations, which output an
unfiltered state of the object). 
</pre>

  Common commands
<pre xxxsmall zoom>
Add (Readiness) Probe
$ oc set probe dc/instantApp --readiness --get-url=http://:8080/<b>health</b>

Get Info (describe) Deployments
$ oc describe dc/instantApp # get list of all deployments made

Get Info of Templates
# list global(loaded by cluster admin) templates
$ oc get templates -n openshift

Get Info (describe) Pods
$ oc get pods # find out instances <b>running</b>
              # add filter with: --selector app=hiworld
  NAME                    READY     STATUS      RESTARTS   AGE
  hiworld-1-build      0/1       Completed   0          37m
  hiworld-2-build      0/1       Completed   0          33m
  ...

Get info (Describe) Pod Instance
$ oc describe pod/hiworld-4-cd3y3 # Info about given instance.
  Name:       hiworld-4-cd3y3
  Namespace:  wfproject
  Image(s):   172.30.210.155:5000/wfproject/hiworld@sha256:9c941cb...
  Node:       origin/10.0.2.15Start
  Time: Thu, 17 Mar 2016 11:30:08 +1100
  Labels:     <b>app=</b>hiworld,<b>deployment=</b>hiworld-4,<b>deploymentconfig=</b>hiworld
  Status:     Running

Shell Access to Running Pod
$ oc rsh hiworld-8

Edit Deployment Configuration Manually
$ oc edit dc/instantApp -o json # -o json => Edit in JSON format
"spec": {
  "<b>strategy</b>": {
    "<b>type</b>": "Rolling",
    "<b>rollingParams</b>": {
      "<b>pre</b>": { "failurePolicy": "Abort",
        "execNewPod": { 
          "command": [
            "/usr/bin/echo",
            "RUNNING PRE HOOK"], "containerName": "instantApp" }
      },
      "<b>post</b>": { "failurePolicy": "Abort",
        "execNewPod": {
          "command": [
             "/usr/bin/echo",
             "RUNNING POST HOOK"
          ],
          "containerName": "instantApp" }
      }
    }
},...


</pre>

</pre>

  Troubleshooting
<pre xxxsmall zoom>
View Cluster Events</span>
$ oc get events 

Review Pod Logs
$ oc logs hiworld-5-cqlo4 (-f)

Log into running pod:
$ oc rsh "pod_id"

Check global project status:
$ oc status
</pre>


  Environement Variables
<pre xxxsmall zoom>
# set/edit env.var for DC
$ oc set env dc/hiworld MYSQL_DATABASE=mysql

# set/edit env.var for BC
$ oc set env bc/hiworld MAVEN_ARGS_APPEND='-Dmaven.test.skip=true'

# list set of env.vars (that will be) exported to a container
$ oc set env dc/hiworld --list 
  # deploymentconfigs hiworld, container hiworld
  PATH=/opt/app-root/src/bin:...
  STI_SCRIPTS_URL=image:///usr/libexec/s2i
  ...
</pre>

</td>  
<td>
  <b>Flow diagram</b>
<pre xxxsmall zoom>
REF: <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/">RedHat OpenS. Dev.Guide</a>
@startuml
</pre>
    <b>New template</b><br/>
    (Java EE app + MongoDB)
<pre xxxsmall zoom>
@startuml
participant github
participant developer as dev
participant OpenshiftConsole as cc
participant OpenshiftWebConsole as wc
participant Project/Pod as project

dev →      cc : oc <b>new-project</b> parksapp \
                --display-name="Baseball Parks"
cc  → +project:
dev →       cc: oc create \
                -f <a href="https://raw.githubusercontent.com/gshipley/openshift3mlbparks/master/mlbparks-template.json">
                    https://..../mlbparks-<b>template</b>.json</a>
cc  →+template: add to project
                (template is now available to project workspace)
                As cluster admin, add to openshift namespace to make available to
                any user
@enduml
</pre>

    oc man
<pre xxxsmall zoom bgorange>
$ oc <command> --help
$ oc options" for a list of global command-line options (applies to all commands).

<b>Basic Commands:</b>
  types           An introduction to concepts and types
  login           Log in to a server
  new-project     Request a new project (new k8s namespace "sub-cluster")
  new-app         Create a new application
  status          Show an <b>overview of the current project</b>
  project         Switch to another project
  projects        Display existing projects
  explain         Documentation of resources
  cluster         Start and stop OpenShift cluster

<b>Build and Deploy Commands:</b>
  rollout         Manage a Kubernetes deployment or OpenShift deployment config
  deploy          View, start, cancel, or retry a deployment
  rollback        Revert part of an application back to a previous deployment
  new-build       Create a new build configuration
  start-build     Start a new build
  cancel-build    Cancel running, pending, or new builds
  import-image    Imports images from a Docker registry
  tag             Tag existing images into image streams

<b>Application Management Commands:</b>
  get             Display one or many resources
                  -o : shows also Runtime associated info
  describe        Show details of a specific resource or group of resources
  edit            Edit a resource on the server
  set             Commands that help set specific features on objects
  label           Update the labels on a resource
  annotate        Update the annotations on a resource
  expose          <b>Expose</b> a replicated application as a <b>service or route</b>
  delete          Delete one or more resources
  scale           Change the number of pods in a deployment
  autoscale       Autoscale a deployment config, deployment, replication controller, or replica set
  secrets         Manage secrets
  serviceaccounts Manage service accounts in your project

<b>Troubleshooting and Debugging Commands:</b>
  logs            Print the logs for a resource
  rsh             Start a shell session in a pod
  rsync           Copy files between local filesystem and a pod
  port-forward    Forward one or more local ports to a pod
  debug           Launch a new instance of a pod for debugging
  exec            Execute a command in a container
  proxy           Run a proxy to the Kubernetes API server
  attach          Attach to a running container
  run             Run a particular image on the cluster
  cp              Copy files and directories to and from containers.

<b>Advanced Commands:</b>
  adm             Tools for managing a cluster
  create          Create a resource by filename or stdin
  replace         Replace a resource by filename or stdin
  apply           Apply a configuration to a resource by filename or stdin
                  See usage for (dev|pre|pro|...)stage change <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-promoting-applications-methods-tools">here</a>
  patch           Update field(s) of a resource using strategic merge patch
  process         Process a template into list of resources
  export          Export resources so they can be used elsewhere
  extract         Extract secrets or config maps to disk
  idle            Idle scalable resources
  observe         Observe changes to resources and react to them (experimental)
  policy          Manage authorization policy
  convert         Convert config files between different API versions
  import          Commands that import applications

<b>Settings Commands:</b>
  logout          End the current server session
  config          Change configuration files for the client
  whoami          Return information about the current session
  completion      Output shell completion code for the given shell (bash or zsh)
</pre>
  oc types
<pre xxxsmall zoom bgorange>
(output of 'oc types')
Concepts and Types 
Kubernetes and OpenShift help developers and operators build, test, and deploy
applications in a containerized cloud environment. 
Applications may be composed of all of the components below, although most
developers will be concerned with Services, Deployments, and Builds for 
delivering changes. 

Concepts: 

* Containers:
    A definition of how to run one or more processes inside of a portable Linux
    environment. Containers are started from an Image and are usually isolated
    from other containers on the same machine.
    
* Image:
    A layered Linux filesystem that contains application code, dependencies,
    and any supporting operating system libraries. An image is identified by
    a name that can be local to the current cluster or point to a remote Docker
    registry (a storage server for images).
    
* Pods [pod]:
    A set of one or more containers that are deployed onto a Node together and
    share a unique IP and Volumes (persistent storage). Pods also define the
    security and runtime policy for each container.
    
* Labels:
    Labels are key value pairs that can be assigned to any resource in the
    system for grouping and selection. Many resources use labels to identify
    sets of other resources.
    
* Volumes:
    Containers are not persistent by default - on restart their contents are
    cleared. Volumes are mounted filesystems available to Pods and their
    containers which may be backed by a number of host-local or network
    attached storage endpoints. The simplest volume type is EmptyDir, which
    is a temporary directory on a single machine. Administrators may also
    allow you to request a Persistent Volume that is automatically attached
    to your pods.
    
* Nodes [node]:
    Machines set up in the cluster to run containers. Usually managed
    by administrators and not by end users.
    
* Services [svc]:
    A name representing a set of pods (or external servers) that are
    accessed by other pods. The service gets an IP and a DNS name, and can be
    exposed externally to the cluster via a port or a Route. It's also easy
    to consume services from pods because an environment variable with the
    name <SERVICE>_HOST is automatically injected into other pods.
    
* Routes [route]:
    A route is an external DNS entry (either a top level domain or a
    dynamically allocated name) that is created to point to a service so that
    it can be accessed outside the cluster. The administrator may configure
    one or more Routers to handle those routes, typically through an Apache
    or HAProxy load balancer / proxy.
    
* Replication Controllers [rc]:
    A replication controller maintains a specific number of pods based on a
    template that match a set of labels. If pods are deleted (because the
    node they run on is taken out of service) the controller creates a new
    copy of that pod. A replication controller is most commonly used to
    represent a single deployment of part of an application based on a
    built image.
    
* Deployment Configuration [dc]:
    Defines the template for a pod and manages deploying new images or
    configuration changes whenever those change. A single deployment
    configuration is usually analogous to a single micro-service. Can support
    many different deployment patterns, including full restart, customizable
    rolling updates, and fully custom behaviors, as well as pre- and post-
    hooks. Each deployment is represented as a replication controller.
    
* Build Configuration [bc]:
    Contains a description of how to build source code and a base image into a
    new image - the primary method for delivering changes to your application.
    Builds can be source based and use builder images for common languages like
    Java, PHP, Ruby, or Python, or be Docker based and create builds from a
    Dockerfile. Each build configuration has web-hooks and can be triggered
    automatically by changes to their base images.
    
* Builds [build]:
    Builds create a new image from source code, other images, Dockerfiles, or
    binary input. A build is run inside of a container and has the same
    restrictions normal pods have. A build usually results in an image pushed
    to a Docker registry, but you can also choose to run a post-build test that
    does not push an image.
    
* Image Streams and Image Stream Tags [is,istag]:
    An image stream groups sets of related images under tags - analogous to a
    branch in a source code repository. Each image stream may have one or
    more tags (the default tag is called "latest") and those tags may point
    at external Docker registries, at other tags in the same stream, or be
    controlled to directly point at known images. In addition, images can be
    pushed to an image stream tag directly via the integrated Docker
    registry.
    
* Secrets [secret]:
    The secret resource can hold text or binary secrets for delivery into
    your pods. By default, every container is given a single secret which
    contains a token for accessing the API (with limited privileges) at
    /var/run/secrets/kubernetes.io/serviceaccount. You can create new
    secrets and mount them in your own pods, as well as reference secrets
    from builds (for connecting to remote servers) or use them to import
    remote images into an image stream.
    
* Projects [project]:
    All of the above resources (except Nodes) exist inside of a project.
    Projects have a list of members and their roles, like viewer, editor,
    or admin, as well as a set of security controls on the running pods, and
    limits on how many resources the project can use. The names of each
    resource are unique within a project. Developers may request projects
    be created, but administrators control the resources allocated to
    projects.
    
For more, see https://docs.openshift.com

Usage:
  oc types [flags]

Examples:
  # View all projects you have access to
  oc get projects
  
  # See a list of all services in the current project
  oc get svc
  
  # Describe a deployment configuration in detail
  oc describe dc mydeploymentconfig
  
  # Show the images tagged into an image stream
  oc describe is ruby-centos7

Use "oc options" for a list of global command-line options (applies to all commands).
</pre>


</td>  
<td>
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#architecture-core-concepts-builds-and-image-streams">Builds</a>@Arch
<pre xxxsmall zoom>
BUILD PROCESS  (REF: <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-how-builds-work">Build</a>@Dev.Guide)
<b>
A build configuration describes:
-  build definition
-  set of triggers for when
   a new build should be created.
</b>
------------------------+-----------------------------------+-------------------------
<a href=https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#how-build-inputs-work">INPUT(@BuildConfig.spec)</a>→         BUILD                     →      OUTPUT
------------------------+-----------------------------------+-------------------------
(1) Git                 | - BuildConfig                     | - Container Image (pushed into Im.Reg specified in BuildConfig.output)
(2) Dockerfile          |    - BuildStrategy                |                  
(3) Binary              |      <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#source-build"  >Source-to-Image (S2I) </a><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#source-to-image-strategy-options">options</a>|                              
(4) Image               |      <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#pipeline-build">Pipeline              </a><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#pipeline-strategy-options"       >options</a>|                              
(5) Input secrets       |      <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#docker-build"  >Docker                </a><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#docker-strategy-options"         >options</a>|                              
(6) External artifacts  |      <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#custom-build"  >Custom                </a><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#custom-strategy-options"         >options</a>|                               
------------------------+-----------------------------------+-------------------------
                                                                ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑
                      --------------------------------------------------------------------------------------------------------------------------------------------------------------------
                      BuildConfig.spec.output KIND               | PUSH TO                                         | ENV.VARS. set in Output IMG               | Image Labels set in Output
                     --------------------------------------------+-------------------------------------------------+-------------------------------------------+-------------------------------------
                      to:                                        | OCP registry and tag in the specified IS        | OPENSHIFT_BUILD_NAME                      | io.openshift.build.commit.author    
                        kind: "ImageStreamTag"                                                                     | OPENSHIFT_BUILD_NAMESPACE                 | io.openshift.build.commit.date      
                        name: "sample-image:latest"                                                                | OPENSHIFT_BUILD_SOURCE     (source URL)   | io.openshift.build.commit.id        
                     --------------------------------------------+-----------------------------------------------  | OPENSHIFT_BUILD_REFERENCE  (Git ref used) | io.openshift.build.commit.message   
                      to:                                        | the name of the output reference                | OPENSHIFT_BUILD_COMMIT                    | io.openshift.build.commit.ref       
                        kind: "DockerImage"                      | will be used as a Docker push specification     | +"any user-defined ENV.VAR"               | io.openshift.build.source-location  
                        name: "myReg.comp.com/myimages/myimg:tag"| The spec. may contain a registry or will default|                                           | + custom ...output.imageLabels
                                                                 | to DockerHub                                    |
                     --------------------------------------------+-----------------------------------------------  |
                      "empty"                                    | then built image will not be pushed             |
                     --------------------------------------------+-----------------------------------------------  |



(1)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#source-code">Git</a>
    source code will be fetched from the location supplied.
    (inline supplied Dockerfile overwrites existing one in Git src)
    |source:
    |  git: 1
    |    uri: "https://github.com/openshift/ruby-hello-world"
    |    ref: "master"
    |  contextDir: "app/dir" 2
    |  dockerfile: "FROM openshift/ruby-22-centos7\nUSER example

(2)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dockerfile-source"         >Dockerfile</a>
(3)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#binary-source"             >Binary</a>
(4)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#image-source"              >Image</a>
(5)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#using-secrets-during-build">Input secrets</a>
(6)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#using-external-artifacts"  >External artifacts</a>
</pre>

  Example BuildConfig 
<pre xxxsmall zoom>
(Ussually a BuildConfig is automatically created during $ oc new-app ...)
Once the BuildConfig has been "tunned" refer to <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#starting-a-build">Manually control Builds</a> to see how to launch/cancel/monitor build
kind: "BuildConfig"
apiVersion: "v1"
metadata:
  name: "ruby-sample-build"
spec:
  runPolicy: "Serial"                  ← whether builds will run simultaneously 

  triggers:                            ← builds new image each time
    - type: "GitHub"                     source code changes
      github:                            or
        secret: "secret101"              container image tag
    - type: "Generic"
      generic:
        secret: "secret101"
    - type: "ImageChange"

  source: 4                           ← primary source of input (Git URL, Dockerfile, Binary payload). 
    git:                                More than one source is allowed.
      uri: "https://github.com/openshift/ruby-hello-world"

  strategy:                           ← strategy will be: Source | Docker | Custom
    sourceStrategy:                     Here we use ruby-20-centos7 container image that Source-To-Image
      from:                             will use for the application build
        kind: "ImageStreamTag"
        name: "ruby-20-centos7:latest"
  output: 6
    to:
      kind: "ImageStreamTag"
      name: "origin-ruby-sample:latest"
    imageLabels:                      ← Custom labels added to output image/IS
    - name: "vendor"
      value: "MyCompany"
    - name: "authoritative-source-url"
      value: "registry.mycompany.com"

  postCommit: 7
      script: "bundle exec rake test"


Other examples of source types:
  source:
    git:
      uri: https://github.com/openshift/ruby-hello-world.git 1
    images:
    - from:
        kind: ImageStreamTag
        name: myinputimage:latest
        namespace: mynamespace
      paths:
      - destinationDir: app/dir/injected/dir 2            ← .../somefile.jar from myinputimage will
        sourcePath: /usr/lib/somefile.jar                   be stored in <workingdir>/app/dir/injected/dir
    contextDir: "app/dir" 3                               ← work.dir. for the build 
    dockerfile: "FROM centos:7\nRUN yum install -y httpd" ← Overwrite /app/dir Dockerfile with this
</pre>


available Options for different Build Strategies
<pre xxxsmall zoom>
 |S2I Strategy            | Docker Strategy        | Custom Strategy              | Pipeline Strategy
 |------------------------+------------------------+------------------------------+-----------------------------------
 |- Incremental Builds    | - FROM Image           | - FROM Image                 | - Provide Jenkinsfile
 |- Extended Builds       | - Dockerfile Path      | - Exposing the Docker Socket |   - Embed Jenkinsfile          in build conf.
 |                        | - No Cache             |                              |   - Include a ref. to git repo in build conf. 
 |                        |                        |                              |
 |- Force Pull            | - Force Pull           |                              |
 |- Environment Variables | - Environment Variables| - Environment Variables      |
 |- secrets               | - secrets              | - Secrets                    |
</pre>
  </hr>

  <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#image-streams">ImageStream</a>
<pre xxxsmall zoom>
NOTE: Image Stream tags must not necesarelly match docker registry images
-  Rather than reference (binary container) images directly,
  application definitions typically abstract the reference
  into an image stream offering a layer of indirection.

- Protect against changes when the current image is upgraded.

- Image streams present a single virtual view of related images,
  similar to an image repository.

- Builds&amp;Deployments can be configured to trigger a new
  Build and/or Deployment when new images are added to the IS
</pre>
</td>
<td>
  <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#deployments">Deployments</a>
<pre xxxsmall zoom>
</pre>


</td>


<td>
  <a TODO  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#replication-controllers">Replication Controllers</a>
<pre xxxsmall zoom>
</pre>
  
  <a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#architecture-core-concepts-routes">Routes</a>
<pre xxxsmall zoom>
  Unique object defined 1 to 1 by service name (vs label selectors)
Options:
 - Edge Termination:
 - Pass-through Termination:
 - Re-encryption Termination:

App promotion:
Routes are the most typical resources that differ stage to stage
in the promotion pipeline, as tests against different stages of
an application access that application via its Route. 
Also, remember that you have options with regard to manual 
specification or auto-generation of host names, as well as
the HTTP-level security of the Route. 
</pre>

 <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#services">Services</a>
<pre xxxsmall zoom>
 Provides fixed IP to access (indirectly) to pods (pods IPs change at redeploy)
 Uses selectos to choose target pods.
 If more than one pod is found "round-roubing" is used.

 3 Services types:
  - (IPTables) HostPort/HosNetwork:  host/port in the "Host" (cluster visible)
  - (IPTables) NodePort: all nodes (included master) will listen in port "X" 
    redirecting to pod
  - (HAProxy?) Routes: "completelly virtual", uses unique URL.
     Can also use session affinity (sticky session based on cookies, ...)
</pre>

   <a href="">Scaling</a>
<pre xxxsmall zoom>
$ oc scale dc hello --replicas 5

$ oc rollout latest hello

HorizontalPodAutoscaler:
(HPA) Performance metrics must be active
$ oc autoscale dc/myapp \
   --min 1 --max 10 \
   --cpu-percent=80
</pre>

  DeploymentConfigs
<pre xxxsmall zoom>
App (dev|pre|pro|...) Promotion:
- primary resource for defining and scoping the environment
  for a given promotion pipeline (dev|pre|canary|pro|...) stage 
- It controls how the app tarts up.
- there will ussually be many modifications to this object
   as it changes stage.
</pre>

  (OCP External) Endpoints 
<pre xxxsmall zoom>
  Certain application-level services (e.g., database instances 
in many enterprises) may not be managed by OpenShift Container Platform.
.
.
.
</pre>

<td>
<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-templates">Templates<a/><br/>
<pre xxxsmall zoom>
Template = [BuildConfig set, ImageStream set, DeploymentConfig set, 
            Service set, Route set, secret set, ...?]
. 
. 
</pre>

Example templates<br/>
(OCP template + app src)
<pre bgorange xxxsmall zoom>
CakePHP: PHP + MySQL 
  <a class="link" href="https://github.com/openshift/origin/tree/master/examples/quickstarts/cakephp-mysql.json">Template definition</a>
  <a class="link" href="https://github.com/openshift/cakephp-ex">Source repository</a>

Django + PostgreSQL
  <a class="link" href="https://github.com/openshift/origin/tree/master/examples/quickstarts/django-postgresql.json">Template definition</a>
  <a class="link" href="https://github.com/openshift/django-ex">Source repository</a>

NodeJS + MongoDB database
  <a class="link" href="https://github.com/openshift/origin/tree/master/examples/quickstarts/nodejs-mongodb.json">Template definition</a>
  <a class="link" href="https://github.com/openshift/nodejs-ex">Source repository</a>

Rails: Ruby + PostgreSQL
  <a class="link" href="https://github.com/openshift/origin/tree/master/examples/quickstarts/rails-postgresql.json">Template definition</a>
  <a class="link" href="https://github.com/openshift/rails-ex">Source repository</a>
</pre>
</td>

</td>

 
</tr>
</table>
<br/>
  Creating/Porting Apps to OpenShift
<table>
<tr>
<td>
Openshift Full Journey
<pre xxxsmall zoom>
$ oc cluster up # once to install

# Remember: A OCP project is a k8s namespace with extra meta-data (policies, users,...)
            A project can be seen as an isolated virtual cluster of running apps/pods

# -- login into cluster --
$ oc login [-u=<username>] \
  [-p=<password>] \
  [-s=<server>] \
  [-n=<project>] \
  [--certificate-authority=</path/to/file.crt>|--insecure-skip-tls-verify]

$ oc new-project myNewProject      # alt 1
$ oc     project myExistingProject # alt 2

$ oc new-app -L  | grep cakephp  # alt 1
$ $ oc new-app --search cakephp  # alt 2
cakephp-...

$ oc new-app cakephp-mysql-example -o yaml    ← Dry run (show what will be created, do NOT run)

# next command will fetch source code,
# create a new BuildConfig,  sets up the builder
# image, builds your application image,
# create a DeploymentConfig and deploys the
# newly created image together
# with the specified environment variables. 
# The application is named "myAppAltName". 
$ oc new-app  \
    --build-env HTTP_PROXY=http://proxy.nt/ \ ← When generating apps from tmpl/src/img
    --build-env BUILD_ENV2=build_value2     \   you can use the --build-env to set
\                                               env.vars to the <b>build container</b>(@runtime)
\                                               Alt: --build-env-file=myBuildEnvVarsFile.env
\
    -env            ENVAR1=app_value1       \ ← When generating apps from tmpl/src/img
    -env            ENVAR1=app_value2       \   you can use the -e|--env to set
    -env            ENVAR1=app_value3       \   env.vars to the <b>app container</b>(@runtime)
\                                               Alt: --env-file=myEnvVarsFile.env

    -label         label1=value1           \ ← When generating apps from tmpl/src/img
                                                you can use the -l|--label to add 
                                                labels to the created objects making it easy to
                                                collectively select, configure, and delete objects
                                                associated with the application.
    -name          "myAppAltName"            ← Do not use default (github project) name 
    https://github.com/openshift/rubyHelloW  ← create first  pod from git repo (autodetect language)
    php+mysql                                ← create second pod from 2 images alternatively 

# verify the environment variables have been added
# by viewing the JSON document of the just created
# DeploymentConfig:

$ oc get dc myAppAltName -o json

# Check the bc/dc/build process:
$ oc logs -f    bc/myAppAltName-1 ← --version(opt) filter available
$ oc logs -f build myAppAltName-1 
$ oc logs -f    dc/myAppAltName-1 ← --version(opt) filter available
$ oc logs -f  pods/myAppAltName-1 ← -c "container" for multicontainer pods

$ oc get pods ← Examine running pods
$ oc status   ← Examine porject status (components+relationships)

# Expose HTTP Service externally:
$ oc expose service myAppAltName --hostname=myExternalDomainName

$ wget http://... ← Ex:check running pod REST service (if any)
</pre>
</td> 
<td>
  Adapting config files to OCP: 
<pre xxxsmall zoom>
Ex: change app config/database.yml to make it "OCP Friendly"

 ORIGINAL APP           |        OPC Friendly
 database.yml           |        database.yml
 -----------------------+----------------------------------------------------------------
                        | <% user     = ENV["DB_USER"] %>
                        | <% pass     = ENV["DB_PASS"] %>
                        | <% db_service = ENV.fetch("DATABASE_SERVICE_NAME","").upcase %>
                        | 
default: &default       | default: &default
  adapter : postgresql  |   adapter: postgresql
  encoding: unicode     |   encoding: unicode
  pool    : 5           |   pool    : <%= ENV["DB_MAX_CONNECTIONS"] || 5 %>
  host    : localhost   |   host    : <%= ENV["#{db_service}_SERVICE_HOST"] %>
  username: rails       |   username: <%= user %>
  password:             |   password: <%= pass %>
  port    : 54323       |   port    : <%= ENV["#{db_service}_SERVICE_PORT"] %>
  database: myApp01DB   |   database: <%= ENV["DB_DATABASE"] %>
                        | 
                        | "Elvis" config: 
                        | <% user = ENV.key?("DB_ADMIN_PASS") ? "root" : ENV["DB_USER"] %>
                        | <% pass = ENV.key?("DB_ADMIN_PASS") ? ENV["DB_ADMIN_PASS"] : ENV["DB_PASS"] %>
</pre>

</td>
<td>
  OCP&amp;Git
<pre xxxsmall zoom>
- OCP requires Git Installed:
  - Building an App usually requires a git remote source repo accesible by the OCP Cluster
</pre>

</td>
<td>
  [Java App] Set Up Maven Mirror
<pre xxxsmall zoom>
 - Use it for non-public artifacts and as cache.
 - Many maven-aware images have a MAVEN_MIRROR_URL ENV.VAR that can be used to simplify the process
   If not, read the Nexus documentation to configure your build properly.

STEPS:

PRE-SETUP:
Cluster Admin: Setup Read/Write Persistent Volume (PV)

$ oc new-app sonatype/nexus ← Download+deploy the official Nexus container image
               ↑↑↑↑↑↑↑↑    
             default login: admin
                     pass : admin123

$ oc expose svc/nexus       ← Create route by exposing the newly created Nexus service

$ oc get routes             ← find out the pod's new external address
(output similar to)
→ NAME      HOST/PORT                              PATH      SERVICES   PORT       TERMINATION
→ nexus     nexus-myproject.192.168.1.173.xip.io             nexus      8081-tcp
    
# (Confirm that Nexus is running with wget|curl|...) 

NOTE: Nexus comes pre-configured for the Central Repository, but you may need others
      for your application. For many Red Hat images, it is recommended to add the
      jboss-ga repository at Maven repository.

This is a good time to set up readiness and liveness probes. These will periodically check to see that Nexus is running properly.

$ oc set probe dc/nexus \         ← Set up liveness probes
    --liveness \
    --failure-threshold 3 \
    --initial-delay-seconds 30 \
    -- echo ok
$ oc set probe dc/nexus \         ← Set up readiness probe
    --readiness \
    --failure-threshold 3 \
    --initial-delay-seconds 30 \
    --get-url=http://:8081/nexus/content/groups/public


PERSISTENCE:

<span bgorange>
$ oc volumes dc/nexus --add \             ← Add PVC to the dc/nexus:
    --name 'nexus-volume-1' \               This command replaces the previous emptyDir vol. 
    --type 'pvc' \                          with a 1GB  PVC  mounted at /sonatype-work, 
    --mount-path '/sonatype-work/' \        
    --claim-name 'nexus-pv' \             ← Due to the change in configuration, pod will 
    --claim-size '1G' \                     be redeployed automatically.
    --overwrite
</span>


CREATE A BUILD TO TEST IT:

Define an (example) build using the created Nexus repository. Th

The example builder image supports ENV.VAR MAVEN_MIRROR_URL.
(othewise modify builder image to fix MVN settings)

$ export BASE_IMAGE=openshift/wildfly-100-centos7:latest
$ export    GIT_SRC=https://github.com/openshift/jee-ex.git
$ oc new-build ${BASE_IMAGE}~${GIT_SRC} \
    -e MAVEN_MIRROR_URL='http://nexus.myNexusProject:8081/nexus/content/groups/public'

$ oc logs build/jee-ex-1 --follow

CONFIRMING SUCCESS:

confirm that application’s dependencies have been creating navigating to:
http://<NexusIP>:8081/nexus/content/groups/public 

</pre>
</pre>


</td>
</tr>
</table>
<br/>
  Project Administration 
<table>
<tr>
<td>
  OpenShift Projects
<pre xxxsmall zoom>
- An OCP Project is a Kubernetes namespace (k8s virtual "subcluster") plus
  extra Openshift metadata for handling users and policies

- Itallows a community of users to organize and manage their content
  in isolation from other communities.
</pre>

  Project creation
<pre xxxsmall zoom>
<b>PRE-SETUP:</b>
  - the authenticated user must first be allowed project creation by cluster administrator.
  - max. number of projects may be limited by the system administrator. 
    Once the limit is reached, existing projects must be deleted to create new ones.

  $ oc new-project $project_name \     Ex: $ oc new-project hello-openshift \
      --description="$description" \           --description="example project demonstrating OpenShift v3" \
      --display-name="$display_name"           --display-name="Hello OpenShift"
</pre>

 List project
<pre xxxsmall zoom>
$ oc get projects
</pre>

 Swith working project: 
<pre xxxsmall zoom>
$ oc project <project_name>
</pre>

  Show Project Status
<pre xxxsmall zoom>
$ oc status ← provides high-level overview of current project,
              with its components and their relationships.
</pre>

  Delete project
<pre xxxsmall zoom>
$ oc delete project <project_name>
</pre>
</td>
<td>
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#architecture-core-concepts-projects-and-users">Project&amp;Users</a>
<pre xxxsmall zoom>
...
</pre>

  Policies
<pre xxxsmall zoom>
2 types:
  - cluster:
    $ oc describe cluster policy

  - project:
    $ oc ??? 
</pre>

  User Roles
<pre xxxsmall zoom>
$ oc policy add-role-to-user edit demo-user
</pre>


</td>


<td>
 Security Ctx Constr. (oc scc)
<pre xxxsmall zoom>
- control the actions a <b>pod</b> (vs a user) can perform
 and what resources it can access.('restricted SCC' by default)

$ oc get scc
→ NAME             PRIV  CAPS SELINUX   RUNASUSER        FSGROUP   SUPGROUP PRIORI READONLY 
→                                                                                  ROOTFS  
→ anyuid           false []   MustRunAs RunAsAny         RunAsAny  RunAsAny 10     false    
→ hostaccess       false []   MustRunAs MustRunAsRange   MustRunAs RunAsAny <none> false    
→ hostmount-anyuid false []   MustRunAs RunAsAny         RunAsAny  RunAsAny <none> false    
→ hostnetwork      false []   MustRunAs MustRunAsRange   MustRunAs MustRunAs<none> false    
→ nonroot          false []   MustRunAs MustRunAsNonRoot RunAsAny  RunAsAny <none> false    
→ privileged       true  []   RunAsAny  RunAsAny         RunAsAny  RunAsAny <none> false    
→ <b>restricted</b>       false []   MustRunAs MustRunAsRange   MustRunAs RunAsAny <none> false    
→ 
→ NAME             VOLUMES
→ 
→ anyuid           [configMap downwardAPI emptyDir              persistentVolumeClaim secret]
→ hostaccess       [configMap downwardAPI emptyDir hostPath     persistentVolumeClaim secret]
→ hostmount-anyuid [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim secret]
→ hostnetwork      [configMap downwardAPI emptyDir              persistentVolumeClaim secret]
→ nonroot          [configMap downwardAPI emptyDir              persistentVolumeClaim secret]
→ privileged       [*]
→ restricted       [configMap downwardAPI emptyDir persistentVolumeClaim secret]

$ oc describe scc anyuid
→ ...
</pre>

  Checking If Users Can Create Pods (scc-*)
<pre xxxsmall zoom>
- Using the scc-review and scc-subject-review options,
  you can see if an individual user, or a user under a
  specific service account, can create or update a pod.

- Using the scc-review option, you can check if a service
  account can create or update a pod:
  The command outputs the security context constraints that admit the resource.

Ex: check if a user with the system:serviceaccount:projectname:default service account
    can a create a pod:

  $ oc policy scc-review -z system:serviceaccount:projectname:default -f my_resource.yaml

Ex: check whether a specific user can create or update a pod:

  $ oc policy scc-subject-review -u <username> -f my_resource.yaml

Ex: check if a user belonging to a specific group can create a pod in a specific file:

$ oc policy scc-subject-review -u <username> -g <groupname> -f my_resource.yaml
</pre>
  
  Show permissions for authenticated user
<pre xxxsmall zoom>
Within your project, you can determine what <b>verbs(/actions)</b> you can
perform against all namespace-scoped resources (including third-party resources).

Test scopes in terms of the user and role:

  $ oc policy can-i --list --loglevel=8
  → (output helps you to determine what API request to make to gather the information)

To receive information back in a user-readable format:

  $ oc policy can-i --list
  → (output provides a full list)

To determine if I you can perform specific verbs:

  $ oc policy can-i <verb> <resource>

User scopes can provide more information about a given scope.
Example:
  $ oc policy can-i <verb> <resource> --scopes=user:info
</pre>
</td> 
</tr>
</table>


<br/>
Cluster Administration
<table>
<tr>
<td>
  <a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#architecture-infrastructure-components-image-registry">Image Registry</a>

<pre xxxsmall zoom>
- Multiple project in the same cluster can access the same image streams.
- Multiple clusters can access the same external registries.
- Clusters can only share a registry if the OpenShift Container Platform internal image registry is exposed via a route. 
.
.
</pre>
</td>
<td>
  <b>Storage (PV/PVC)</b><br/>
    Seq. diagram
<pre xxxsmall zoom>
STEP 1: PersistentVolume: specific resource
STEP 2: PersistentVolumeClaim : 
        request PV + min size, r/rw, ...  Used by pods.
STEP 3: Use PVC in Pod
</pre>
Pod Definition with a Claim
<pre xxxsmall zoom>
Cluster administrators create PersistenVolume(s) to isolate from the underlying OCP technology (NFS, Gluster, iSCSI,...)

Users create PersistemVolumeClaim(s) to reuse part of the PV created by cluster admins.

mounts can be:
 -  Read Only
 - Read Write Once: Only a pod can write
 -  Read Write Many: Many pods can write

(Admins can reserve a PV to a PVC)
STEP 1(admin)                            STEP 2(dev)                    STEP 3(dev)
apiVersion: v1                          |apiVersion: "v1"              |apiVersion: "v1"
kind: PersistentVolume                  |kind: "PersistentVolumeClaim" |kind: "Pod"
metadata:                               |metadata:                     |...
  name: pv0001                          |  name: <b>"claim1"</b>              |spec:
spec:                                   |spec:                         |  containers:
  capacity:                             |  accessModes:                |    -
    storage: 1Gi                        |    - "ReadWriteOnce"         |      ...
  accessModes:                          |  resources:                  |      volumeMounts:
  - ReadWriteOnce                       |    requests:                 |        -
  nfs:                                  |      storage: "1Gi"          |          mountPath: "/var/www/html"
    path: /tmp                          |  volumeName: "pv0001"        |          name: <b>"pvol"</b>
    server: 172.17.0.2                  |                              |  volumes:
  persistentVolumeReclaimPolicy: Recycle|                                   -
  claimRef:                             |                                     name: <b>"pvol"</b>
    name: <b>claim1</b>                        |                                     persistentVolumeClaim:         
    namespace: default                  |                                       claimName: <b>"claim1"</b>

</pre>

   Storage Notes:
   <ul xxxsmall zoom>
     <li>Specifying a volumeName in your PVC does not prevent a different PVC from binding to the specified PV before yours does.  Your claim will remain Pending until the PV is Available.</li>
     <li>Therefore, to avoid these scenarios and ensure your claim gets bound to the volume you want, you must ensure that both volumeName and claimRef are specified.</li>
     <li>You can tell that your setting of volumeName and/or claimRef influenced the matching and binding process by inspecting a Bound PV and PVC pair for the pv.kubernetes.io/bound-by-controller annotation. The PVs and PVCs where you set the volumeName and/or claimRef yourself will have no such annotation, but ordinary PVs and PVCs will have it set to "yes".</li>
      <li>When a PV has its claimRef set to some PVC name and namespace, and is reclaimed according to a Retain or Recycle reclaim policy, its claimRef will remain set to the same PVC name and namespace even if the PVC or the whole namespace no longer exists.</li>
   </ul>
</td>
<td>
  <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-volumes">Managing Volumes (Dev.Guide)</a>
<pre xxxsmall zoom>



</pre>


</td>
<td>
  <b>OpenVSwitch</b>
  (Software defined network)
<pre xxxsmall zoom>
Types:
  ovs-subnet
  ovs-mutitenant     (network by project)
  ovs-networkpolicy  (fine grained policy access)
</pre>


</td>
</tr>
</table>


</body>
</html>

<!-- {

$ oc get events
____________________
$ rol self-provisioning es  lo que permite crear projectos
______________
$ Groups:
  cluster:
  - cluster-admin "todo"
  - cluster-view  "ver todo"

 admin     "todo dentro de un proyecto", volume?
 edit      "no puede manipular secrets, conectar a un contenedor, ..", volume-claim
 view      "ver proyecto salvo secrets"

______________

$ oc login guarda el token en ~/.kube/config
______________
Service Accounts


Overview

When a person uses the command line or web console, their API token authenticates them to the OpenShift API. However, when a regular user’s credentials are not available, it is common for components to make API calls independently. For example:

    Replication controllers make API calls to create or delete pods

    Applications inside containers can make API calls for discovery purposes

    External applications can make API calls for monitoring or integration purposes

Service accounts provide a flexible way to control API access without sharing a regular user’s credentials.
Usernames and groups

Every service account has an associated username that can be granted roles, just like a regular user. The username is derived from its project and name: system:serviceaccount:<project>:<name>

For example, to add the view role to the robot service account in the top-secret project:

$ oc policy add-role-to-user view system:serviceaccount:top-secret:robot

Every service account is also a member of two groups:

    system:serviceaccounts, which includes all service accounts in the system

    system:serviceaccounts:<project>, which includes all service accounts in the specified project

For example, to allow all service accounts in all projects to view resources in the top-secret project:

$ oc policy add-role-to-group view system:serviceaccounts -n top-secret

To allow all service accounts in the managers project to edit resources in the top-secret project:

$ oc policy add-role-to-group edit system:serviceaccounts:managers -n top-secret

Enable service account authentication

Service accounts authenticate to the API using tokens signed by a private RSA key. The authentication layer verifies the signature using a matching public RSA key.

To enable service account token generation, update the master configuration serviceAccountConfig stanza to specify a privateKeyFile (for signing), and a matching public key file in the publicKeyFiles list:

serviceAccountConfig:
  ...
  masterCA: ca.crt 
  privateKeyFile: serviceaccounts.private.key 
  publicKeyFiles:
  - serviceaccounts.public.key 
  - ...

    CA file used to validate the API server’s serving certificate
    Private RSA key file (for token signing)
    Public RSA key files (for token verification). If private key files are provided, then the public key component is used. Multiple public key files can be specified, and a token will be accepted if it can be validated by one of the public keys. This allows rotation of the signing key, while still accepting tokens generated by the previous signer.
Managed service accounts

Service accounts are required in each project to run builds, deployments, and other pods. The managedNames setting in the master configuration file controls which service accounts are automatically created in every project:

serviceAccountConfig:
  ...
  managedNames: 
  - builder 
  - deployer 
  - default 
  - ...

    List of service accounts to automatically create in every project
    A builder service account in each project is required by build pods, and is given the system:image-builder role, which allows pushing images to any image stream in the project using the internal Docker registry.
    A deployer service account in each project is required by deployment pods, and is given the system:deployer role, which allows viewing and modifying replication controllers and pods in the project.
    A default service account is used by all other pods unless they specify a different service account.

All service accounts in a project are given the system:image-puller role, which allows pulling images from any image stream in the project using the internal Docker registry.
Infrastructure service accounts

Several infrastructure controllers run using service account credentials. The following service accounts are created in the OpenShift infrastructure namespace at server start, and given the following roles cluster-wide:

    The replication-controller service account is assigned the system:replication-controller role

    The deployment-controller service account is assigned the system:deployment-controller role

    The build-controller service account is assigned the system:build-controller role. Additionally, the build-controller service account is included in the privileged security context constraint in order to create privileged build pods.

To configure the namespace where those service accounts are created, set the openshiftInfrastructureNamespace field in the master configuration file:

policyConfig:
  ...
  openshiftInfrastructureNamespace: openshift-infra

Set limitSecretReferences field in master configuration file to true to require pod secret references to be whitelisted by their service accounts. Set its value to false to allow pods to reference any secret in the namespace.

serviceAccountConfig:
  ...
  limitSecretReferences: false

i
Service Accounts

    Overview
    Usernames and groups
    Enable service account authentication
    Managed service accounts
    Infrastructure service accounts

Overview

When a person uses the command line or web console, their API token authenticates them to the OpenShift API. However, when a regular user’s credentials are not available, it is common for components to make API calls independently. For example:

    Replication controllers make API calls to create or delete pods

    Applications inside containers can make API calls for discovery purposes

    External applications can make API calls for monitoring or integration purposes

Service accounts provide a flexible way to control API access without sharing a regular user’s credentials.
Usernames and groups

Every service account has an associated username that can be granted roles, just like a regular user. The username is derived from its project and name: system:serviceaccount:<project>:<name>

For example, to add the view role to the robot service account in the top-secret project:

$ oc policy add-role-to-user view system:serviceaccount:top-secret:robot

Every service account is also a member of two groups:

    system:serviceaccounts, which includes all service accounts in the system

    system:serviceaccounts:<project>, which includes all service accounts in the specified project

For example, to allow all service accounts in all projects to view resources in the top-secret project:

$ oc policy add-role-to-group view system:serviceaccounts -n top-secret

To allow all service accounts in the managers project to edit resources in the top-secret project:

$ oc policy add-role-to-group edit system:serviceaccounts:managers -n top-secret

Enable service account authentication

Service accounts authenticate to the API using tokens signed by a private RSA key. The authentication layer verifies the signature using a matching public RSA key.

To enable service account token generation, update the master configuration serviceAccountConfig stanza to specify a privateKeyFile (for signing), and a matching public key file in the publicKeyFiles list:

serviceAccountConfig:
  ...
  masterCA: ca.crt 
  privateKeyFile: serviceaccounts.private.key 
  publicKeyFiles:
  - serviceaccounts.public.key 
  - ...

    CA file used to validate the API server’s serving certificate
    Private RSA key file (for token signing)
    Public RSA key files (for token verification). If private key files are provided, then the public key component is used. Multiple public key files can be specified, and a token will be accepted if it can be validated by one of the public keys. This allows rotation of the signing key, while still accepting tokens generated by the previous signer.
Managed service accounts

Service accounts are required in each project to run builds, deployments, and other pods. The managedNames setting in the master configuration file controls which service accounts are automatically created in every project:

serviceAccountConfig:
  ...
  managedNames: 
  - builder 
  - deployer 
  - default 
  - ...

    List of service accounts to automatically create in every project
    A builder service account in each project is required by build pods, and is given the system:image-builder role, which allows pushing images to any image stream in the project using the internal Docker registry.
    A deployer service account in each project is required by deployment pods, and is given the system:deployer role, which allows viewing and modifying replication controllers and pods in the project.
    A default service account is used by all other pods unless they specify a different service account.

All service accounts in a project are given the system:image-puller role, which allows pulling images from any image stream in the project using the internal Docker registry.
Infrastructure service accounts

Several infrastructure controllers run using service account credentials. The following service accounts are created in the OpenShift infrastructure namespace at server start, and given the following roles cluster-wide:

    The replication-controller service account is assigned the system:replication-controller role

    The deployment-controller service account is assigned the system:deployment-controller role

    The build-controller service account is assigned the system:build-controller role. Additionally, the build-controller service account is included in the privileged security context constraint in order to create privileged build pods.

To configure the namespace where those service accounts are created, set the openshiftInfrastructureNamespace field in the master configuration file:

policyConfig:
  ...
  openshiftInfrastructureNamespace: openshift-infra

Set limitSecretReferences field in master configuration file to true to require pod secret references to be whitelisted by their service accounts. Set its value to false to allow pods to reference any secret in the namespace.

serviceAccountConfig:
  ...
  limitSecretReferences: false
_______________
Minishift:
The server is accessible via web console at:
https://192.168.64.2:8443
You are logged in as:
  User: developer
  Password: <any value>
To login as administrator:
$  oc login -u system:admin
  Logged into "https://192.168.64.2:8443" as "system:admin"

________________
<td>
    <b>Users + HTPPassword:</b><br/>
    Enable HTTP Auth. in master config.:
<pre xxxsmall zoom>
vim /etc/origin/master/master-config.yaml
  provider: 
-   #kind: denyAllPasswordIdentityProvider
+   kind: HTPPasswordIdentityProvider

$ systemctl restart ???-openshift-???
</pre>
    Adding a new user (id):
<pre xxxsmall zoom>
$ htpasswd -cb  \
    /etc/origin/master/openshift-passwd ricardo redhat
</pre>
</td>
________________________
<pre xxxsmall zoom>
REF: <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/">RedHat OpenS. Dev.Guide</a>
@startuml
title Wilfly + DDBB

participant github
participant developer as dev
participant OpenshiftConsole as cc
participant OpenshiftWebConsole as wc
participant Project/Pod as project

<b STEP 1: Create app from template</b>
dev →   github: fork
dev →       cc: oc <b>new-project</b> insultapp \
                   --display-name="Elizabe..."
cc  → +project:
dev →       git: new project repo (https://github.com/myUser/myProject)
dev →        cc: oc <b>new-app</b> \
                    wildfly:lastest~<span xxmall><a href="https://github.com/myUser/myProject">...github...</a></span> \
                    -name='insults'
                 (OpenShift will generate build&amp;deployment configs)
cc  →     +pod: new pod(project) from template
cc  →       cc: display log
note over cc
    →Found image
    ...
    →<b>Success</b>
    ...
    Run 'oc status' to view your app
end note
dev →       cc: oc <b>expose service</b> insults
                (using web con. the route is automatically created)
dev →       cc: oc <b>get routes</b>
                (verify app by visiting the URL)
  Output is similar to:
  NAME    HOST/PORT        PATH    SERVICE  LABELS
  insults insul....xip.io  insults          app=insults

note over github, project STEP 2: Create database

dev →       wc: go to Project 
                 → click Add to project
                  → filter by "postgres"
                   → Selecte Database postgresql-persistent
dev →       wc: setup conf. settings (Mem. limit, service name, ..)
dev →       wc: click create
wc  →     +pod: creates pod running database
                env vars are created:
                POSTGRESQL_PORT_5432_TCP_ADDR=172.30.76.249
                ...
dev →       cc: oc <b>env dc</b> insults \
                  -e POSTGRESQL_USER=insult
                  -e PGPASSWORD=insult
                  -e POSTGRESQL_DATABASE=insults
                # adds env.vars to DeploymentConfig (<b>common to all pods</b>)

dev →      dev: edit createSchema.sql
dev →       wc: open terminal for running postgres pod and execute:
                $ cat createSchema.sql | psql \
                  -h $POSTGRESQL_SERVICE_HOST \
                  -p $POSTGRESQL_SERVICE_PORT \
                  -U $POSTGRESQL_USER $POSTGRESQL_DATABASE 
note over dev,wc:
   secrets can be used to automate password creation
end note
dev →       dev: Edit pom.xml to add deps on driver
                 &lt;dependency&gt;
                   &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
                     &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
                     &lt;version&gt;9.4-1200-jdbc41&lt;/version&gt;
                 &lt;/dependency&gt;

<b>STEP 3: Create REST Endpoint</b>
dev →       dev: Edit pom.xml
                 &lt;dependency&gt;
                   &lt;groupIdgt;javax&lt;/groupId&gt;
                     &lt;artifactIdgt;javaee-api&lt;/artifactId&gt;
                     &lt;versiongt;7.0&lt;/version&gt;
                     &lt;scopegt;provided&lt;/scope&gt;
                 &lt;/dependency&gt;
</pre>
______________________________
 If a Jenkinsfile exists in the root or specified context directory of the source repository when creating a new application, OpenShift Container Platform generates a Pipeline build strategy. Otherwise, if a Dockerfile is found, OpenShift Container Platform generates a Docker build strategy. Otherwise, it generates a Source build strategy.

You can override the build strategy by setting the --strategy flag to either docker, pipeline or source.

$ oa new-app /home/user/code/myapp --strategy=docker
_______________________________
https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-environment-variables
___________________________
App Up&amp;Running Flow?
PRE-CONDITIONS:
  - OCP Cluster up&amp;Running
  - Project and users created

PRE-SETUP:
  - Kubernetes/OCP Cluster "admin":
      Create Persistent-Volumes for App (Pods sets to be scheduled)
    
  - OCP Project "admin":
      secret set → (Used by) → (BuildConfig set, DeploymentConfig set)

  - App Developer: 
      Creates Repository with Source Code → git_repo → (Used by) → BuildConfig

  - App Developer?: 
      Creates Dockerbuild set →(generates) → Container_Image set → ImageStream set

RECURRENT:
  - App Developer: 
      Commit to git_repo → (triggers) → New BuildConfig instance
  
- "Deploy/Run app":
    git_repo
       ↓
    BuildConfig set → (generates)→ ImageStream set → (Used by) → DeploymentConfig set

    Service     set                                → (Used by) → DeploymentConfig set
       ↓
    Route       set                                → (Used by) → DeploymentConfig set
  
  
  DeploymentConfig set → Kubernetes → Schedules pods
_____________________________________
<!-- 
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#docker-build">Docker strategy</a>
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#source-build">Source Strategy</a>
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#custom-build">Custom Strategy</a>
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#pipeline-build">Pipeline Build</a>
-->

} -->
