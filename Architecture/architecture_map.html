<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>architecture map (alpha)</title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body onLoad='onPageLoaded()'>
<b id='initialMessage' orange>Hint double-click/long-press on elements to zoom!!</b>
<div id='zoomDiv'></div>
<div style="position:fixed; right:0.3%; bottom:0; width:auto;">
<b style="font-size:1.5rem" orange><a onclick="onZoomOut()">[-A]</a></b>
<b style="font-size:1.5rem"       >                                 </b>
<b style="font-size:2.0rem" orange><a onclick="onZoomIn ()">[A+]</a></b>
</div>
<table>
<tr>
<td>
<pre xxsmall zoom>
Tags:
[performance]
[redundancy]
[log]        Loggin
[monitoring]
[analytics]
[testing]
[CI]         Continuous Integration
[IaC]        Infrastructure as code
</pre>
</td>


<td>
  [log]<a href="https://www.fluentd.org/">FluentD</a><br/>
  (Improved "logstat")
<pre xxxsmall zoom { >
Open Source data collector for unified logging layer.

Fluentd allows you to unify data collection and consumption for
a better use and understanding of data.

Syslog                      Elasticsearch
Apache/Nginx logs    → → →  MongoDB
Mobile/Web app logs  → → →  Hadoop
Sensors/IoT                 AWS, GCP, ...
</td>

<td>
  Hadoop:
  Hadoop "vs" Spark
  <a href="https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html">REF</a>
<pre xxxsmall zoom>
Hadoop is essentially a distributed data infrastructure: 
 -It distributes massive data collections across multiple nodes 
  within a cluster of commodity servers
 -It also indexes and keeps track of that data, enabling
  big-data processing and analytics far more effectively
  than was possible previously. 
Spark, on the other hand, is a data-processing tool that operates on those 
distributed data collections; it doesn't do distributed storage.

You can use one without the other: 
  - Hadoop includes not just a storage component, known as the 
  Hadoop Distributed File System, but also a processing component called 
  MapReduce, so you don't need Spark to get your processing done.
  - Conversely, you can also use Spark without Hadoop. Spark does not come with
  its own file management system, though, so it needs to be integrated with one
  -- if not HDFS, then another cloud-based data platform. Spark was designed for
  Hadoop, however, so many agree they're better together.

Spark is generally a lot faster than MapReduce because of the way it processes 
data. While MapReduce operates in steps, Spark operates on the whole data set 
in one fell swoop:
   "The MapReduce workflow looks like this: read data from the cluster, perform
    an operation, write results to the cluster, read updated data from the 
    cluster, perform next operation, write next results to the cluster, etc.," 
    explained Kirk Borne, principal data scientist at Booz Allen Hamilton. 
    Spark, on the other hand, completes the full data analytics operations 
    in-memory and in near real-time: 
    "Read data from the cluster, perform all of the requisite analytic 
    operations, write results to the cluster, done," Borne said.
Spark can be as much as 10 times faster than MapReduce for batch processing and 
p to 100 times faster for in-memory analytics, he said.
  You may not need Spark's speed. MapReduce's processing style can be just fine 
if your data operations and reporting requirements are mostly static and you 
can wait for batch-mode processing. But if you need to do analytics on 
streaming data, like from sensors on a factory floor, or have applications that
require multiple operations, you probably want to go with Spark.
 Most machine-learning algorithms, for example, require multiple operations. 

Recovery: different, but still good. 
Hadoop is naturally resilient to system faults or failures since data 
are written to disk after every operation, but Spark has similar built-in
resiliency by virtue of the fact that its data objects are stored in something 
called resilient distributed datasets distributed across the data cluster. 
"These data objects can be stored in memory or on disks, and RDD provides full 
recovery from faults or failures," Borne pointed out.
</pre>
</td>

<td>
  <a href="https://prometheus.io/">Prometheus</a> 
  <a xxsmall href="https://github.com/prometheus/prometheus">Git</a><br/>
  monitoring metrics analyzer and alerting
  <ul xxxsmall zoom>
  <li>stores all data as time series: streams of timestamped values belonging
    to the same metric and the same set of labeled dimensions.
  </li>
  <li>provides an expression language to  select and aggregate time
    series data in real time. The result can either be shown as a graph,
    tabular data or consumed by external systems via HTTP API.
  </li>
  <li>Grafana supports querying Prometheus</li>
  <li>(optionally) integrates with remote storage systems.</li>
  <li>alerts can be defined using expression language
    to send notifications about firing alerts to an external service</li>
  </ul>
</td>
<td>
  <a href="http://kafka.apache.org/">Kafka</a> <br/>
  log and events collection , (persistent fault-tolerant) streaming real-time processing platform, ...<br/>
  Used for two broad classes of applications:
    real-time streaming data pipelines that reliably get data between systems or applications<br/>
    real-time streaming applications that transform or react to the streams of data <br/>
  <ul xxxsmall zoom>
  <li>Each broker in the Kafka cluster has an identity which can be used to find other
brokers in the cluster. The brokers also need some type of a database to 
store partition logs. It's important to configure a Persistent Volume (PV) for 
Kafka, otherwise you will lose the logs.
  </li>
  </ul>
  <a href="http://kafka.apache.org/uses">(Popular) Use cases</a>
  <ul xxxsmall zoom>
  <li><def>Messaging</def> works well as a replacement for a more traditional 
    message broker.  Message brokers are used for a variety of reasons 
   (decoupling processing from data producers, buffering unprocessed messages, 
   etc). In comparison Kafka has better throughput, built-in partitioning, 
   replication, and fault-tolerance which makes it a good solution for large 
   scale message processing applications.
  </li>
  <li><def>Website Activity Tracking</def> was the original use case for Kafka
    to be able to rebuild a user activity tracking pipeline as a set of
    real-time publish-subscribe feeds. Page views, searches, .... is published
    to central topics with one topic per activity type. These feeds are 
    available for subscription for a range of use cases including real-time 
    processing, real-time monitoring, and loading into Hadoop or offline data 
    warehousing systems for offline processing and reporting.
  </li>
  <li><def>Metrics</def> operational monitoring data. This involves aggregating
    statistics from distributed applications to produce centralized feeds of 
    operational data.
  </li>
  <li><def>Log Aggregation</def> replacement for a log aggregation solution. 
    Log aggregation typically collects physical log files off servers and puts 
    them in a central place (a file server or HDFS perhaps) for processing. 
    Kafka abstracts away the details of files and gives a cleaner abstraction 
    of log or event data as a stream of messages. This allows for lower-latency
    processing and easier support for multiple data sources and distributed 
    data consumption. In comparison to log-centric systems like Scribe or Flume,
     Kafka offers equally good performance, stronger durability guarantees due 
     to replication, and much lower end-to-end latency.
  </li>
  <li><def>Stream Processing</def>: Kafka process data in processing pipelines 
   consisting of multiple stages, where raw input data is consumed from Kafka 
   topics and then aggregated, enriched, or otherwise transformed into new 
   topics for further consumption or follow-up processing. For example, a 
   processing pipeline for recommending news articles might crawl article 
   content from RSS feeds and publish it to an "articles" topic; further 
   processing might normalize or deduplicate this content and published the 
   cleansed article content to a new topic; a final processing stage might 
   attempt to recommend this content to users. Such processing pipelines 
   create graphs of real-time data flows based on the individual topics. 
   Starting in 0.10.0.0, a light-weight but powerful stream processing library 
   called Kafka Streams is available in Apache Kafka to perform such data 
   processing as described above. Apart from Kafka Streams, alternative open 
   source stream processing tools include Apache Storm and Apache Samza.
  </li>
  <li><def>Event Sourcing</def> style of application design where state 
    changes are logged as a time-ordered sequence of records.
    Kafka's support for very large stored log data makes it an excellent 
    backend for an application built in this style.
  </li>
  <li><def>Commit Log</def>: Used as kind of external commit-log for a
    distributed system. The log helps replicate data between nodes and acts as 
    a re-syncing mechanism for failed nodes to restore their data. The log 
    compaction feature in Kafka helps support this usage. In this usage Kafka 
    is similar to Apache BookKeeper project. </li>
  </ul>
</td>
</tr>
</table>

<table>
<tr>

  <td>
  <a href="http://spark.apache.org/">Apache Spark</a><br/>
  Cluster computing platform
  <ul xxxsmall zoom>
  <li>
  </li>
  </ul>
<pre xxxsmall zoom { >
  general framework for large-scale data processing that supports lots of 
different programming languages and concepts such as MapReduce, in-memory
processing, stream processing, graph processing, and Machine Learning. This can
also be used on top of Hadoop. Data can be ingested from many sources like 
Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex 
algorithms expressed with high-level functions like map, reduce, join and window.

Common applications for Spark include real-time marketing campaigns, online 
product recommendations, cybersecurity analytics and machine log monitoring.
</pre } >
<br/>
Kafka vs Spark Streaming: <a href="https://dzone.com/articles/spark-streaming-vs-kafka-stream-1">REF</a>
<p xxxsmall zoom { >
""" If event time is very relevant and latencies in the seconds range are
completely unacceptable, Kafka should be your first choice. Otherwise, 
Spark works just fine. <br/>
...
Apache Spark can be used with Kafka to stream the data, but if you are 
deploying a Spark cluster for the sole purpose of this new application, that is
definitely a big complexity hit.<br/>
...
Conclusion
I believe that Kafka Streams is still best used in a "Kafka > Kafka" context, 
while Spark Streaming could be used for a "Kafka > Database" or 
"Kafka > Data science model" type of context.
</p } >
  </td>
<td>
  <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a><br/>
  distributed, RESTful (all-document-type) search and analytics engine
  <ul xxxsmall zoom>
  <li>Implemented on top of Lucene</li>
  <li>Elasticsearch is developed alongside a data-collection and log-parsing engine
     called Logstash, and an analytics and visualisation platform called Kibana.
     The three products are designed for use as an integrated solution, referred
     to as the "Elastic Stack" (formerly the "ELK stack").</li>
  <li>At the heart of the Elastic Stack,  data is stored centrally</li>
  <li>""discover the expected and uncover the unexpected""</li>
  </ul>
</td>

<td>  
  <a href="https://www.elastic.co/products/kibana">Kibana</a><br/>
  <span cite>A Picture's Worth a Thousand Log Lines</span>
  <ul xxxsmall zoom>
  <li>""visualize (Elasticsearch) data and navigate the Elastic Stack,
  learning  understanding the impact rain might have on your 
  quarterly numbers""</li>
  </ul>

  
</td>

  <td>
  <a href="https://grafana.com/">Grafana</a><br/>
  time series analytics
  <br/>
  <a xsmall href="https://logz.io/blog/grafana-vs-kibana/">Grafana vs Kibana</a>
  </td>
</tr>
</table>

<table>
<tr>
  <th colspan=3 header_delimit xsmall >distributed cache</th>
  <td colsep ></td>
  <th colspan=1 header_delimit xsmall >Local InMemory Cache</th>
</tr>
<tr>
<td>
  <a href="https://www.memcached.org/">Memcached</a><br/>
  distributed memory object caching system
.
  <ul>
  <li>Memcached servers are unaware of each other. There is no crosstalk, no 
     syncronization, no broadcasting, no replication. Adding servers increases 
     the available memory. Cache invalidation is simplified, as clients delete 
     or overwrite data on the server which owns it directly</li> 
  <li>initially intended to speed up dynamic web applications alleviating database load</li> 
  </ul> 
 
  <a href="https://github.com/magro/memcached-session-manager">
    Memcached-session-manager</a>
  tomcat session manager that keeps sessions in memcached or Redis, for highly 
  available, scalable and fault tolerant web applications
  <ul>
  <li>supports sticky and non-sticky configurations</li>
  <li>Failover is supported via migration of sessions</li>
  </ul>
</td>
<td><a href="https://redis.io/">Redis</a>
  in-memory data structure store, used as a database, cache and message broker
  <ul xxxsmall zoom >
  <li>supports data structures such as strings, hashes, lists, sets, sorted sets
    with range queries, bitmaps, hyperloglogs and geospatial indexes with radius queries</li>
  <li>Redis has built-in replication, Lua scripting, LRU eviction, transactions
    and different levels of on-disk persistence, and provides high availability
    via Redis Sentinel and automatic partitioning with Redis Cluster</li>
  </ul>
</td>
</td>
<td>
  <a href="http://www.ehcache.org/">Ehcache</a>
  <ul xxxsmall zoom >
  <li>Can be used as tcp service (distributed cache) or process-embedded<br/>
    <span TODO>Same API for local and distributed objects?</span></li>
  <li>open source, standards-based cache that boosts performance, offloads I/O</li>
  <li>Integrates with other popular libraries and frameworks</li>
  <li>It scales from in-process caching, all the way to mixed in-process/out-of-process
  deployments with terabyte-sized caches</li>
  </ul>
<pre xxxsmall zoom { >
Coding to Ehcache 3 API:
<pre xxxsmall { >
CacheManager cacheManager = CacheManagerBuilder.newCacheManagerBuilder()
    .withCache("preConfigured",
         CacheConfigurationBuilder.newCacheConfigurationBuilder(Long.class, String.class,
             ResourcePoolsBuilder.heap(100))
         .build())
    .build(true);

Cache&lt;Long, String&gt; preConfigured
    = cacheManager.getCache("preConfigured", Long.class, String.class);

Cache&lt;Long, String&gt; myCache = cacheManager.createCache("myCache",
    CacheConfigurationBuilder.newCacheConfigurationBuilder(Long.class, String.class,
                                  ResourcePoolsBuilder.heap(100)).build());

myCache.put(1L, "da one!");
String value = myCache.get(1L);

cacheManager.close();
</pre }>
(simpler/lighter solution but not so escalable could be to use Google Guava Cache)
  <br/>
  <a href="http://jbosscache.jboss.org/">JBoss Cache</a>
</td>
<td colsep>
<td><a href="https://github.com/google/guava/wiki/CachesExplained">Guava Cache</a>
  <b>non-distributed</b> easy-to-use Java library for data caching
  <ul xxxsmall zoom >
  <li>A Cache is similar to ConcurrentMap, but not quite the same. The most 
    fundamental difference is that a ConcurrentMap persists all elements that 
    are added to it until they are explicitly removed. A Cache on the other 
    hand is generally configured to evict entries automatically, in order to 
    constrain its memory footprint. In some cases a LoadingCache can be useful
    even if it doesn't evict entries, due to its automatic cache loading.
  </li>
  </ul>
</td>
</tr>
</table>


<table>
<tr>
<td>
  <b>Message Queues</b>
  Defined by
  <ul xxxsmall zoom>
  <li>message oriented architecture</li>
  <li>Persistence (or durability until comsuption)</li>
  <li>queuing</li>
  <li>Routing: point-to-point / publish-and-subscribe</li>
  <li>No processing/transformation of message/data</li>
  </ul>
  Implementations
  <ul xxxsmall zoom>
  <li>
    <a href="https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol">AMQP</a>
    Open standard application layer protocol for message-oriented middleware. <br/>
    Often compared to JMS (Java Message Service): <br/>
    JMS defines the API interfaces, while AMQP defines the network protocol.<br/>
     JMS has no requirement for how messages are formed and transmitted and thus 
    every JMS broker can implement the messages in a different (incompatible) format<br/>
     AMQP publishes its specifications in a downloadable XML format. 
    This availability makes it easy for library maintainers to generate APIs 
    driven by the specs while also automating construction of algorithms to 
    marshal and demarshal messages.<br/>
     These advantages and the openness of the spec have inspired the creation of
     multiple brokers that support AMQP, including:<br/>
    RabbitMQ, ActiveMQ, Qpid, Solace
  </li>
  <li>Java JMS</li>
  <li>...</li>
  </ul>
</td>
<td>
  <b>Message Brokers</b>
  <ul xxxsmall zoom>
  <li>Routing</li>
  <li>(De-)Multiplexing of messages from/into multiple messages to different recipients</li>
  <li>Durability</li>
  <li>Transformation (translation of message between formats)</li>
  <li>"things usually get blurry - many solutions are both (message queue and message
    broker) - for example RabbitMQ or QDB.  Samples for message queues are 
    Gearman, IronMQ, JMS, SQS or MSMQ."<br/>
    Message broker examples are Qpid, Open AMQ or ActiveMQ.</li>
  </ul>
</td>
<td>
  <b>Enterprise Service Bus (ESB)</b>
  Can be defined by next feautes:
  <ul xxxsmall zoom>
  <li>Monitoring of services/messages passed between them</li>
  <li>wire Protocol bridge between HTTP, AMQP, SOAP, gRPC, CVS in Filesystem,...</li>
  <li>Scheduling, mapping, QoS management, error handling, ..</li>
  <li>Data transformation</li>
  <li>Data pipelines</li>
  <li>Mule, JBoss Fuse (Camel + "etc..."), BizTalk, Apache ServiceMix, ...</li>
  <li>
<pre>
REF: https://en.wikipedia.org/wiki/Enterprise_service_bus#/media/File:ESB_Component_Hive.png
    ^   Special App. Services
    |
E   |   Process Automation                 BPEL, Workflow
n   |
t   |   Application Adapters               RFC, BABI, IDoc, XML-RPC, ...
e m |
r e |   Application Data Consolidation     MDM, OSCo, ...
p s |
r s |   Application Data Mapping           EDI, B2B
i a |   _______________________________
s g |   Business Application Monitoring
e e |   _______________________________
    |   Traffic Monitoring Cockpit
S c |
e h |   Special Message Services           Ex. Test Tools
r a |
v n |   Web Services                       WSDL, REST, CGI
i n |
c e |   Protocol Conversion                XML, XSL, DCOM, CORBA
e l |
    |   Message Consolidation              N.N (data locks, multi-submit,...)
B   |
u   |   Message Routing                    XI, WBI, BIZTALK, Seeburger
s   |
    |   Message Service                    MQ Series, MSMQ, ...
</pre>

  </li>
  </ul>
</td>
</tr>
</table>




<table>
<tr>
<th colspan=10 header_delimit xsmall >Example Architectures</th>
</tr>
<tr>
<td>
observability: login + monitoring + tracing

  Ex: (REF: <a href="https://www.rsaconference.com/writable/presentations/file_upload/csv-r14-fim-and-system-call-auditing-at-scale-in-a-large-container-deployment.pdf">File Integrity Monitoring at scale: (RSA Conf)</a>)<br/>
  Auditing log to gain insights at scale:
<pre xxxsmall zoom>
                         +→ Pagerduty
            +-→ Grafana -+→ Email
   Elastic  |            +→ Slack
   Search  -+-→ Kibana
            |
            +-→ Pre-processing  -> TensorFlow

Alt1:
  User   │ go-audit-                          User space
  land   │ container                             app
  ───────├─────  Netlink ───── Syscall iface ───────────
  Kernel │        socket           ^
         │          ^              |
                    └─  Kauditd ───┘
</pre>
</td>
</tr>
</table>


<table>
<tr>
  <th header_delimit colspan=10  xsmall >Continuous Integration</td>
</tr>
<tr>
<td>[CI]<a href="https://jenkins.io/doc/">Jenkins</a>
   <ul xxxsmall zoom>
   <li><a href="https://jenkins.io/solutions/pipeline/">Pipeline as Code</a></li>
   </ul>
</td>
<td>
  [CI]<a href="https://github.com/spinnaker/kayenta">Kayenta</a> Canary Testing
  <ul>
  <li>Kayenta is a platform for Automated Canary Analysis (ACA) </li>
  </ul>
</td>

</tr>
</table>

<br/>
</body>

<!--
https://thenewstack.io/5-things-to-know-before-adopting-microservice-and-container-architectures/
____________________
________________
Full list of CI tools
https://xebialabs.com/technology/nevercode/
_____________
https://en.wikipedia.org/wiki/Infrastructure_as_Code
Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools
_____________
http://www.terracotta.org/

https://en.wikipedia.org/wiki/Terracotta,_Inc.

    BigMemory: a commercial in-memory data management suite for real-time Big Data applications that stores up to hundreds of terabytes in distributed memory across multiple servers without performance hits from the Java garbage collector
    Ehcache: an open source, standards-based cache used to boost performance, offload the database and simplify scalability[5]

    Enterprise Ehcache: a commercial product based on the open source Ehcache project

    Quartz: an open source job scheduling service

    Quartz Scheduler: a commercial product, based on the open source Quartz project

    Web Sessions: a commercial product that provides high availability for Java Servlet Sessions, allowing user session data to survive server restarts and failures[6]
    Terracotta Enterprise Suite: a commercial product that includes BigMemory, Web Sessions and Quartz Scheduler
    In-Genius: a native, in-memory platform for real-time insight and intelligent action on Big Data. General availability is planned for the third quarter of calendar 2013

___________________________
Redis

Cloudify
___________________

Spring  Batch:

FROM https://stackoverflow.com/questions/33188368/spring-batch-vs-quartz-jobs

Quartz is a scheduling framework. Like "execute something every hour or every last friday of the month"

Spring Batch is a framework that defines that "something" that will be executed. You can define a job, that consists of steps. Usually a step is something that consists of item reader, optional item processor and item writer, but you can define a custom stem. You can also tell Spring batch to commit on every 10 items and a lot of other stuff.  From Spring 2 , it can also schedule tasks
(See also https://jcp.org/en/jsr/detail?id=352, Batch applications for the Java Platform)
_________________________
- Stress monitoring in Kubernetes
  """How to achieve a complete monitoring in Kubernetes with prometheus + elasticsearch + grafana + opsgenie"""

paradigm shift: monitoring/visibility moves from  operations team to most of the developers

Prometheus for developers: Instrumenting applications with prometheus and Grafana
  """ We will present how developers can instrument their applications with
     different technologies (nodejs, springboot and .net core) to get technical and business metrics"""


Beyond Docker monitoring and security: troubleshooting and forensics with Sysdig
  """The Sysdig container intelligence platform provides a unified platform to deliver monitoring,
    security, troubleshooting and forensics in a microservices-friendly platform. In this session we will demo how Sysdig helps to:
  - Monitor and alerting on Kubernetes infrastructure, services and applications
  - Unified interface to troubleshoot issues inside containers with full system visibility
  - Implement microservices based security policies looking at run-time behavior
  - Enable post-mortem analysis and forensics on containers after they are long gone
  """

Full Stack Visibility with Elastic: Logs, Metrics and Traces
  """"With microservices every outage is like a murder mystery""" is a common complaint. But it doesn't have to be!
  This talk gives an overview on how to monitor distributed applications. We dive into:
  - System metrics: Keep track of network traffic and system load.
  - Application logs: Collect structured logs in a central location.
  - Audit info: Watch for user and processes activity in the system.
  - Uptime monitoring: Ping services and actively monitor their availability and response time.
  - Application metrics: Get metrics and health information from for application via REST or JMX.
  - Request tracing: Gather timing data by using Elastic APM to retrieve and show call traces.
______________
<a href="https://www.opsgenie.com/">opsgenie</a>
______________
https://www.terraform.io/
   Terraform: Infrastructure as Code: """HashiCorp Terraform enables you to safely and predictably create, change, and improve infrastructure. It is an open source tool that codifies APIs into declarative configuration files that can be shared amongst team members, treated as code, edited, reviewed, and versioned."""

   IaC: define stacks de servicios, security groups, networks 
Terraform: common wrapper to AWS CloudFormations, Azure ????, OpenStack Heat?, ...
Nexus
______________________

typical uses: running a cluster storage daemon (glusterd, ceph,...), log collection daemon (fluentd, logstash), node monitoring (Prometheus,  Node Exporter, collectd, Datadog agent, New Relic agent, or Ganglia gmond)
____________

  Ex.  DaemonSet for fluentd-elasticsearch:
____________
From K8s map:
... Service deployments and batch processing pipelines are often multi-dimensional entities (e.g., multiple partitions or deployments, multiple release tracks, multiple tiers, multiple micro-services per tier). Management often requires cross-cutting operations, which breaks encapsulation of strictly hierarchical representations, especially rigid hierarchies determined by the infrastructure rather than by users....
_________________
Tiers: frontend, backend, cache, persistence, ...?
___________________
https://www.howtoforge.com/tutorial/ubuntu-log-io-realtime-log-viewer/
Log.io is a simple but effective log management tool built on top of Node.js and Socket.io that monitors the system and provides real-time information on the system logs in a web browser. Log.io consists of two services, log.io server and harvester. Harvester watches the log files listed in harvester configuration file and send them to log.io server. Then log.io server broadcast all the messages coming from harvester to client browser.
___________________
https://techdecoded.intel.io/resources/harp-daal-for-high-performance-big-data-computing/?cid=em-elq-35946&utm_source=elq&utm_medium=email&utm_campaign=35946&elq_cid=3023559
Harp-DAAL for High-Performance Big Data Computing
The Key to Simultaneously Boosting Productivity and Performance

Judy Qiu, Intelligent Systems Engineering Department, School of Informatics, Computing, and Engineering, Indiana University
Email
@IntelDevTools| No ratings yet. Rate this

Large-scale data analytics is revolutionizing many business and scientific domains. And easy-to-use, scalable parallel techniques are vital to process big data and gain meaningful insights. This article introduces a novel high-performance computing (HPC)-cloud convergence framework named Harp-DAAL and demonstrates how the combination of big data (Hadoop*) and HPC techniques can simultaneously boost productivity and performance.

Harp* is a distributed Hadoop-based framework that orchestrates node synchronization1. Harp uses Intel® Data Analytics Acceleration Library (Intel® DAAL)2 for its highly-optimized kernels on Intel® Xeon® and Xeon Phi\u2122 processor architectures. This way, the high-level API of big data tools can be combined with intranode, fine-grained parallelism that\u2019s optimized for HPC platforms.

We illustrate this framework in detail with K-means clustering, a compute-bound algorithm used in image clustering. We also show the broad applicability of Harp-DAAL by discussing the performance of three other big data algorithms:

    Subgraph Counting by color coding
    Matrix Factorization
    Latent Dirichlet Allocation

These algorithms share characteristics such as load imbalance, irregular structure, and communication issues that create performance challenges.

The categories in Figure 1 illustrate a classification of data-intensive computation into five models that map into five distinct system architectures. It starts with Sequential, followed by centralized batch architectures corresponding exactly to the three forms of MapReduce: Map-Only, MapReduce, and Iterative MapReduce. Category five is the classic Message Passing Interface (MPI) model.

Harp brings Hadoop users the benefits of supporting all five classes of data-intensive computation, from naturally parallel to machine learning and simulation. It expands the applicability of Hadoop (with a Harp plugin) to more classes of big data applications, especially complex data analytics such as machine learning and graph analysis. The design consists of a modular software stack with native kernels (with Intel DAAL) to effectively utilize scale-up servers for machine learning and data analytics applications. Harp-DAAL shows how simulations and big data can use common programming environments with a runtime based on a rich set of collective operations and libraries.

Figure 1 \u2013 Cloud-HPC interoperable software for high-performance big data analytics at scale

Interfacing Harp and Intel DAAL

Intel DAAL provides a native C/C++ API but also provides interfaces to higher-level programming languages such as Java* and Python*. Harp is written in Java and extended from the Hadoop ecosystem, so Java was the natural choice to interface Harp and Intel DAAL.

In Harp-DAAL, data is stored in a hierarchical data structure called Harp-Table, which consists of tagged partitions. Each partition contains a partition ID (metadata) and a user-defined serializable Java object such as a primitive array. When doing communication, data is transferred among distributed cluster nodes via Harp collective communication operations. When doing local computation, data moves from Harp-Table (JVM heap memory) to Intel DAAL native kernels (off-JVM heap memory). Copying data between a Java object and C/C++ allocated memory space is unavoidable. Figure 2 illustrates two approaches to this data copy:

    Direct Bulk Copy: If an Intel DAAL kernel allocates a continuous memory space for dense problems, Harp-DAAL will launch a bulk copy operation between a Harp-Table and a native memory address.
    Multithreading Irregular Copy: If an Intel DAAL kernel uses an irregular and sparse data structure\u2015which means the data will be stored in non-consecutive memory segments\u2015Harp-DAAL performs a second copy operation using Java/OpenMP threads, where the threads transfer data segments concurrently.

Figure 2 \u2013 Direct Bulk Copy (left) versus Multi-Threading Irregular Copy (right)

Applying Harp-DAAL to Data Analytics

K-means is a widely-used and relatively simple clustering algorithm that provides a clear example of how to use Harp-DAAL. K-means uses cluster centers to model data and converges quickly via iterative refinement. K-means clustering was performed on a large image dataset from Flickr*, which includes 100 million images, each with 4,096 dimensional deep features extracted using a deep convolutional neural network model trained on ImageNet*. Data preprocessing includes format transformation and dimensionality reduction from 4,096 to 128 using Principal Component Analysis (PCA). (The Harp-DAAL tutorial contains a description of the data preparation.)3

Harp-DAAL provides modular Java functions for developers to customize the K-means algorithm as well as tuning parameters for end users. The programming model consists of map functions linked by collectives. The K-means example takes seven steps.

Step 1: Load Training Data (Feature Vectors) and Model Data (Cluster Centers)

Use this function to load training data from the Hadoop Distributed File System (HDFS):

// create a pointArray
List<double[]> pointArrays = LoadTrainingData();

Similarly, create a Harp table object cenTable and load centers from HDFS. Since centers are requested by all the mappers, the master mapper will load and broadcast them to all other mappers. Different center initialization methods can be supported in this fashion:

// create a table to hold cluster centers
Table<DoubleArray> cenTable = new Table<>(0, new DoubleArrPlus());

if (this.isMaster()) {
  createCenTable(cenTable);
  loadCentroids(cenTable);
}

// Bcast centers to other mappers
bcastCentroids(cenTable, this.getMasterID());

Step 2: Convert Training Data from Harp to Intel DAAL

The training data loaded from HDFS is stored in the Java heap memory. To invoke the Intel DAAL kernel, this step converts the data to an Intel DAAL NumericTable, which includes allocating the native memory for the NumericTable and copying data from pointArrays to trainingdata_daal.

// convert training data from Harp to DAAL
NumericTable trainingdata_daal = convertTrainData(pointArrays);

Step 3: Create and Set Up an Intel DAAL K-means Kernel

Intel DAAL provides Java APIs to invoke native kernels for K-means on each node. It is called with:

    A specification of the input training data object
    The number of centers
    The number of threads to be used by the thread scheduler (Intel® Threading Building Blocks and Intel® Math Kernel Library)

// create a DAAL K-means kernel object
DistributedStep1Local kmeansLocal = new DistributedStep1Local(daal_Context,
Double.class, Method.defaultDense, this.numCentroids);
// set up input training data
kmeansLocal.input.set(InputId.data, trainingdata_daal);
// specify the number of threads used by DAAL kernel
Environment.setNumberOfThreads(numThreads);
// create cenTable on DAAL side
NumericTable cenTable_daal = createCenTableDAAL();

Step 4: Convert Center Format from Harp to Intel DAAL

The centers are stored in the Harp table cenTable for inter-process (mapper) communication. The centers are converted to Intel DAAL format at each iteration.

// Convert center format from Harp to DAAL
convertCenTableHarpToDAAL(cenTable, cenTable_daal);

Step 5: Local Computation by Intel DAAL Kernel

Call Intel DAAL K-means kernels of local computation at each iteration.

// specify cluster centers to DAAL kernel
kmeansLocal.input.set(InputId.inputCentroids, cenTable_daal);
// first step of local computation by using DAAL kernels to get a partial result
PartialResult pres = kmeansLocal.compute();

Step 6: Inter-Mapper Communication

Harp-DAAL K-means uses an AllReduce computation model where each mapper keeps a local copy of the whole model data (cluster centers). However, Harp provides different communication operations to synchronize model data among mappers:

    regroup & allgather (default)
    allreduce
    broadcast & reduce
    push & pull

In a regroup & allgather operation, it first combines the same center from different mappers and redistributes them to mappers by a specified order. After averaging the centers, an allgather operation makes every mapper get a complete copy of the averaged centers.

comm_regroup_allgather(cenTable, pres);

In an allreduce operation, the centers are reduced and copied to every mapper. Then, on each mapper, an average operation is applied to the centers:

comm_allreduce(cenTable, pres);


At the end of each iteration, call printTable to check the clustering result:

// for iteration i, print the first ten centers of the first ten dimensions
printTable(cenTable, 10, 10, i);

Step 7: Release Memory and Store Cluster Centers

After all of the iterations, release the memory allocated for Intel DAAL and for the Harp table object. The center values are stored on HDFS as the output:

// free memory and record time
cenTable_daal.freeDataMemory();
trainingdata_daal.freeDataMemory();
// Write out the cluster centers
if (this.isMaster()) {
     KMUtil.storeCentroids(this.conf, this.cenDir,
     cenTable, this.cenVecSize, \u201coutput\u201d);
}
cenTable.release();


Performance Results

The performance for Harp-DAAL is illustrated by the results for four applications<sup>4,5</sup> with different algorithmic features:

    K-means: A dense clustering algorithm with regular memory access
    MF-SGD (Matrix Factorization for Stochastic Gradient Descent): A dense recommendation algorithm with irregular memory access and large model data
    Subgraph Counting: A sparse graph algorithm with irregular memory access
    Latent Dirichlet Allocation (LDA): A sparse, topic modeling algorithm with large model data and irregular memory access

The testbed has two clusters:

    One with Intel Xeon E5 2670 processors and the InfiniBand Interconnect*
    One with Intel Xeon Phi 7250 processors and the Intel® Omni-Path Interconnect

In Figure 3, Harp-DAAL achieves around a 30x speedup over Spark* for K-means on 30 nodes using its highly vectorized kernels from Intel Math Kernel Library, which is part of Intel DAAL. MF-SGD was run on up to 30 nodes and achieved a 3x speedup over NOMAD*, a state-of-the-art MPI C/C++ solution. The benefits come from Harp\u2019s rotation collective operation that accelerates the communication of the big model data in the recommender system.

Figure 3 \u2013 Performance comparison on three different important machine learning algorithms:
K-means, MF-SGD, and Subgraph counting

Harp-DAAL subgraph counting on 16 Intel Xeon E5 processor-based nodes has a 1.5x to 4x speedup over MPI-Fascia for large subtemplates with billion-edged Twitter graph data. The performance improvement comes from node-level pipeline overlapping of computation and communication. Single-node thread concurrency improved by neighbor list partitioning of the graph vertex.

Figure 4 shows that Harp LDA achieves better convergence and speedup over other state-of-the-art MPI implementations such as LightLDA* and NomadLDA*5. This advantage comes from two optimizations for parallel efficiency:

    Harp adopts the rotation computation model for inter-node communication of the latest model, and at the same time utilizes timer control to reduce the overhead of synchronization.
    At the intra-node level, a dynamic scheduling mechanism is developed to mitigate load imbalance.

Figure 4 \u2013 Performance of various LDA implementations on the Clueweb dataset (30 billion tokens, 5,000 topics)

The current Harp-DAAL system provides 13 distributed data analytics and machine learning algorithms leveraging the local computation kernels like K-means from the Intel DAAL 2018 release. In addition, Harp-DAAL is developing its own data-intensive kernels. This includes the large-scale subgraph counting algorithm given above, which can process a social network Twitter graph with billions of edges and subtemplates of 10 vertices in 15 minutes. The Harp-DAAL framework and machine learning algorithms are publicly accessible5 so you can download the software, explore the tutorials, and apply Harp-DAAL to other data-intensive applications.
_________________________
https://techdecoded.intel.io/resources/understanding-the-instruction-pipeline/?cid=em-elq-35946&utm_source=elq&utm_medium=email&utm_campaign=35946&elq_cid=3023559
Understanding the Instruction Pipeline
The Key to Adaptability in Modern Application Programming

Alex Shinsel, Technical Consulting Engineer, Intel Corporation
Email
@IntelDevTools| No ratings yet. Rate this

Understanding the instruction pipeline, on at least a basic level, is as critical to achieving high efficiency in modern application programming as understanding color theory is to painting. It\u2019s a fundamental and ubiquitous concept. While sources vary on exact dates and definitions, instruction pipelining as we know it started gaining popularity at some point in the 1970s or 1980s and is omnipresent in modern machines.

Processing an instruction isn\u2019t instantaneous. There are several steps involved. While the exact details of implementation vary from machine to machine, conceptually it boils down to five main steps:

    Fetching an instruction
    Decoding it
    Executing it
    Accesing memory
    Writing back the results

Without pipelining, each instruction is processed from start to finish before moving on to the next. If we assume that each of the five steps takes one cycle, then it would take 15 cycles to process three instructions (Figure 1).

Figure 1 \u2013 Sequential instruction processing at one step per clock cycle

Because each step is handled by a different section of hardware, modern processors improve efficiency by pipelining the instructions, allowing the various hardware sections to each process a different instruction simultaneously. For instance, in cycle 3 of Figure 2, the processor is fetching instruction C, decoding instruction B, and executing instruction A. All three instructions are completed by the end of cycle seven\u2015eight cycles sooner than if they\u2019d been processed sequentially.

We can compare this to washing a second load of laundry while your first load is in the dryer. While processing an instruction certainly involves more steps than doing laundry, we can still divide it into two sections:

    The Front End, the part of the CPU that fetches and decodes instructions
    The Back End, the part that executes and retires instructions

Figure 2 \u2013 Pipelined instruction processing

Of course, Figure 2 is an oversimplification of instruction pipelining. In reality, the number of steps in the pipeline varies among implementations, with each of the steps used in the example often being split into multiple substeps. However, this doesn\u2019t affect conceptual understanding, so we\u2019ll continue to use the simplified five-step model. Also, this simplified model doesn\u2019t take into account superscalar design, which results in multiple pipelines per processor core because it duplicates functional units such as arithmetic logic units (ALUs) and fetches multiple instructions at once to keep the extra units busy.

The number of pipelines available is called the width. Figure 3 represents a two-wide design that fetches instructions A and B on the first cycle, and instructions C and D on the second cycle. The width is (theoretically) defined in terms of how many instructions can be issued each cycle, but this is somewhat complicated by the way pipelining is done with CISC designs such as the ever-popular x86.

Pipelining works best with RISC designs, those with a small number of simpler instructions that run quickly. The varying complexity and running times for more elaborate instructions like those found in x86 can make
pipelining difficult for multiple reasons:

Figure 3 \u2013 A two-wide instruction pipeline

    Slow instructions can bog down the pipeline.
    Complicated instructions may be more likely to stall on data dependencies.

The solution to this problem was to break down these complex operations into smaller micro-operations, or \u03bcops. For convenience, the \u03bc is often replaced with u\u2015thus, the uop. The x86 instructions are therefore fetched and decoded, converted into uops, and then dispatched from a buffer to be executed and, ultimately, retired. This disconnect between x86 instructions being fetched and uops being dispatched makes it hard to define the width of a processor using this methodology, and this difficulty is exacerbated by the fact that pairs of uops can sometimes be fused together.

The difficulty of precisely defining the processor width in this scenario makes an abstraction appealing. Regardless of semantics or underlying hardware, there\u2019s ultimately a fixed number of uops that can be issued from the Front End per cycle, and a fixed number of uops that can be retired from the Back End per cycle. This is the number of pipeline slots available and, as a general rule of thumb, the magic number is usually four on modern Intel® processors.

The concept of the pipeline slot is useful for application optimization because each slot can be classified into one of four categories on any given cycle based on what happens to the uop it contains (Figure 4). Each pipeline slot category is expected to fall within a particular percentage range for a well-tuned application of a given type (e.g., client, desktop, server, database, scientific). A tool like Intel® VTune\u2122 Amplifier can help to measure the percentage of pipeline slots in an application that fall into each category, which can be compared to the expected ranges. If a category other than Retiring exceeds the expected range for the appropriate application type, it indicates the presence and nature of a performance bottleneck.

Figure 4 \u2013 Pipeline slot categorization flowchart

Much has already been written on the technique of using these measurements for performance optimization, including the Intel VTune Amplifier tuning guides, and these methods are outside the scope of this article, so we won\u2019t cover them here. (See the suggested readings at the end of this article for additional tuning advice.) Instead, we\u2019ll focus on understanding what\u2019s going on within the pipeline in these situations. For the sake of simplicity, our diagrams will have only a single pipeline.

We\u2019ve already discussed the Retiring category. It represents normal functionality of the pipeline, with no stalls or interruptions. The Back-End-Bound and Front-End-Bound categories, on the other hand, both represent situations where instructions weren\u2019t able to cross from the Front End to the Back End due to a stall. The stalls that cause Back-End-Bound and Front-End-Bound slots can have many root causes, including everything from cache misses to overly high demand for a particular type of execution unit. But the effects ultimately boil down to uops not leaving their current stages on schedule.

A Front-End-Bound slot occurs when an instruction fails to move from the Front End into the Back End despite the Back End being able to accommodate it. In Figure 5, instruction B takes an extra cycle to finish decoding, and remains in that stage on cycle 4 instead of passing into the Back End. This creates an empty space that propagates down the pipeline\u2015known as a pipeline bubble\u2015marked here with an exclamation point.

Figure 5 \u2013 Example of a Front-End-Bound slot

A Back-End-Bound slot occurs when the Back End cannot take incoming uops (regardless of whether the Front End is capable of actually supplying them) (Figure 6). In this example, instruction B takes an extra cycle to execute and, because it is still occupying the Execute stage on cycle 5, instruction C can\u2019t move into the Back End. This also results in a pipeline bubble.

Figure 6 \u2013 Example of a Back-End-Bound slot

Note that the delay doesn\u2019t have to occur in the Decode or Execute stages. In Figure 5, if B had taken an extra cycle to fetch, no instruction would have passed into the Decode stage on cycle 3, creating a bubble, so there would be no instruction to pass into the Back End on cycle 4. Likewise, in Figure 6, if instruction A had taken an extra cycle in the Memory stage, then B would have been incapable of moving out of the Execute stage on cycle 5, whether it was ready to or not. Therefore, it would remain where it was, blocking C from proceeding into the Back End.

The final category is Bad Speculation. This occurs whenever partially processed uops are cancelled before completion. The most common reason uops are cancelled is due to branch misprediction, though there are other causes (e.g., self-modifying code). A branch instruction must be processed to a certain point before it\u2019s known whether the branch will be taken or not. Once again, the implementation details vary, but are conceptually similar. For the sake of demonstration, we\u2019ll assume that we\u2019ll know whether to take path X or path Y when the branch instruction reaches the end of the Execute stage (Figure 7). Branches are so common that it\u2019s infeasible to incur a performance penalty every time one is encountered by waiting until it finishes executing to start loading the next instruction. Instead, elaborate algorithms predict which path the branch will take.

Figure 7 \u2013 Instructions from path X being loaded into the pipeline before the branch is resolved

Figure 8 \u2013 Correct branch prediction

From here, there are two possible outcomes:

    The branch prediction is correct (Figure 8) and things proceed as normal.
    The branch is mispredicted (Figure 9), so the incorrect instructions are discarded, leaving bubbles in their place, and the correct instructions begin entering the pipeline.

The performance penalty is effectively the same as if the pipeline had simply waited for the execution of the branch to resolve before beginning to load instructions, but only occurs when the prediction algorithm is wrong rather than every time a branch is encountered. Because of this, there\u2019s a constant effort to improve prediction algorithms.

Figure 9 \u2013 Branch misprediction

Anyone with a performance analyzer can access Bad Speculation, Front-End-Bound, and Back-End-Bound slot counts for an application. But without understanding where those numbers come from or what they mean, they\u2019re useful for little more than blindly following instructions from a guide, utterly dependent on the author\u2019s recommendations. Understanding is the key to adaptability and, in the fluid world of software, it\u2019s crucial to be able to respond to the unique needs of your own application\u2015because some day, you\u2019ll encounter a scenario that hasn\u2019t been written about.
Learn More

    Patterson, Jason R. C. \u201cModern Microprocessors: A 90-Minute Guide!\u201d Lighterra, May 2015. Sections 2-7, 12.
    Walton, Jarred. \u201cProcessor Architecture 101 \u2013 the Heart of your PC.\u201d PCGamer. Future plc, 28 December 2016.
    \u201cTuning Guides and Performance Analysis Papers.\u201d Intel Developer Zone. Intel.
    \u201cTuning Applications Using a Top-Down Microarchitecture Analysis Method.\u201d Intel Developer Zone. Intel.

_____________________________
https://techdecoded.intel.io/resources/parallel-cfd-with-the-hifun-solver-on-the-intel-xeon-scalable-processor/?cid=em-elq-35946&utm_source=elq&utm_medium=email&utm_campaign=35946&elq_cid=3023559 

  Parallel CFD with the HiFUN* Solver on the Intel® Xeon® Scalable Processor
Maximizing HPC Platforms for Fast Numerical Simulations

Rama Kishan Malladi, Technical Marketing Engineer; S.V. Vinutha, Technical Marketing Engineer; and Austin Cherian, Account Executive; Research & Analysis Team, S&I Engineering Solutions Pvt. Ltd., Bangalore, India
Email
@IntelDevTools| No ratings yet. Rate this

Computational fluid dynamics (CFD) is a branch of science that deals with numerical solutions for equations governing fluid flow\u2015with the help of high-speed computers. And, because they\u2019re based on a mountain of data, it\u2019s essential for CFD solutions to get every last bit of performance out of today\u2019s high-performance computing (HPC) hardware platforms.

CFD uses the Navier-Stokes equations\u2015non-linear, partial differential equations describing mass, momentum, and energy conservation for a fluid flow. In CFD, discretization is a technique to convert the Navier-Stokes equations into a set of algebraic equations. Due to the geometric complexity and complicated flow physics associated with an industrial application, the typical size of the algebraic system varies from a few million to over a billion equations. That means realistic numerical simulations need to be carried out on large-scale HPC platforms to obtain design data in a timeframe short enough to impact the design cycle.

In this article, we\u2019ll explore the HiFUN* solver, proprietary software from S&I Engineering Solutions (SandI) Pvt. Ltd., as an example of a CFD application that can take full advantage of the architecture of massively parallel supercomputing platforms.1,2
______________
https://techdecoded.intel.io/resources/improving-vasp-materials-simulation-performance/?cid=em-elq-35946&utm_source=elq&utm_medium=email&utm_campaign=35946&elq_cid=3023559
mproving VASP* Materials Simulation Performance
Using the Latest Intel® Software Development Tools to Make More Efficient Use of Hardware

Fedor Vasilev, CRT Engineer; Dmitry Sivkov, PhD, DTC Engineer; and Jeongnim Kim, PhD, Senior HPC Application Engineer; Intel Corporation
Email
@IntelDevTools| No ratings yet. Rate this

Materials modeling and design is a very important area of science and engineering. Synthetic materials are a part of modern life. Materials for our computers, cars, planes, roads, and even houses and food are the result of advances in materials science. High-performance computing (HPC) makes modern materials simulation and design possible. And Intel® Software Development Tools can help both developers and end users achieve maximum performance in their simulations.

VASP* is one of the top applications for modeling quantum materials from first principles. It\u2019s an HPC application designed with computing performance in mind. It uses OpenMP* to take advantage of all cores within a system, and MPI* to distribute computations across large HPC clusters.

Let\u2019s see how Intel Software Development Tools can help with performance profiling and tuning of a VASP workload.
____________________
https://www.kotlindevelopment.com/data-pipelines-kotlin-akka-kafka/
  Building data pipelines with Kotlin using Kafka and Akka
Chances are that you have a bunch of services that expose some APIs over HTTP, also that you use message queues, relational databases, etc. Let's say you want to build an analytics component that polls one of your services, writes data into a message queue for later analysis, then, after post-processing, updates statistics in a SQL database.
Introducing Kotlin GitHub monitor

For demonstration purposes, I've built an app that polls GitHub's events API, and monitors the whole Kotlin organization so I can keep track of what's going on with the development of the language.

The app has the following functionality:

    polls GitHub and watches for new events;
    writes all events into a Kafka topic for later use;
    reads events from Kafka and filters out PushEvents;
    updates a Postgres database with: who pushed changes, when, into which repository.

I intentionally tried to keep the code very simple, omitting a few things that you probably would have implemented in case of production use, like recovery from errors, retires, metrics, etc.
Stack

As a foundation, I used the Akka toolkit, more specifically Akka Streams and a companion project called Alpakka. The former is general purpose stream processing library (implementing the Reactive Streams specification), and the latter is a collection of connectors and tools to build data integration between various technologies (e.g. AWS S3, MongoDB, or Google Cloud Pub/Sub).

Akka originates from the Scala ecosystem (also, most of its source code is written in Scala), but has a full-fledged Java API, making it possible to use it from Kotlin seamlessly. Usually, a lot of things can be expressed much more simply when you use the Scala API vs the Java API, but here's the catch: Kotlin has very nice language features and some syntactic sugar, making the two APIs almost identical, in terms of ergonomy.

This post is not intended to be a "getting started guide for Akka Streams", so I won't explain how Akka Streams works in detail, but you don't have to fully understand what happens under the hood to be able to follow the code. There are basically two things that you have to remember: data is moving from Sources to Sinks. If you are familiar with RxJava, Source roughly translates to Observable and Sink to a specialized Observer.
Let's write code!

The entry point of our application is a regular Java main function:

fun main(vararg args: String) {
  val system = ActorSystem.create()
  val materializer = ActorMaterializer.create(system)

  val gitHubClient = GitHubClient(system, materializer)
  val eventsProducer = EventsProducer(system, materializer)
  val eventsConsumer = EventsConsumer(system)
  val pushEventProcessor = PushEventProcessor(materializer)

  eventsProducer.write(gitHubClient.events())
  pushEventProcessor.run(eventsConsumer.read())
}
 
 
 
 
 
 
 
 

Let's break it down:

    2-3 are basically boilerplate for using Akka and Akka Streams (pretty much everything you do in Akka requires an ActorSystem, and ActorMaterializer is something required to run your streams, more on this a bit later);
    5 instance used to poll the GitHub events API;
    6 instance used to write events into Kafka;
    7 instance used to read events from Kafka;
    8 instance used to filter PushEvents and update the database;
    10-11 put things in motion.

Now that we have a high-level overview of the moving pieces, let's look inside them one by one.
GitHubClient

While communicating with the GitHub Events API, there are a few things we have to watch out for. The API is optimized for polling, but GitHub requires its users to follow a couple of simple rules:

    do not fetch data unnecessarily (it uses the HTTP ETag header mechanism and returns 304 Not Modified in case nothing has changed)
    do not poll the API too frequently (it uses a special header X-Poll-Interval to signal the allowed rate for the user)

This means that after an initial request, we have to keep track of a previous request-response before executing a new one and use the appropriate ETag header and keep the poll interval. Akka Streams has an operator for such recursive use-cases, called Source.unfoldAsync.

fun poll(): Source<GitHubResponse, NotUsed> =
  Source.unfoldAsync(GitHubRequest(), { request ->
    executeWithDelay(request).thenApply { response ->
      val nextRequest = GitHubRequest(eTagOpt = response.eTagOpt, delayOpt = response.pollIntervalOpt)
      Optional.of(Pair.create(nextRequest, response))
    }
  })
 
 
 
 

    2 creates the initial GitHubRequest (with no ETag header and poll interval);
    3 executes the request;
    4 creates a subsequent request based on the response headers returned for the initial request;
    5 returns a non-empty Option with the current response and the next request supposed to be executing, signaling to unfoldAsync that we haven't finished (if we would have returned Optional.empty() the Source would have stopped).

And the code for executing each request:

fun executeWithDelay(request: GitHubRequest): CompletionStage<GitHubResponse> {
  fun execute(request: GitHubRequest): CompletionStage<GitHubResponse> {
    val httpRequest = mapToHttpRequest(request)
    logger.debug("executing request: $httpRequest")
    return client.singleRequest(httpRequest).thenCompose { response -> mapToGitHubResponse(response) }
  }
  return Source
    .single(request)
    .delay(FiniteDuration(request.delayOpt.orElse(lastPollInterval), TimeUnit.SECONDS), DelayOverflowStrategy.backpressure())
    .mapAsync(1, ::execute)
    .runWith(Sink.head(), materializer)
}
 
 
 
 
 
 
 

    2-6 helper function for logging and executing a request;
    9 introduces a delay before moving forward to the helper execute method;
    11 here we have an interesting concept, specific for Akka Streams
        when you define Sinks and Sources, what you actually do is building blueprints;
        these blueprints are not really doing anything interesting on their own, just define what should happen with your data eventually;
        in order to actually set these blueprints in motion, you have to "materialize" them, in this case with the runWith method;
        Sink.head() means that we take the first item as the materialized value of the stream;
        because streams are asynchronous, Akka exposes the materialized value as a Java 8 CompletionStage.

Each time we receive a response from GitHub, we parse it and send individual events downstream.

fun events(): Source<JsonNode, NotUsed> =
  poll().flatMapConcat { response ->
    response.nodesOpt
      .map { nodes -> Source.from(nodes) }
      .orElse(Source.empty())
  }
 

    3 the response may or may not contain an array of events (e.g. in case of an HTTP 304 there are no events), if it does, we create a new Source from them.

The rest of the code is just boilerplate for processing the request/response and extracting header values. Now that we have the infrastructure for communicating with GitHub, let's move on to writing/reading events to Kafka.
EventsProducer and EventsConsumer

Here comes the power of Akka Streams and Alpakka. There is a module called Akka Streams Kafka, which greatly reduces the amount of code that we have to write for integrating with Kafka. Publishing events into a Kafka topic look like the following.

fun write(events: Source<JsonNode, NotUsed>): CompletionStage<Done> =
  events
    .map { node -> ProducerRecord<ByteArray, String>("kotlin-events", objectMapper.writeValueAsString(node)) }
    .runWith(Producer.plainSink(settings), materializer)
 
 

    3 maps a GitHub event into a Kafka ProducerRecord (serializing JsonNode as a String);
    4 connects the Source to a special purpose Sink defined in Akka Streams Kafka, called Producer.plainSink, which is taking care of communicating with Kafka.

The other way around, reading from Kafka, is also super simple.

fun read(): Source<JsonNode, NotUsed> =
  Consumer.plainSource(settings, Subscriptions.assignmentWithOffset(TopicPartition("kotlin-events", 0), 0L))
    .map { record -> objectMapper.readTree(record.value()) }
    .mapMaterializedValue { NotUsed.getInstance() }
 
 

    2 Consumer.plainSource is a custom Source also defined in Akka Streams Kafka, which takes care of connecting to a topic and reading records from Kafka;
    3 maps a record back to a JsonNode, so basically we can work with them as if they would have come from our GitHubClient.

At this point, we have a copy of GitHub's events feed for github.com/Kotlin stored in Kafka, so we can time travel and run different analytics jobs on our local dataset.
PushEventProcessor

According to our specification, we want to filter out PushEvents from the stream and update a Postgres database with the results. Alpakka has a package for interacting with SQL databases called Slick (JDBC) Connector. There is one downside of using the Slick Connector with the Java API: we have to write all the SQL code with String interpolation (Slick has a powerful Scala API that can be used to write type-safe queries and other things). Using Kotlin's raw string support can help though.

First, we need to be able to create a new table (if it doesn't exist).

fun createTableIfNotExists(): Source<Int, NotUsed> {
  val ddl =
    """
      |CREATE TABLE IF NOT EXISTS kotlin_push_events(
      |  id         BIGINT    NOT NULL,
      |  name       VARCHAR   NOT NULL,
      |  timestamp  TIMESTAMP NOT NULL,
      |  repository VARCHAR   NOT NULL,
      |  branch     VARCHAR   NOT NULL,
      |  commits    INTEGER   NOT NULL
      |);
      |CREATE UNIQUE INDEX IF NOT EXISTS id_index ON kotlin_push_events (id);
    """.trimMargin()
  return Slick.source(session, ddl, { _ -> 0 })
}
 

    14 Slick.source can be used to execute SQL code via JDBC and process its results:
        the third function that we pass to Slick.source has the signature of (SlickRow) -> T, where T will be the generic type of the source returned: Source<T, NotUsed>;
        in case of e.g. a SELECT statement, we could use this function to map rows in the result set to a custom data class.

Similarly, the function to update the database looks like this.

fun Source<PushEvent, NotUsed>.updateDatabase(): CompletionStage<Done> =
  createTableIfNotExists().flatMapConcat { this }
    .runWith(Slick.sink<PushEvent>(session, 20, { event ->
      """
        |INSERT INTO kotlin_push_events(id, name, timestamp, repository, branch, commits)
        |VALUES (
        |  ${event.id},
        |  '${event.actor.login}',
        |  '${Timestamp.valueOf(event.created_at)}',
        |  '${event.repo.name}',
        |  '${event.payload.ref}',
        |  ${event.payload.distinct_size}
        |)
        |ON CONFLICT DO NOTHING
      """.trimMargin()
    }), materializer)
 
 
 

    1 please note, that I defined updateDatabase as an extension method on the type Source<PushEvent, NotUsed> (you'll see why);
    2 creates the table then switches back to the original Source;
    3 Slick.sink takes each item flowing through the stream and executes the provided SQL statement, in this case, an INSERT.

We are almost done, what's left is filtering and mapping from JsonNode to PushEvent and composing the methods together.

fun Source<JsonNode, NotUsed>.filterPushEvents(): Source<PushEvent, NotUsed> =
  filter { node -> node["type"].asText() == "PushEvent" }
    .map { node -> objectMapper.convertValue(node, PushEvent::class.java) }
 

    1 filterPushEvents is also defined as an extension method.

And finally, all the functions composed together look like this.

fun run(events: Source<JsonNode, NotUsed>): CompletionStage<Done> =
  events
    .filterPushEvents()
    .updateDatabase()

This is why we've used the extension methods above, so we can describe transformations like this, simply chained together. That's it, after running the app for a while (gradle app:run) we can see the activities around different Kotlin repositories. You can find the complete source code on GitHub.

A very nice property of using Akka Streams and Alpakka is that it makes really easy to migrate/reuse your code, e.g. in case you want to store data in Cassandra later on instead of Postgres. All you would have to do is define a different Sink with CassandraSink.create. Or if GitHub events would be dumped in a file located in AWS S3 instead of published to Kafka, all you would have to do is create a Source with S3Client.download(bucket, key). The current list of available connectors is located here, and the list is growing.
Summary

Akka is a very powerful technology, that helps you write complex (and correct) code more easily. Using Kotlin makes the code much more readable and I think there is huge potential here for either the community or Lightbend (the company behind Akka) to create even more idiomatic APIs and wrappers to use with Kotlin. This is one of the benefits of the JVM platform, enabling code to happily co-exist in a single runtime, witten in different languages, e.g. Java, Scala and Kotlin. I really think that 2018 will be an even more exciting year for the Kotlin community and that its adoption will grow further not just on mobile, but for other use-cases, like server side development, data science, machine learning, etc. If you've enjoyed this post, keep an eye out on our blog where we will cover more of these topics.
________________________________

http://www.time-travellers.org/shane/papers/NFS_considered_harmful.html
...  this can cause reliability problems (see http://www.time-travellers.org/shane/papers/NFS_considered_harmful.html). Specifically, delayed (asynchronous) writes to the NFS server can cause data corruption problems. If possible, mount the NFS file system synchronously (without caching) to avoid this hazard. Also, soft-mounting the NFS file system is not recommended.
_______________
https://media.consensys.net/blockchain-vs-distributed-ledger-technologies-1e0289a87b16
... Further parallels can be shown in protocols such as Tendermint, a BPFT consensus engine being designed with 
similar functionalities as tools like Apache Zookeeper. 

Internally there has also been research along the lines of event sourcing databases which can replicate several
functionalities desired from a coordinated data sharing system.
______________
Hive,  Zookeeper.
___________________
https://dzone.com/articles/top-4-innovations-in-containers-and-cloud-computin 
_______________________
https://www.infoq.com/news/2018/05/medium-reactjs-graphql-migration

____________________________
Streaming Architectures:
   https://www.infoq.com/streaming?utm_source=infoqEmail&utm_medium=editorial&utm_campaign=SpecialNL&utm_content=05182018
- Streaming Reactive Systems & Data Pites w. Squbs
  https://www.infoq.com/presentations/squbs?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming
- Streaming SQL Foundations: Why I Love Streams+Tables 
  https://www.infoq.com/presentations/sql-streaming?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming

- Next Steps in Stateful Streaming with Apache Flink:
  https://www.infoq.com/presentations/flink-stateful-streaming?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming

- Kafka Streams - from the Ground Up to the Cloud 
  https://www.infoq.com/presentations/kafka-streams-spring-cloud?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming

-  Data Decisions with Real-Time Stream Processing
https://www.infoq.com/presentations/facebook-stream-processing?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming

- Foundations of streamng SQL:
https://www.infoq.com/presentations/beam-model-stream-table=theory?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming


- The Power of Distributed Snapshots in Apache Flink:
  https://www.infoq.com/presentations/distributed-stream-processing-flink?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming

- Panel: SQL over Streams, Ask the Experts:
  https://www.infoq.com/presentations/sql-streams-panel?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming

- Survival of the Fittest - Streaming Architectures:
  https://www.infoq.com/presentations/hbc-digital-streaming?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming 

- Streaming for Personalization Datasets at Netflix:
 https://www.infoq.com/presentations/netflix-personalization-datasets-streaming?utm_source=presentations_about_Streaming&utm_medium=link&utm_campaign=Streaming
_________________________________
http://ceph.com/ceph-storage/

Ceph’s RADOS provides you with extraordinary data storage  scalability—thousands of client hosts or KVMs accessing petabytes to 
exabytes of data. Each one of your applications can use the object, block or file system interfaces to the same RADOS cluster 
simultaneously, which means your Ceph storage system serves as a  flexible foundation for all of your data storage needs. You can use Ceph
 for free, and deploy it on economical commodity hardware. Ceph is a  better way to store data.

By decoupling the namespace from the underlying hardware, object-based storage systems enable you to build much larger storage clusters. You 
can scale out object-based storage systems using economical commodity hardware, and you can replace hardware easily when it malfunctions or 
fails.

Ceph’s CRUSH algorithm liberates storage clusters from the scalability and performance limitations imposed by centralized data table mapping. It 
replicates and re-balance data within the cluster dynamically—elminating this tedious task for administrators, while delivering
 high-performance and infinite scalability.

See more at: http://ceph.com/ceph-storage/#sthash.KNp2tGf5.dpuf
_________________________________

Ref http://xmodulo.com/2014/09/create-cloud-based-encrypted-file-system-linux.html

S3QL is one of the most popular open-source cloud-based file systems.  It is a FUSE-based file
system backed by several commercial or open-source cloud storages, such as Amazon S2, 
Google Cloud Storage, Rackspace CloudFiles, or OpenStack.  As a full featured file system, S3QL boasts 
of a number of powerful capabilities, such as unlimited capacity, up to 2TB file sizes, 
compression, UNIX attributes, encryption, snapshots with copy-on-write, immutable trees,
de-duplication, hardlink/symlink support, etc.  Any bytes written to an S3QL file system are 
compressed/encrypted locally before being transmitted to cloud backend. 
When you attempt to read contents stored in an S3QL file system, the 
corresponding objects are downloaded from cloud (if not in the local 
cache), and decrypted/uncompressed on the fly.
_________________________________

http://tachyon-project.org/index.html
Tachyon is a memory-centric distributed file system enabling reliable file sharing at memory-speed
across cluster frameworks, such as Spark and MapReduce. It achieves high performance by leveraging
lineage information and using memory aggressively. Tachyon caches working set files in memory,
thereby avoiding going to disk to load datasets that are frequently read. This enables different
jobs/queries and frameworks to access cached files at memory speed.

Tachyon is Hadoop compatible. Existing Spark and MapReduce programs can run on top of it without
any code change. The project is open source (Apache License 2.0) and is deployed at multiple companies.
It has more than 40 contributors from over 15 institutions, including Yahoo, Intel, and Redhat.

The project is the storage layer of the Berkeley Data Analytics Stack (BDAS) and also part of the
Fedora distribution.
_____________________
Kafka vs ...: (http://kafka.apache.org/documentation/#introduction)
  In comparison to log-centric systems like Scribe or Flume, Kafka offers equally
  good performance, stronger durability guarantees due to replication, and much lower end-to-end latency. 

  Kafka works well as a replacement for a more traditional message broker. 
  Message brokers are used for a variety of reasons (to decouple processing from data
  producers, to buffer unprocessed messages, etc). In comparison to most messaging 
  systems Kafka has better throughput, built-in partitioning, replication, and
  fault-tolerance which makes it a good solution for large scale message processing applications.
  In our experience messaging uses are often comparatively low-throughput, but may require
  low end-to-end latency and often depend on the strong durability guarantees Kafka provides.
  In this domain Kafka is comparable to traditional messaging systems such as ActiveMQ or RabbitMQ. 

  Kafka can serve as a kind of external commit-log for a distributed system. The log helps
  replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore
  their data. The log compaction feature in Kafka helps support this usage. In this usage Kafka
  is similar to Apache BookKeeper project. 
______________________
Messaging traditionally has two models: queuing and publish-subscribe. In a queue,
a pool of consumers may read from a server and each record goes to one of them;
in publish-subscribe the record is broadcast to all consumers. Each of these two
models has a strength and a weakness. The strength of queuing is that it allows 
you to divide up the processing of data over multiple consumer instances, which
lets you scale your processing. Unfortunately, queues aren't multi-subscriber—once
one process reads the data it's gone. Publish-subscribe allows you broadcast data
to multiple processes, but has no way of scaling processing since every message
goes to every subscriber
______________________
Event sourcing is a style of application design where state changes are logged 
as a time-ordered sequence of records. Kafka's support for very large stored log
data makes it an excellent backend for an application built in this style. 
https://martinfowler.com/eaaDev/EventSourcing.html
___________________
GDPR Auditing tools:
  - IMPERVA:
  - IBM Guardium:
  - FORTINET:
  - hexatier:
  - Oracle Audit Vault:
___________________
 Correo muy intesante "quejandose" de las diferentes soluciones de sistemas de fichero Cluster   http://zgp.org/linux-tists/20040101205016.E5998@shaitan.lightconsulting.com.html"

___________________
rch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster.

What is Kibana? – Amazon Web Services
aws.amazon.com/elasticsearch-service/kibana

Learn how Kibana is used as an open-source data visualization and exploration tool for log and time series analytics, application monitoring, and operational ...
__________________________________
REF: https://dzone.com/articles/consensus-clustering-via-apache-spark
    In this article, we will discuss a technique called Consensus Clustering to assess the stability
    of clusters generated by a clustering algorithm with respect to small perturbations in the data
    set. We will review a sample application built using the Apache Spark machine learning library 
    to show how consensus clustering can be used with K-means, Bisecting K-means, and Gaussian 
    Mixture, three distinct clustering algorithms
_____________________________
https://www.rosehosting.com/blog/how-to-install-the-elk-stack-on-debian-9/
In this tutorial we will show you how to install and configure the ELK Stack on Debian 9. ELK stack
 is a collection of three open-source products, Elasticsearch, Logstash and Kibana and is a robust 
solution for searching, analyzing and visualizing data. Elasticsearch is a distributed, RESTful 
search and analytics NoSQL engine based on Lucene, Logstash is a light-weight data processing pipeline
for managing events and logs from a wide variety of sources and Kibana is a web application for 
visualizing data that works on top of Elasticsearch. This ELK Stack tutorial should work on other 
Linux VPS systems as well but was tested and written for an Debian 9 VPS. Installing ELK Stack on 
Debian 9 is an easy and straightforward task, just follow the steps bellow, and you should have it 
installed in less then 10 minutes
________________________________
https://en.wikipedia.org/wiki/Stream_processing
_________________________________
Modern real-time streaming architectures
(https://www.safaribooksonline.com/live-training/courses/modern-real-time-streaming-architectures/0636920182825/)
  - Arun Kejariwal (Open Soft developed at Twitter)
  - Karthik Ramasamy

What you must understand:
  - The various facets of a stream processing pipeline
  - The types of analysis that can be carried out on data streams
  - Commonly used algorithms (data sketches) for analyzing your data streams

To be able to:
  - Determine and use the stream processing framework best suited to your needs
  - Carry out analysis on data streams.
  - mine inbound data streams to guide decision making.
    (fraud detection and assessing/optimization marketing campaigns,
     bot detection, intrusion detection, and real-time monitoring, ..)
  - build a stream processing pipeline for inbound data streams.

___________________________________________
An Introduction to Time Series with Team Apache 
https://www.safaribooksonline.com/library/view/an-introduction-to/9781491934951/
Apache Cassandra evangelist Patrick McFadin shows how to solve time-series data
problems with technologies from Team Apache: Kafka, Spark and Cassandra.

  - Kafka: handle real-time data feeds with this "message broker"
  - Spark: parallel processing framework that can quickly and efficiently 
           analyze massive amounts of data
  - Spark 
    Streaming: perform effective stream analysis by ingesting data in micro-batches
  - Cassandra: distributed database where scaling and uptime are critical
  - Cassandra Query Language (CQL): navigate create/update your data and data-models
  - Spark+Cassandra: perform expressive analytics over large volumes of data

___________________________________________
Sharding: Powerful but manual scaling
__________________________________________
In 1973, Carl Hewitt had an idea inspired by quantum mechanics. He wanted to develop computing machines that were capable of parallel execution of tasks, communicating with each other seamlessly while containing their own local memory and processors.
The actor model in computer science is a mathematical model of concurrent computation that treats "actors" as the universal primitives of concurrent computation. In response to a message that it receives, an actor can: make local decisions, create more actors, send more messages, and determine how to respond to the next message received. Actors may modify their own private state, but can only affect each other through messages (avoiding the need for any locks).
The actor model originated in 1973.[1] It has been used both as a framework for a theoretical understanding of computation and as the theoretical basis for several practical implementations of concurrent systems. The relationship of the model to other work is discussed in Actor model and process calculi.
_____________________________
Safran Data Architect H/F:

- Mise en place de solution : Hadoop, HDFS, Yarn, solutions in-memory.
- Echanges et traitement des données : EDI, API, ETL, MapReduce, Hive, Pig, Spark, Storm, NoSQL type MongoDB, SQL.
- Algorithmique : évaluation de la complexité, structure de données, parcours de graphe, calculs parallèles.
- Outils de visualisation des données.
- Technologies EDI (ETL, protocoles d'échanges standards)
_____________________________
Tools BigData:

                   | LOCAL                    | GOOGLE                  | AWS               | AZURE
---------------------------------------------------------------------------------------------------
Data Cleaning      | Trifacta Wrangler        | DataPrep                | Glue              | MLStudio
---------------------------------------------------------------------------------------------------
SandBox/Notebook   | Jupiter,...              | DataLab                 | JupiterHub        | MLSutdio
---------------------------------------------------------------------------------------------------
Hadoop grid        | Cloudera/HortonWorks,... | DataProc                | EMR               | HDinsights
---------------------------------------------------------------------------------------------------
Ingest             | Kafka                    | Pub/Sub                 | Kinesys           | Event Hub
---------------------------------------------------------------------------------------------------
Stream Processing  | Apache Flume/ETL         | DataFlow                | Data Pipeline     | Data Factory
---------------------------------------------------------------------------------------------------
Neural Networks    | TensorFlow               | Mach.Learning Eng+APIs  | Tensorflow on AWS | MLStudio
---------------------------------------------------------------------------------------------------
DwH                | Teradata Sybase IQ       | BigQuery                | Redshift          | AzureDB DWH
---------------------------------------------------------------------------------------------------
Others             | MongoDB, Cassandra,...   | BigQuery, Bigtable      | Redshift          | CosmosDB
---------------------------------------------------------------------------------------------------
______________________
Handling secrets:
Three 'well known' solutions
are Square's <a href="https://square.github.io/keywhiz/">Keywhiz</a>,
Hashicorp's <a href="https://www.vaultproject.io/">Vault</a> and
<a href="https://xordataexchange.github.io/crypt/">crypt</a> in combination
with etcd or consul.</p>
-->



</html>
