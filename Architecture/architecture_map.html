<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>architecture map (alpha) (ignore)</title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body onLoad='onPageLoaded()'>
<b id='initialMessage' orange>Hint double-click on elements to zoom!!</b>
<div id='zoomDiv'></div>
<div style="position:fixed; right:0.3%; bottom:0; width:auto;">
<b style="font-size:1.5rem" orange><a onclick="onZoomOut()">[-A]</a></b>
<b style="font-size:1.5rem"       >                                 </b>
<b style="font-size:2.0rem" orange><a onclick="onZoomIn ()">[A+]</a></b>
</div>
<!-- {{{ START }}} -->
<table>
<tr>
  <td>
  <a href="http://www.ehcache.org/">Ehcache</a><br/>
  open source, standards-based cache that boosts performance, offloads I/O.<br/>
   Integrates with other popular libraries and frameworks.<br/>
  It scales from in-process caching, all the way to mixed in-process/out-of-process
  deployments with terabyte-sized caches
<pre xxxsmall zoom { >

Coding to Ehcache 3 API:
<pre xxxsmall { >
CacheManager cacheManager = CacheManagerBuilder.newCacheManagerBuilder()
    .withCache("preConfigured",
         CacheConfigurationBuilder.newCacheConfigurationBuilder(Long.class, String.class,
             ResourcePoolsBuilder.heap(100))
         .build())
    .build(true);

Cache<Long, String> preConfigured
    = cacheManager.getCache("preConfigured", Long.class, String.class);

Cache<Long, String> myCache = cacheManager.createCache("myCache",
    CacheConfigurationBuilder.newCacheConfigurationBuilder(Long.class, String.class,
                                  ResourcePoolsBuilder.heap(100)).build());

myCache.put(1L, "da one!");
String value = myCache.get(1L);

cacheManager.close();
</pre }>
(simpler/lighter solution but not so escalable could be to use Google Guava Cache)
  </td>

  <td>
  [log]<a href="https://www.fluentd.org/">FluentD</a><br/>
  (Improved "logstat")
<pre xxxsmall zoom { >
Open Source data collector for unified logging layer.

Fluentd allows you to unify data collection and consumption for
a better use and understanding of data.

Syslog                      Elasticsearch
Apache/Nginx logs    → → →  MongoDB
Mobile/Web app logs  → → →  Hadoop
Sensors/IoT                 AWS, GCP, ...
  </td>
  <td>
  Kafka: log and events collection, streaming platform, ...
<pre xxxsmall zoom { >
Each broker in the Kafka cluster has an identity which can be used to find other
brokers in the cluster. The brokers also need some type of a database to 
store partition logs. It's important to configure a Persistent Volume (PV) for 
Kafka, otherwise you will lose the logs.
</pre } >
  </td>
  <td>
  <a href="https://prometheus.io/">Prometheus</a> , 
  <a href="https://github.com/prometheus/prometheus">Git</a>
<pre xxxsmall zoom { >
Power your metrics and alerting with a leading open-source monitoring solution.
- Prometheus fundamentally stores all data as time series: streams of 
  timestamped values belonging to the same metric and the same set of labeled 
  dimensions. Besides stored time series, Prometheus may generate temporary 
  derived time series as the result of queries.
- Prometheus provides a functional expression language that lets the user 
  select and aggregate time series data in real time. The result of an expression
  can either be shown as a graph, viewed as tabular data in Prometheus's 
  expression browser, or consumed by external systems via the HTTP API.
- Grafana supports querying Prometheus. The Grafana data source for Prometheus 
  is included since Grafana 2.5.0 (2015-10-28).
- Prometheus includes a local on-disk time series database, but also optionally
  integrates with remote storage systems.
- Alerting rules allow you to define alert conditions based on Prometheus 
  expression language expressions and to send notifications about firing alerts
  to an external service.
</pre } >
  </td>
  <td>
  Apache Spark:</br>
<pre xxxsmall zoom { >
  general framework for large-scale data processing that supports lots of 
different programming languages and concepts such as MapReduce, in-memory
processing, stream processing, graph processing, and Machine Learning. This can
also be used on top of Hadoop. Data can be ingested from many sources like 
Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex 
algorithms expressed with high-level functions like map, reduce, join and window.

Common applications for Spark include real-time marketing campaigns, online 
product recommendations, cybersecurity analytics and machine log monitoring.
</pre } >
<br/>
Kafka vs Spark Streaming: <a href="https://dzone.com/articles/spark-streaming-vs-kafka-stream-1">REF</a>
<pre xxxsmall zoom { >
""" If event time is very relevant and latencies in the seconds range are
completely unacceptable, Kafka should be your first choice. Otherwise, 
Spark works just fine. 
...
Apache Spark can be used with Kafka to stream the data, but if you are 
deploying a Spark cluster for the sole purpose of this new application, that is
definitely a big complexity hit.
...
Conclusion
I believe that Kafka Streams is still best used in a "Kafka > Kafka" context, 
while Spark Streaming could be used for a "Kafka > Database" or 
"Kafka > Data science model" type of context.
</pre } >
  </td>
  <td>
  Hadoop:
  Hadoop "vs" Spark
  <a href="https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html">REF</a>
<pre xxxsmall zoom>
Hadoop is essentially a distributed data infrastructure: 
 -It distributes massive data collections across multiple nodes 
  within a cluster of commodity servers
 -It also indexes and keeps track of that data, enabling
  big-data processing and analytics far more effectively
  than was possible previously. 
Spark, on the other hand, is a data-processing tool that operates on those 
distributed data collections; it doesn't do distributed storage.

You can use one without the other: 
  - Hadoop includes not just a storage component, known as the 
  Hadoop Distributed File System, but also a processing component called 
  MapReduce, so you don't need Spark to get your processing done.
  - Conversely, you can also use Spark without Hadoop. Spark does not come with
  its own file management system, though, so it needs to be integrated with one
  -- if not HDFS, then another cloud-based data platform. Spark was designed for
  Hadoop, however, so many agree they're better together.

Spark is generally a lot faster than MapReduce because of the way it processes 
data. While MapReduce operates in steps, Spark operates on the whole data set 
in one fell swoop:
   "The MapReduce workflow looks like this: read data from the cluster, perform
    an operation, write results to the cluster, read updated data from the 
    cluster, perform next operation, write next results to the cluster, etc.," 
    explained Kirk Borne, principal data scientist at Booz Allen Hamilton. 
    Spark, on the other hand, completes the full data analytics operations 
    in-memory and in near real-time: 
    "Read data from the cluster, perform all of the requisite analytic 
    operations, write results to the cluster, done," Borne said.
Spark can be as much as 10 times faster than MapReduce for batch processing and 
p to 100 times faster for in-memory analytics, he said.
  You may not need Spark's speed. MapReduce's processing style can be just fine 
if your data operations and reporting requirements are mostly static and you 
can wait for batch-mode processing. But if you need to do analytics on 
streaming data, like from sensors on a factory floor, or have applications that
require multiple operations, you probably want to go with Spark.
 Most machine-learning algorithms, for example, require multiple operations. 

Recovery: different, but still good. 
Hadoop is naturally resilient to system faults or failures since data 
are written to disk after every operation, but Spark has similar built-in
resiliency by virtue of the fact that its data objects are stored in something 
called resilient distributed datasets distributed across the data cluster. 
"These data objects can be stored in memory or on disks, and RDD provides full 
recovery from faults or failures," Borne pointed out.
</pre>
  </td>
  <td>
  <a href="https://grafana.com/">Grafana</a><br/>
  time series analytics
  </td>
  <td>
  Graphite monitoring tool 
<pre xxxsmall zoom>
runs equally well on cheap hardware or Cloud infrastructure. Teams use Graphite
to track the performance of their websites, applications, business services, 
and networked servers. It marked the start of a new generation of monitoring 
tools, making it easier than ever to store, retrieve, share, and visualize 
time-series data.
</pre>
  </td>
</tr>
</table>
<br/>
<b>Network and SDN</b>
<table>
<tr>
  <td>
  <a href="https://www.onap.org/">Open Network Automation Platform</a>
  <a href="https://www.lightreading.com/nfv/containers/onap-cncf-come-together-on-containers/d/d-id/741790">ONAP, CNCF Come Together on Containers</a>
  </td>
  <td>
  <a href="https://www.cncf.io/">Cloud Native Computing Foundation</a><br/>
  CNCF serves as the vendor-neutral home for many of the fastest-growing projects on GitHub,
  including Kubernetes, Prometheus and Envoy, fostering collaboration between the 
  industry's top developers, end users, and vendors.
  </td>
  <td>
  <a href="https://www.envoyproxy.io/">Envoy</a><br/>
  Open Source edge and service proxy, designed for cloud-native applications 
  </td>
  <td>
<pre xxxsmall zoom { >
  As on the ground microservice practitioners quickly realize, the majority of 
  operational problems that arise when moving to a distributed architecture are
  ultimately grounded in two areas: networking and observability. It is simply 
  an orders of magnitude larger problem to network and debug a set of 
  intertwined distributed services versus a single monolithic application.
  
  Originally built at Lyft, Envoy is a high performance C++ distributed proxy 
  designed for single services and applications, as well as a communication bus
  and “universal data plane” designed for large microservice “service mesh” 
  architectures. Built on the learnings of solutions such as NGINX, HAProxy, 
  hardware load balancers, and cloud load balancers, Envoy runs alongside every
  application and abstracts the network by providing common features in a 
  platform-agnostic manner. When all service traffic in an infrastructure flows
  via an Envoy mesh, it becomes easy to visualize problem areas via consistent
  observability, tune overall performance, and add substrate features in a 
  single place.


  Out of process architecture
  
  Envoy is a self contained, high performance server with a small memory 
  footprint. It runs alongside any application language or framework.

  HTTP/2 and gRPC support
  Envoy has first class support for HTTP/2 and gRPC for both incoming and 
  outgoing connections. It is a transparent HTTP/1.1 to HTTP/2 proxy.

  Advanced load balancing
  Envoy supports advanced load balancing features including automatic retries, 
  circuit breaking, global rate limiting, request shadowing, zone local load 
  balancing, etc.

  APIs for configuration management
  Envoy provides robust APIs for dynamically managing its configuration.

  Observability
  Deep observability of L7 traffic, native support for distributed tracing, and
  wire-level observability of MongoDB, DynamoDB, and more.
</pre } >
  </td>
  <td>
  Utilising Linux BPF:
<pre xxxsmall zoom { >
https://www.infoq.com/news/2018/03/cilium-linux-bpf?utm_medium=SpecialNL_EditorialContent&utm_cam
Cilium is open source software for transparently securing the network 
connectivity between application services deployed using Linux container 
management platforms like Docker and Kubernetes. Cilium 1.0.0-rc4 has recently 
been released, which includes: the Cloud Native Computing Foundation 
(CNCF)-hosted Envoy configured as the default HTTP/gRPC proxy; the addition of 
a simple health overview for connectivity and other errors; and an improved 
scalable kvstore interaction layer.

Microservices applications tend to be highly dynamic, and this presents both a 
challenge and an opportunity in terms of securing connectivity between 
microservices. Modern approaches to overcoming this issue have coalesced around
the CNCF-hosted Container Network Interface (CNI) and the increasingly popular 
"service mesh" technologies, such as Istio and Conduit. According to the Cilium
documentation, traditional Linux network security approaches (such as iptables)
filter on IP address and TCP/UDP ports. However, the highly volatile life cycle
of containers and IP addresses cause these approaches to struggle to scale 
alongside the application as the large number of load balancing tables and 
access control lists must be updated continually.

Cilium attempts to address the issue with scaling by utilising a (relatively) 
new technology called Berkeley Packet Filter (BPF). BPF is a Linux kernel 
bytecode interpreter that was originally introduced to filter network packets, 
as seen in tcpdump and socket filters. It has been extended with additional 
data structures such as hash tables and arrays as well as additional actions to
support packet mangling, forwarding, encapsulation, etc. An in-kernel verifier 
ensures that BPF programs are safe to run and a JIT compiler converts the 
bytecode to CPU architecture specific instructions for native execution 
efficiency. For readers keen to explore BPF in further detail, performance Guru
Brendan Gregg has written and talked extensively about "Linux BPF Superpowers".
</pre }>
  </td>
</tr>
</table>
</body>
<!--

____________________
https://thenewstack.io/5-things-to-know-before-adopting-microservice-and-container-architectures/
____________________
TOPICS:
We now deploy software applications by stringing together services that run on a distributed set of computing resources and communicate over different networking protocols: A typical application can include:
- web servers
- application servers
- memory-based caching systems
- task queues
- message queues
- SQL databases
- NoSQL datastores
- load balancers

We also need to make sure we have:
 - redundancies 
 - logging
 - monitoring
 - analytics
 - third-party services:
   - REST APIs
   - infrastructure-as-a-service (IaaS) endpoints

 - configuration management:
   - Ansible playbooks, (Chef, Puppet, Salt,..)
 - Infrastructure as code: Jenkins "scripts" , ...
________________
Alan Kay’s maxim: "Simple things should be simple; complex things should be possible."
________________
Elastic Search
________________
Kibana
_________________
Kayenta Canary Testing:
https://www.infoq.com/news/2018/04/kayenta
____________________
observability:   login + monitoring + tracing
_____________
[DataAnalysis] pandas
__________
https://opensource.com/tools/enterprise-resource-planning



-->

</html>
