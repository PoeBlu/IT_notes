<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>architecture map (alpha) (ignore)</title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body onLoad='onPageLoaded()'>
<b id='initialMessage' orange>Hint double-click on elements to zoom!!</b>
<div id='zoomDiv'></div>
<div style="position:fixed; right:0.3%; bottom:0; width:auto;">
<b style="font-size:1.5rem" orange><a onclick="onZoomOut()">[-A]</a></b>
<b style="font-size:1.5rem"       >                                 </b>
<b style="font-size:2.0rem" orange><a onclick="onZoomIn ()">[A+]</a></b>
</div>
<table>
<tr>
<td>
<pre xxsmall zoom>
Tags:
[performance]
[redundancy]
[log]        Loggin
[monitoring]
[analytics]
[testing]
[CI]         Continuous Integration
[IaC]        Infrastructure as code
</pre>
</td>


<td>
  [log]<a href="https://www.fluentd.org/">FluentD</a><br/>
  (Improved "logstat")
<pre xxxsmall zoom { >
Open Source data collector for unified logging layer.

Fluentd allows you to unify data collection and consumption for
a better use and understanding of data.

Syslog                      Elasticsearch
Apache/Nginx logs    → → →  MongoDB
Mobile/Web app logs  → → →  Hadoop
Sensors/IoT                 AWS, GCP, ...
</td>

<td>
  Hadoop:
  Hadoop "vs" Spark
  <a href="https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html">REF</a>
<pre xxxsmall zoom>
Hadoop is essentially a distributed data infrastructure: 
 -It distributes massive data collections across multiple nodes 
  within a cluster of commodity servers
 -It also indexes and keeps track of that data, enabling
  big-data processing and analytics far more effectively
  than was possible previously. 
Spark, on the other hand, is a data-processing tool that operates on those 
distributed data collections; it doesn't do distributed storage.

You can use one without the other: 
  - Hadoop includes not just a storage component, known as the 
  Hadoop Distributed File System, but also a processing component called 
  MapReduce, so you don't need Spark to get your processing done.
  - Conversely, you can also use Spark without Hadoop. Spark does not come with
  its own file management system, though, so it needs to be integrated with one
  -- if not HDFS, then another cloud-based data platform. Spark was designed for
  Hadoop, however, so many agree they're better together.

Spark is generally a lot faster than MapReduce because of the way it processes 
data. While MapReduce operates in steps, Spark operates on the whole data set 
in one fell swoop:
   "The MapReduce workflow looks like this: read data from the cluster, perform
    an operation, write results to the cluster, read updated data from the 
    cluster, perform next operation, write next results to the cluster, etc.," 
    explained Kirk Borne, principal data scientist at Booz Allen Hamilton. 
    Spark, on the other hand, completes the full data analytics operations 
    in-memory and in near real-time: 
    "Read data from the cluster, perform all of the requisite analytic 
    operations, write results to the cluster, done," Borne said.
Spark can be as much as 10 times faster than MapReduce for batch processing and 
p to 100 times faster for in-memory analytics, he said.
  You may not need Spark's speed. MapReduce's processing style can be just fine 
if your data operations and reporting requirements are mostly static and you 
can wait for batch-mode processing. But if you need to do analytics on 
streaming data, like from sensors on a factory floor, or have applications that
require multiple operations, you probably want to go with Spark.
 Most machine-learning algorithms, for example, require multiple operations. 

Recovery: different, but still good. 
Hadoop is naturally resilient to system faults or failures since data 
are written to disk after every operation, but Spark has similar built-in
resiliency by virtue of the fact that its data objects are stored in something 
called resilient distributed datasets distributed across the data cluster. 
"These data objects can be stored in memory or on disks, and RDD provides full 
recovery from faults or failures," Borne pointed out.
</pre>
</td>

<td>
  <a href="https://prometheus.io/">Prometheus</a> 
  <a xxsmall href="https://github.com/prometheus/prometheus">Git</a><br/>
  monitoring metrics analyzer and alerting
  <ul xxxsmall zoom>
  <li>stores all data as time series: streams of timestamped values belonging
    to the same metric and the same set of labeled dimensions.
  </li>
  <li>provides an expression language to  select and aggregate time
    series data in real time. The result can either be shown as a graph,
    tabular data or consumed by external systems via HTTP API.
  </li>
  <li>Grafana supports querying Prometheus</li>
  <li>(optionally) integrates with remote storage systems.</li>
  <li>alerts can be defined using expression language
    to send notifications about firing alerts to an external service</li>
  </ul>
</td>
<td>
  <a href="http://kafka.apache.org/">Kafka</a> <br/>
  log and events collection, streaming platform, ...
  <ul xxxsmall zoom>
  <li>Each broker in the Kafka cluster has an identity which can be used to find other
brokers in the cluster. The brokers also need some type of a database to 
store partition logs. It's important to configure a Persistent Volume (PV) for 
Kafka, otherwise you will lose the logs.
  </li>
  </ul>
  <a href="http://kafka.apache.org/uses">(Popular) Use cases</a>
  <ul xxxsmall zoom>
  <li><def>Messaging</def> works well as a replacement for a more traditional 
    message broker.  Message brokers are used for a variety of reasons 
   (decoupling processing from data producers, buffering unprocessed messages, 
   etc). In comparison Kafka has better throughput, built-in partitioning, 
   replication, and fault-tolerance which makes it a good solution for large 
   scale message processing applications.
  </li>
  <li><def>Website Activity Tracking</def> was the original use case for Kafka
    to be able to rebuild a user activity tracking pipeline as a set of
    real-time publish-subscribe feeds. Page views, searches, .... is published
    to central topics with one topic per activity type. These feeds are 
    available for subscription for a range of use cases including real-time 
    processing, real-time monitoring, and loading into Hadoop or offline data 
    warehousing systems for offline processing and reporting.
  </li>
  <li><def>Metrics</def> operational monitoring data. This involves aggregating
    statistics from distributed applications to produce centralized feeds of 
    operational data.
  </li>
  <li><def>Log Aggregation</def> replacement for a log aggregation solution. 
    Log aggregation typically collects physical log files off servers and puts 
    them in a central place (a file server or HDFS perhaps) for processing. 
    Kafka abstracts away the details of files and gives a cleaner abstraction 
    of log or event data as a stream of messages. This allows for lower-latency
    processing and easier support for multiple data sources and distributed 
    data consumption. In comparison to log-centric systems like Scribe or Flume,
     Kafka offers equally good performance, stronger durability guarantees due 
     to replication, and much lower end-to-end latency.
  </li>
  <li><def>Stream Processing</def>: Kafka process data in processing pipelines 
   consisting of multiple stages, where raw input data is consumed from Kafka 
   topics and then aggregated, enriched, or otherwise transformed into new 
   topics for further consumption or follow-up processing. For example, a 
   processing pipeline for recommending news articles might crawl article 
   content from RSS feeds and publish it to an "articles" topic; further 
   processing might normalize or deduplicate this content and published the 
   cleansed article content to a new topic; a final processing stage might 
   attempt to recommend this content to users. Such processing pipelines 
   create graphs of real-time data flows based on the individual topics. 
   Starting in 0.10.0.0, a light-weight but powerful stream processing library 
   called Kafka Streams is available in Apache Kafka to perform such data 
   processing as described above. Apart from Kafka Streams, alternative open 
   source stream processing tools include Apache Storm and Apache Samza.
  </li>
  <li><def>Event Sourcing</def> style of application design where state 
    changes are logged as a time-ordered sequence of records.
    Kafka's support for very large stored log data makes it an excellent 
    backend for an application built in this style.
  </li>
  <li><def>Commit Log</def>: Used as kind of external commit-log for a
    distributed system. The log helps replicate data between nodes and acts as 
    a re-syncing mechanism for failed nodes to restore their data. The log 
    compaction feature in Kafka helps support this usage. In this usage Kafka 
    is similar to Apache BookKeeper project. </li>
  </ul>
</td>
</tr>
</table>

<table>
<tr>

  <td>
  <a href="http://spark.apache.org/">Apache Spark</a><br/>
  Cluster computing platform
  <ul xxxsmall zoom>
  <li>
  </li>
  </ul>
<pre xxxsmall zoom { >
  general framework for large-scale data processing that supports lots of 
different programming languages and concepts such as MapReduce, in-memory
processing, stream processing, graph processing, and Machine Learning. This can
also be used on top of Hadoop. Data can be ingested from many sources like 
Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex 
algorithms expressed with high-level functions like map, reduce, join and window.

Common applications for Spark include real-time marketing campaigns, online 
product recommendations, cybersecurity analytics and machine log monitoring.
</pre } >
<br/>
Kafka vs Spark Streaming: <a href="https://dzone.com/articles/spark-streaming-vs-kafka-stream-1">REF</a>
<p xxxsmall zoom { >
""" If event time is very relevant and latencies in the seconds range are
completely unacceptable, Kafka should be your first choice. Otherwise, 
Spark works just fine. <br/>
...
Apache Spark can be used with Kafka to stream the data, but if you are 
deploying a Spark cluster for the sole purpose of this new application, that is
definitely a big complexity hit.<br/>
...
Conclusion
I believe that Kafka Streams is still best used in a "Kafka > Kafka" context, 
while Spark Streaming could be used for a "Kafka > Database" or 
"Kafka > Data science model" type of context.
</p } >
  </td>
<td>
  <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a><br/>
  distributed, RESTful search and analytics engine 
  <ul xxxsmall zoom>
  <li>At the heart of the Elastic Stack,  data is stored centrally</li>
  <li>""discover the expected and uncover the unexpected""</li>
  </ul>
</td>

<td>  
  <a href="https://www.elastic.co/products/kibana">Kibana</a><br/>
  <span cite>A Picture's Worth a Thousand Log Lines</span>
  <ul xxxsmall zoom>
  <li>""visualize (Elasticsearch) data and navigate the Elastic Stack,
  learning  understanding the impact rain might have on your 
  quarterly numbers""</li>
  </ul>

  
</td>

  <td>
  <a href="https://grafana.com/">Grafana</a><br/>
  time series analytics
  <br/>
  <a xsmall href="https://logz.io/blog/grafana-vs-kibana/">Grafana vs Kibana</a>
  </td>
</tr>
</table>

<table>
<tr>
  <th colspan=3 header_delimit xsmall >distributed cache</th>
</tr>
<tr>
<td>
  <a href="http://www.ehcache.org/">Ehcache</a><br/>
  <ul xxxsmall zoom >
  <li>open source, standards-based cache that boosts performance, offloads I/O</li>
  <li>Integrates with other popular libraries and frameworks</li>
  <li>It scales from in-process caching, all the way to mixed in-process/out-of-process
  deployments with terabyte-sized caches</li>
  </ul>
<pre xxxsmall zoom { >
Coding to Ehcache 3 API:
<pre xxxsmall { >
CacheManager cacheManager = CacheManagerBuilder.newCacheManagerBuilder()
    .withCache("preConfigured",
         CacheConfigurationBuilder.newCacheConfigurationBuilder(Long.class, String.class,
             ResourcePoolsBuilder.heap(100))
         .build())
    .build(true);

Cache&lt;Long, String&gt; preConfigured
    = cacheManager.getCache("preConfigured", Long.class, String.class);

Cache&lt;Long, String&gt; myCache = cacheManager.createCache("myCache",
    CacheConfigurationBuilder.newCacheConfigurationBuilder(Long.class, String.class,
                                  ResourcePoolsBuilder.heap(100)).build());

myCache.put(1L, "da one!");
String value = myCache.get(1L);

cacheManager.close();
</pre }>
(simpler/lighter solution but not so escalable could be to use Google Guava Cache)
  <br/>
  <a href="http://jbosscache.jboss.org/">JBoss Cache</a>
</td>
<td>
  <a href="https://www.memcached.org/">Memcached</a><br/>
  distributed memory object caching system
  <ul>
  <li>initially intended to speed up dynamic web applications alleviating 
     database load</li> 
  </ul> 
 
  <a href="https://github.com/magro/memcached-session-manager">
    Memcached-session-manager</a>
  tomcat session manager that keeps sessions in memcached or Redis, for highly 
  available, scalable and fault tolerant web applications
  <ul>
  <li>supports sticky and non-sticky configurations</li>
  <li>Failover is supported via migration of sessions</li>
  </ul>
</td>
<td><a href="https://redis.io/">Redis</a>
  in-memory data structure store, used as a database, cache and message broker
  <ul xxxsmall zoom >
  <li>supports data structures such as strings, hashes, lists, sets, sorted sets
    with range queries, bitmaps, hyperloglogs and geospatial indexes with radius queries</li>
  <li>Redis has built-in replication, Lua scripting, LRU eviction, transactions
    and different levels of on-disk persistence, and provides high availability
    via Redis Sentinel and automatic partitioning with Redis Cluster</li>
  </ul>
</td>
<td><a href="https://github.com/google/guava/wiki/CachesExplained">Guava Cache</a>
  <b>non-distributed</b> easy-to-use Java library for data caching
  <ul xxxsmall zoom >
  <li>A Cache is similar to ConcurrentMap, but not quite the same. The most 
    fundamental difference is that a ConcurrentMap persists all elements that 
    are added to it until they are explicitly removed. A Cache on the other 
    hand is generally configured to evict entries automatically, in order to 
    constrain its memory footprint. In some cases a LoadingCache can be useful
    even if it doesn't evict entries, due to its automatic cache loading.
  </li>
  </ul>
</td>
</tr>
</table>


<table>
<tr>
<td>
  <b>Message Queues</b>
  Defined by
  <ul xxxsmall zoom>
  <li>message oriented architecture</li>
  <li>Persistence (or durability until comsuption)</li>
  <li>queuing</li>
  <li>Routing: point-to-point / publish-and-subscribe</li>
  <li>No processing/transformation of message/data</li>
  </ul>
  Implementations
  <ul xxxsmall zoom>
  <li>
    <a href="https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol">AMQP</a>
    Open standard application layer protocol for message-oriented middleware. <br/>
    Often compared to JMS (Java Message Service): <br/>
    JMS defines the API interfaces, while AMQP defines the network protocol.<br/>
     JMS has no requirement for how messages are formed and transmitted and thus 
    every JMS broker can implement the messages in a different (incompatible) format<br/>
     AMQP publishes its specifications in a downloadable XML format. 
    This availability makes it easy for library maintainers to generate APIs 
    driven by the specs while also automating construction of algorithms to 
    marshal and demarshal messages.<br/>
     These advantages and the openness of the spec have inspired the creation of
     multiple brokers that support AMQP, including:<br/>
    RabbitMQ, ActiveMQ, Qpid, Solace
  </li>
  <li>Java JMS</li>
  <li>...</li>
  </ul>
</td>
<td>
  <b>Message Brokers</b>
  <ul xxxsmall zoom>
  <li>Routing</li>
  <li>(De-)Multiplexing of messages from/into multiple messages to different recipients</li>
  <li>Durability</li>
  <li>Transformation (translation of message between formats)</li>
  <li>"things usually get blurry - many solutions are both (message queue and message
    broker) - for example RabbitMQ or QDB.  Samples for message queues are 
    Gearman, IronMQ, JMS, SQS or MSMQ."<br/>
    Message broker examples are Qpid, Open AMQ or ActiveMQ.</li>
  </ul>
</td>
<td>
  <b>Enterprise Service Bus (ESB)</b>
  Can be defined by next feautes:
  <ul xxxsmall zoom>
  <li>Monitoring of services/messages passed between them</li>
  <li>wire Protocol bridge between HTTP, AMQP, SOAP, gRPC, CVS in Filesystem,...</li>
  <li>Scheduling, mapping, QoS management, error handling, ..</li>
  <li>Data transformation</li>
  <li>Data pipelines</li>
  <li>Mule, JBoss Fuse (Camel + "etc..."), BizTalk, Apache ServiceMix, ...</li>
  <li>
<pre>
REF: https://en.wikipedia.org/wiki/Enterprise_service_bus#/media/File:ESB_Component_Hive.png
    ^   Special App. Services
    |
E   |   Process Automation                 BPEL, Workflow
n   |
t   |   Application Adapters               RFC, BABI, IDoc, XML-RPC, ...
e m |
r e |   Application Data Consolidation     MDM, OSCo, ...
p s |
r s |   Application Data Mapping           EDI, B2B
i a |   _______________________________
s g |   Business Application Monitoring
e e |   _______________________________
    |   Traffic Monitoring Cockpit
S c |
e h |   Special Message Services           Ex. Test Tools
r a |
v n |   Web Services                       WSDL, REST, CGI
i n |
c e |   Protocol Conversion                XML, XSL, DCOM, CORBA
e l |
    |   Message Consolidation              N.N (data locks, multi-submit,...)
B   |
u   |   Message Routing                    XI, WBI, BIZTALK, Seeburger
s   |
    |   Message Service                    MQ Series, MSMQ, ...
</pre>

  </li>
  </ul>
</td>
</tr>
</table>




<table>
<tr>
<th colspan=10 header_delimit xsmall >Example Architectures</th>
</tr>
<tr>
<td>
observability: login + monitoring + tracing

  Ex: (REF: <a href="https://www.rsaconference.com/writable/presentations/file_upload/csv-r14-fim-and-system-call-auditing-at-scale-in-a-large-container-deployment.pdf">File Integrity Monitoring at scale: (RSA Conf)</a>)<br/>
  Auditing log to gain insights at scale:
<pre xxxsmall zoom>
                         +→ Pagerduty
            +-→ Grafana -+→ Email
   Elastic  |            +→ Slack
   Search  -+-→ Kibana
            |
            +-→ Pre-processing  -> TensorFlow

Alt1:
  User   │ go-audit-                          User space
  land   │ container                             app
  ───────├─────  Netlink ───── Syscall iface ───────────
  Kernel │        socket           ^
         │          ^              |
                    └─  Kauditd ───┘
</pre>
</td>
</tr>
</table>


<table>
<tr>
  <th header_delimit colspan=10  xsmall >Continuous Integration</td>
</tr>
<tr>
<td>[CI]Jenkins
   <ul xxxsmall zoom>
   <li><a href="https://jenkins.io/solutions/pipeline/">Pipeline as Code</a></li>
   </ul>
</td>
<td>[CI]CircleCI
</td>
<td>
  [CI]<a href="https://github.com/spinnaker/kayenta">Kayenta</a> Canary Testing
  <ul>
  <li>Kayenta is a platform for Automated Canary Analysis (ACA) </li>
  </ul>
</td>

</tr>
</table>

<br/>
</body>

<!--
https://thenewstack.io/5-things-to-know-before-adopting-microservice-and-container-architectures/
____________________
________________
Full list of CI tools
https://xebialabs.com/technology/nevercode/
_____________
https://en.wikipedia.org/wiki/Infrastructure_as_Code
Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools
_____________
http://www.terracotta.org/

https://en.wikipedia.org/wiki/Terracotta,_Inc.

    BigMemory: a commercial in-memory data management suite for real-time Big Data applications that stores up to hundreds of terabytes in distributed memory across multiple servers without performance hits from the Java garbage collector
    Ehcache: an open source, standards-based cache used to boost performance, offload the database and simplify scalability[5]

    Enterprise Ehcache: a commercial product based on the open source Ehcache project

    Quartz: an open source job scheduling service

    Quartz Scheduler: a commercial product, based on the open source Quartz project

    Web Sessions: a commercial product that provides high availability for Java Servlet Sessions, allowing user session data to survive server restarts and failures[6]
    Terracotta Enterprise Suite: a commercial product that includes BigMemory, Web Sessions and Quartz Scheduler
    In-Genius: a native, in-memory platform for real-time insight and intelligent action on Big Data. General availability is planned for the third quarter of calendar 2013

___________________________
Redis

Cloudify
___________________

Spring  Batch:

FROM https://stackoverflow.com/questions/33188368/spring-batch-vs-quartz-jobs

Quartz is a scheduling framework. Like "execute something every hour or every last friday of the month"

Spring Batch is a framework that defines that "something" that will be executed. You can define a job, that consists of steps. Usually a step is something that consists of item reader, optional item processor and item writer, but you can define a custom stem. You can also tell Spring batch to commit on every 10 items and a lot of other stuff.  From Spring 2 , it can also schedule tasks
(See also https://jcp.org/en/jsr/detail?id=352, Batch applications for the Java Platform)


-->

</html>
