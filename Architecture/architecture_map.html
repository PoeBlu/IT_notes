<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>architecture map (alpha) <!-- ignore --></title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body>

<span title>DB Engines Types</span><br/>
<div groupv>
<pre zoom>
<span xsmall>Ext.Links</span>
@[https://db-engines.com/]
@[https://db-engines.com/en/ranking]
@[https://dzone.com/articles/rant-there-is-no-nosql-data-storage-engine]

<span small>TODO</span>
@[https://www.infoq.com/news/2018/09/pinterest-goku-timeseries-db]
  Pinterest Switches from OpenTSDB to Their Own Time Series Database
- https://facebook.github.io/prophet/
  Prophet: TSDB forecasting library (wrapper around Stan), particularly
  approachable place to use Bayesian Inference for forecasting use cases
  general purpose.
  - Prophet is a procedure for forecasting time series data based on an
    additive model where non-linear trends are fit with yearly, weekly, and daily
    seasonality, plus holiday effects. It works best with time series that have
    strong seasonal effects and several seasons of historical data. Prophet is
    robust to missing data and shifts in the trend, and typically handles outliers well.

@[https:// technologyconversations.com/2015/09/08/service-discovery-zookeeper-vs-etcd-vs-consul/]
@[https://www.infoq.com/news/2019/05/hashicorp-consul-1.5.0]
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>RDBMS</span>
ºRDBMSº
ºFeaturesº
- support E-R data model.
- table schema defined by the
  table name and fixed number
  of attributes and data types.
- A record (=entity) corresponds
  to a row in the table.
- basic operations are defined on
  the tables/relations:
  - CRUD: Create/Read/Update/Delete
  - set ops: union|intersect|difference
  - subset selection defined by filters
  - Projection of subset of table columns
  - JOIN: combination of:
    Cartesian_product+selection+projection
  - TX ACID control
  - user management
- Ops defined in  standard SQL
ºHistoryº
- beginning of 1980s
- Most widely used DBMS

ºMain  Examplesº
- Oracle
- MySQL
- TiDB
  - MySQL compatible
  - RAFT-distributed
  - Rust/go written
  - Features:
    - "infinite" horizontal scalability
    - strong consistency
    - HA
- SQL Server
- PostgreSQL
- DB2
- Hive (https://db-engines.com/en/system/Hive)
  - Home: https://hive.apache.org/
  - RºWARNº: No Foreign keys, NO ACID
  -ºEventual Consistencyº
  - Data Warehouse designed for Hadoop
  - Implemented in Java
  - supports analysis of large datasets
    stored in Hadoop's HDFS and
    compatible file systems such as
    Amazon S2 filesystem.
  - SQL-like DML and DDL statements
    Traditional SQL queries implemented in
    MapReduce Java API to  execute SQL apps:
    - necessary SQL abstraction provided to
      integrate SQL-like queries (HiveQL) into
      the underlying Java without the need
      for low-level queries.
  - Hive aids portability of SQL-based apps
    to Hadoop
  - JDBC, ODBC, Thrift
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>KEY-VALUE</span>
ºKEY-VALUE STORESº                    
-ºsimplestºform of DBMS.              
- store pairs of keys and values      
-ºhigh performanceº                   
- not adequate for complex apps       
- extended forms allows to sort the   
  keys, enabling range queries as well
  as an ordered processing of keys.   
- Can evolve to document stores and   
  wide column stores.                 
                                      
ºExamplesº                            
- Redis                               
- Amazon DynamoDB                     
- Memcached                           
- Microsoft Azure Cosmos DB           
- Hazelcast
  - Designed for caching.
  - Hazelcast clients, by default, will
    connect to all cache cluster nodes
    an know about the cluster partition
    table to route request to the correct
    node.
  - Java JCache compliant
  - Advanced cache eviction algorithms
    based on heuristics with sanitized O(1)
    runtime behavior.
- LabelDB ("Ethereum State")          
- TiKV                                
  - dev lang:ºRustº                   
  - Incubation Kubernetes project     
  - used also for TiDB                
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Search</span>
- "NoSQL" DBMS for search of content         
- Optimized for:                             
  - complex search expressions               
  - Full text search                         
  - reducing words to stem                   
  - Ranking and grouping of results          
  - Geospatial search                        
  - Distributed search for high              
    scalability                              
ºExamplesº                                   
- Elasticsearch                              
- Splunk                                     
- Solr                                       
- MarkLogic                                  
- Sphinx                                     
- Eclipse Hawk:                              
  projects.eclipse.org/projects/modeling.hawk
  heterogeneous model indexing framework:    
  - indexes collections of models            
    transparently and incrementally into     
    NoSQL DDBB, which can be queried         
    efficiently.                             
  - can mirror EMF, UML or Modelio           
    models (among others) into a Neo4j or    
    OrientDB graph, that can be queried      
    with native languages, or Hawk ones.     
    Hawk will watch models and update        
    the graph incrementally on change.       
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Time Series</span>
- Managing time series data
- very high TX load:
  - designed to efficiently collect,
    store and query various time
    series.
- Ex use-case:
  SELECT SENSOR1_CPU_FREQUENCY / SENSOR2_HEAT'
  joins two time series based on the
  overlapping areas of time providing
  new time-serie
                                               
ºExamplesº
- Timescale.com
  - PostgreSQL optimized for Time Series.
    by modifying the insert path,
    execution engine, and query planner
    to "intelligently" process queries
    across chunks.
- InfluxDB
- Kdb+
- Graphite. Simple system that will just:
  - Store numeric time series data
  - Render graphs of this data
  It willºNOTº:
  - collect data (Carbon needed)
  - render graphs (external apps exists)
- RRDtool
- Prometheus
  - https://prometheus.io/
  - CNCF project,ºkubernetes friendlyº
  - GoLang based:
    - binaries statically linked
    - easy to deploy.
  - Many client libraries.
  - monitoring metrics analyzer
    and alerting
  - Highly dimensional data model.
    Time series are identified by a
    metric name and a set of
    key-value pairs.
  - stores all data as time series:
    streams of timestamped values
    belonging to same metric and
    same set of labeled dimensions.
  - Multi-mode data visualization.
  - Grafana integration
  - Built-in expression browser.
  - PromQL Query language allowing
    to select+aggregate time-series
    data in real time.
    - result can either be shown as
      graph, tabular data or consumed
      by external HTTP API.
  - Precise alerts based on PromQL.
  - scaling:
    - sharding
    - federation
  - "Singleton" servers relying only
    on local storage.
    (optionally remote storage).
                                               
- Uber M3
  www.infoq.com/news/2018/08/uber-metrics-m3
  - Large Scale Metrics Platform
""" built to replace Graphite+Carbon cluster,
    and Nagios for alerting and Grafana for
    dashboarding due to issues like
    poor resiliency/clustering, operational
    cost to expand the Carbon cluster,
    and a lack of replication""".
  ºFeaturesº
  - cluster management, aggregation,
    collection, storage management,
    a distributed TSDB
  - M3QL query language (with features
    not available in PromQL).
  - tagging of metrics.
  - local/remote integration similar to
    the Prometheus Thanos extension providing
    cross-cluster federation, unlimited
    storage and global querying across
    clusters, works.
  - query engine: single global view
    without cross region replication.
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Graph DBMS</span>
- represent data in graph structures           
  of nodes and edges (relations).              
- easy processing, simple calculation          
  of specific graph properties:                
  - number of steps needed to                  
    get from one node to another node          
- (usually) DON'T provide indexes on           
  all nodes. Direct access to                  
  nodes based on attribute values              
  is not possible                              
ºExamplesº                                     
- Neo4j                                        
- Azure Cosmos DB                              
- Datastax Enterprise                          
- OrientDB                                     
- ArangoDB                                     
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>RDF stores</span>
db-engines.com/en/article/RDF+Stores    
- Resource Description Framework stores 
  ºdescribes information in triplets:º  
 º(subject,predicate,object)º           
- Originally used for describing        
  IT-resources-metadata.                
- Today often used in semantic web.     
- RDS is a subclass of graph DBMS:      
  (subject,predicate,object)            
   ^          ^      ^                  
   node      edge   node                
  but it offer specific methods         
  beyond general graph DBMS ones. Ex:   
  SPARQL, SQL-like query lang. for      
  RDF data, supported by most           
  RDF stores.                           
                                        
ºExamplesº                              
- MarkLogic                             
- Jena                                  
- Virtuoso                              
- Amazon Neptune                        
- GraphDB
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Service Registry</span>
 ºand Discoveryº

 - Specialized key/value DBMS:
   -ºtwo processes must existsº:
     -ºService registration processº
       storing final-app-service
       (host,port,...)
     -ºService discovery processº
       - let final-app-services query
         the data
   - other aspects to consider:
     - auto-delete of non-available services
     - Support for replicated services
     - Remote API is provided

 ºExamplesº
 - ZooKeeper
   - originated from Hadoop ecosystem.
   - data-format similar to file system.
   - cluster mode.
   - Disadvantages:
     - complex:
     - Java plus big number of dependencies
   - Still used by kafka for config but
     plans exists to replace it.
     issues.apache.org/jira/browse/KAFKA-6598
 - etcd
   - distributed key/value store
   - HTTP remote API.
   - hierarchical config.
   - Very easy to deploy, setup and use
   - reliable data persistence
   - very good doc
   - coreos.com/blog/introducing-zetcd
   Disadvantages:
   - needs to be combined with few
     third-party tools for  serv.discover:
     - etcd-registrator: keep updated  list
                       of docker containers
     - etcd-registrator-confd: keep updated
                               config files
     - ...
   - Used by Kubernetes for config

 - Consul
   - strongly consistent datastore
   - multidatacenter gossip protocol
     for dynamic clusters
   - hierarchical key/value store
   - adds the notion of app-service
     (and app-service-data).
     - "watches" can be used for:
       - sending notifications of
         data changes
       - (HTTP, TTLs , custom)
         health checks and
         output-dependent
         commands

   - embedded service discovery:
     no need to use third-party one
     (like etcd). Discovery includes:
     - node         health checks
     - app-services health checks
     - ...
   - Consul provides a built in framework
     for service discovery.
     (vs etcd basic key/value + custom code)
     - Clients just register services and
       perform discovery using the DNS
       or HTTP interface.
   - out of the box native support for
     multiple datacenters.
   - template-support for config files.
   - Web UI: display all services and nodes,
     monitor health checks, switch
     from one datacenter to another.
 - doozerd (TODO)
 See also:
 - Comparision chart:
 coreos.com/etcd/docs/latest/learning/why.html
</pre>

<pre zoom>
<span xsmall>Zookeeper</span>
<span xsmall>Internals</span>
https://zookeeper.apache.org/doc/r3.2.2/zookeeperInternals.html#sc_guaranteesPropertiesDefinitions
</pre>

<pre zoom>
<span xsmall>Zookeeper</span>
<span xsmall>vs etcd3</span>
https://loneidealist.wordpress.com/2017/07/12/apache-zookeeper-vs-etcd3/
</pre>

</div>

<div groupv>
<pre zoom>
<span xsmall>Wide Column Stores</span>
(also called extensible record stores)       
- store data in records                      
- ability to hold very large numbers         
  of dynamic columns.                        
  - column names and record keys             
   ºare not fixedº                           
    ^^^^^^^^^^^^^                            
    (schema-free)                            
- a record can have billions of              
  columns.                                   
-ºshort of two-dimensionalº                  
 ºkey-value stores.       º                  
- not to be confused with                    
  column oriented storage of RDMS            
  (last one is an internal concept           
  for improving performance storing          
  table-data column-by-column vs             
  record-by-record)                          
ºExamplesº                                   
- Cassandra                                  
  - SQL-like SELECT, DML and DDL             
    statements (CQL)                         
  - Designed to handle large amounts         
    of data across many commodity            
    servers, providing high availability     
    with no single point of failure.         
  - robust support for clusters              
    spanning multiple datacenters,           
    with asynchronous masterless             
    replication allowing low latency         
    operations for all clients.              
  - Related:                                 
    www.infoq.com/news/2018/10/spotify-cstar 
    Spotify Open Sources cstar,              
    its Cassandra Orchestration Tool:        
    ... Cstar emerged from the necessity     
    of running shell commands in Cassandra   
    nodes                                    
  -ºIntegrated Managed Data    º             
   ºLayer Solutions with Spark,º             
   ºKafka and Elasticsearch.   º             
                                             
- Scylla                                     
  - compatible with Cassandra                
    (same CQL and Thrift protocols,          
     and same SSTable file formats)          
    with  higher throughputs and             
    lower latencies (10x).                   
  - C++14 (vs Cassandra Java)                
  -ºSeastar async lib replacing threadsº     
  - sharded design by node:                  
    - each CPU core handles a data-subset    
    - authors claim to achieve much better   
      performance on modern NUMA SMP archs,  
      and to scale very well with the number 
      of cores:                              
      -ºUp to 2 million req/sec per machineº 
- HBase. Apache alternative to Google BigTable
  Internally uses skip-lists:
@[../programming_theory.html?query=e65f9917-5f27-4b78-8a5a-0653640b6b88]
_ CosmosDB                                   
REF: Google bigTable-osdi06.pdf              
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Document Stores</span>
- also called document-oriented DBMS           
- schema-free:                                 
  - different records may have                 
    different columns                          
  - values of individual columns               
    can have dif. types                        
- Columns can be multi-value                   
- Records can have a nested structure          
- Often use internal notations,                
  mostly JSON.                                 
- features:                                    
  - secondary indexes in (JSON) objects        
                                               
ºExamplesº                                     
- MongoDB                                      
  www.infoq.com/articles/Starting-With-MongoDB 
  14 Things I Wish I’d Known When Starting with
  MongoDB                                      
- Amazon DynamoDB                              
- Couchbase                                    
- Cosmos DB                                    
- CouchDB                                      
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Data(log)</span>
<span xsmall>collect</span>
ºFluentDº                                     |ºLogstashº 
 "Improved" logstat                           |- TheºLº in ELK
 - https://www.fluentd.org/                   |-*OS data Collector
 - data collector for unified logging layer   |- TODO:
 - increasingly used Docker, GCP,             |  osquery.readthedocs.io/en/stable/
   and Elasticsearch communities              |- low-level instrumentation framework 
 - https://logz.io/blog/fluentd-logstash      |  with system analytics and monitoring
   FluentD vs Logstash compared               |  both performant and intuitive.
ºFeaturesº                                    |- osquery exposes an operating system
 - unify data collection and consumption      |  as a high-performance SQL RDMS:
   for better understanding of data.          |  - SQL tables represent concepts such
                                              |    as running processes, loaded kernel
                                              |    modules, open network connections,
Syslog                      Elasticsearch     |    browser plugins, hardware events
Apache/Nginx logs    → → →  MongoDB           |ºFeatures:º
Mobile/Web app logs  → → →  Hadoop            |  - File Integrity Monitoring (FIM):
Sensors/IoT                 AWS, GCP, ...     |  - DNS
                                              |  - *1

*1: https://medium.com/palantir/osquery-across-the-enterprise-3c3c9d13ec55



|ºPrometheus Node ExporteRº                     |ºOthersº
|- https://github.com/prometheus/node_exporter  |- collectd
|- TODO:                                        |- Dynatrace OneAgent
                                                |- Datadog agent
                                                |- New Relic agent
                                                |- Ganglia gmond
                                                |- ...
</pre>
<pre zoom>
<span xsmall>Loki</span>
@[https://grafana.com/loki]
- logging backend, optimized for Prometheus and Kubernetes
- optimized to search, visualize and explore your logs natively in Grafana.
</pre>
<pre zoom>
<span xsmall>ELK</span>
@[https://opensource.com/article/18/9/open-source-log-aggregation-tools]
  - Elasticsearch, Logstash, and Kibana
  - developed and maintained by Elastic.
  - Elasticsearch:
    - essentially a NoSQL, Lucene search engine implementation.
  - Logstash: log pipeline (ingest/transform/load it into a store 
              like Elasticsearch)
    (It is common to replace Logstash with Fluentd)
  - Kibana: visualization layer on top of Elasticsearch.
  - In productiona few other pieces might be included like:
    - Kafka, Redis, NGINX, ....

<span xsmall>Graylog</span>
  - gaining popularity in the Go community with
    the introduction of the Graylog Collector Sidecar
    written in Go.
  - ... it still lags far behind the ELK stack.
  - Composed  under the hood of:
    - Elasticsearch
    - MongoDB
    - Graylog Server.
  - comes with alerting built into the open source version
    and streaming, message rewriting, geolocation, ....

<span xsmall>Fluentd</span>
  - ºIt is not a log aggregation system.º
  - developed at Treasure Data, 
  - Adopted by the CNCF as Incubating project.
  - Recommended by AWS and Google Cloud.
  - Common replacement for Logstash 
    - It acts as a local aggregator to collect
      all node logs and send them off to central 
      storage systems.
  - 500+ plugins for  quick and easy integrations with
    different data input/outputs.
  - common choice in Kubernetes environments due to:
    - low memory requirements (tens of megabytes)
      (each pod has a Fluentd sidecar)
    - high throughput.
</pre>
</div>

<div groupv>
<pre zoom TODO>
<span xsmall>Logreduce</span>
<span xsmall> IA filter</span>
<a xsmall TODO href="https://opensource.com/article/18/9/quiet-log-noise-python-and-machine-learning">Logreduce IA filter</a>
  - <a href=https://pypi.org/project/logreduce/">logreduce@pypi</a>
  - Quiet log noise with Python and machine learning
</pre>
</div>

<div groupv>

<pre zoom>
<span xsmall TODO>Streams</span>
Distributed (<a href="https://martinfowler.com/eaaDev/EventSourcing.html">Event</a>) Stream Processors
REF:
- <a href="https://en.wikipedia.org/wiki/Stream_processing">Stream_processing@Wikipedia</a>
- <a href="https://iwringer.wordpress.com/2015/08/03/patterns-for-streaming-realtime-analytics/">Patterns for streaming realtime anaylitics</a>
- <a href="https://www.infoq.com/streaming">???</a>
- <a href="https://www.infoq.com/presentations/squbs">Streaming Reactive Systems & Data Pites w. Squbs</a>
- <a href="https://www.infoq.com/presentations/sql-streaming">Streaming SQL Foundations: Why I Love Streams+Tables </a>
- <a href="https://www.infoq.com/presentations/flink-stateful-streaming">Next Steps in Stateful Streaming with Apache Flink</a>
- <a href="https://www.infoq.com/presentations/kafka-streams-spring-cloud">Kafka Streams - from the Ground Up to the Cloud </a>
- <a href="https://www.infoq.com/presentations/facebook-stream-processing">Data Decisions with Real-Time Stream Processing</a>
- <a href="https://www.infoq.com/presentations/beam-model-stream-table=theory">Foundations of streamng SQL</a>
- <a href="https://www.infoq.com/presentations/distributed-stream-processing-flink">The Power of Distributed Snapshots in Apache Flink</a>
- <a href="https://www.infoq.com/presentations/sql-streams-panel">Panel: SQL over Streams, Ask the Experts</a>
- <a href="https://www.infoq.com/presentations/hbc-digital-streaming">Survival of the Fittest - Streaming Architectures</a>
- <a href="https://www.infoq.com/presentations/netflix-personalization-datasets-streaming">Streaming for Personalization Datasets at Netflix</a>
- < href="https://www.safaribooksonline.com/library/view/an-introduction-to/9781491934951/">An Introduction to Time Series with Team Apache </a>
""" Apache Cassandra evangelist Patrick McFadin shows how to solve time-series data
    problems with technologies from Team Apache: Kafka, Spark and Cassandra.
      - Kafka: handle real-time data feeds with this "message broker"
      - Spark: parallel processing framework that can quickly and efficiently
               analyze massive amounts of data
      - Spark Streaming: perform effective stream analysis by ingesting data 
               in micro-batches
      - Cassandra: distributed database where scaling and uptime are critical
      - Cassandra Query Language (CQL): navigate create/update your data and data-models
      - Spark+Cassandra: perform expressive analytics over large volumes of data

|ºKafkaº@[./kafka_map.html]                             | ºSparkº (TODO)
|-@[https://kafka.apache.org]                           | @[http://spark.apache.org/]
|- scalable,persistent and fault-tolerant               | - Zeppelin "Spark Notebook"(video):
|  real-time log/event processing cluster.              | @[https://www.youtube.com/watch?v=CfhYFqNyjGc]
|- It's NOT a real stream processor  but a              | 
|  broker/message-bus to store stream data              | 
|  for comsuption.                                      | 
|- "Kafka stream" can be use to add                     | 
|  data-stream-processor capabilities                   | 
|- Main use cases:                                      | 
|  - real-time reliable streaming data                  | ºFlinkº TODO
|    pipelines between applications                     | - Includes a powerful windowing system
|  - real-time streaming applications                   |   supports many types of windows:
|    that transform or react to the                     |   - Stream windows and win.aggregations are crucial
|    streams of data                                    |     building block for analyzing data streams.
|- Each node-broker in the cluster has an               |
|  identity which can be used to find other             |
|  brokers in the cluster. The brokers also             |
|  need some type of a database to store                |
|  partition logs.                                      |
|-@[http://kafka.apache.org/uses]                       |
|  (Popular) Use cases                                  |
|  BºMessagingº: good replacement for traditional       |
|    message brokers decoupling processing from         |
|    data producers, buffering unprocessed messages,    |
|    ...) with better throughput, built-in              |
|    partitioning, replication, and fault-tolerance     |
|  BºWebsite Activity Trackingº(original use case)      |
|    Page views, searches, ... are published            |
|    to central topics with one topic per activity      |
|    type.                                              |
|  BºMetricsº operational monitoring data.              |
|  BºLog Aggregationº replacement.                      |
|    - Group logs from different servers in a central   |
|      place.                                           |
|    - Kafka abstracts away the details of files and    |
|      gives a cleaner abstraction of logs/events as    |
|      a stream of messages allowing for lower-latency  |
|      processing and easier support for multiple data  |
|      sources and distributed data consumption.        |
|      When compared to log-centric systems like        |
|      Scribe or Flume, Kafka offers equally good       |
|      performance, stronger durability guarantees due  |
|      to replication, and much lower end-to-end        |
|      latency.                                         |
|  BºStream Processingº: processing pipelines           |
|    consisting of multiple stages, where raw input     |
|    data is consumed from Kafka topics and then        |
|    aggregated/enriched/transformed into new           |
|    topics for further consumption.                    |
|    - Kafka 0.10.+ includes Kafka-Streams to           |
|      easify this pipeline process.                    |
|    - alternative open source stream processing        |
|      tools include Apache Storm and Samza.            |
|  BºEvent Sourcing architectureº: state changes        |
|    are logged as a time-ordered sequence of           |
|    records.                                           |
|  BºExternal Commit Log for distributed systemsº:      |
|    - The log helps replicate data between             |
|      nodes and acts as a re-syncing mechanism         |
|      for failed nodes to restore their data.          |

See also:
-Flink vs Spark Storm:
@[https://www.confluent.io/blog/apache-flink-apache-kafka-streams-comparison-guideline-users/]
Check also: <a href="https://data-artisans.com/blog/serializable-acid-transactions-on-streaming-data">Streaming Ledger</a>. Stream ACID TXs on top of Flink


- LodDevice(Facebook)
@[https://www.infoq.com/news/2018/09/logdevice-distributed-logstorage]
  - LogDevice has been compared with other log storage systems
    like Apache BookKeeper and Apache Kafka. 
  - The primary difference with Kafka 
  (@[https://news.ycombinator.com/item?id=17975328] 
    seems to be the decoupling of computation and storage
  - Underlying storage based on RocksDB, a key value store
    also open sourced by Facebook
</pre>

</pre>

<pre zoom>
<span xsmall>Event Based</span>
<span xsmall>Architecture</span>
@[https://www.infoq.com/news/2017/11/jonas-reactive-summit-keynote]

Jonas Boner ... talked about event driven services (EDA) and 
event stream processing (ESP)... on distributed systems.

... background on EDA evolution over time:
- Tuxedo, Terracotta and Staged Event Driven Architecture (SEDA).

ºevents represent factsº
- Events drive autonomy in the system and help to reduce risk.
- increase loose coupling, scalability, resilience, and traceability.
- ... basically inverts the control flow in the system 
- ... focus on the behavior of systems as opposed to the
  structure of systems.

- TIP for developers:
 ºDo not focus on just the "things" in the systemº
 º(Domain Objects), but rather focus on what happens (Events)º
- Promise Theory:
  - proposed by Mark Burgess
  - use events to define the Bounded Context through
    the lense of promises.

quoting Greg Young:
"""Modeling events forces you to have a temporal focus on what’s going on in the 
system. Time becomes a crucial factor of the system."""
""" Event Logging allows us to model time by treating event as a snapshot in time 
   and event log as our full history. It also allows for time travel in the 
   sense that we can replay the log for historic debugging as well as for 
auditing and traceability. We can replay it on system failures and for data replication."""

Boner discussed the following patterns for event driven architecture:
- Event Loop
- Event Stream
- Event Sourcing
- CQRS for temporal decoupling
- Event Stream Processing

Event stream processing technologies like Apache Flink, Spark Streaming, 
Kafka Streams, Apache Gearpump and Apache Beam can be used to implement these 
design patterns.
</pre>



</div>

<div groupv>
<pre zoom>
<span TODO xsmall>Text indexing</span>
<span TODO xsmall>Search</span>
ºElasticsearchº
@[https://www.elastic.co/products/elasticsearch]
- distributed, RESTful (all-document-type) search 
  and analytics engine
- Implemented on top of Lucene
- Developed alongside a data-collection and
  log-parsing engine called Logstash, and the 
  analytics and visualisation platform  Kibana.
  - The three products form the "Elastic Stack" 
    (formerly the "ELK stack"
  - At the heart of the ELK Stack, data is 
    stored centrally
</pre>
<pre zoom>
<span xsmall>Solr</span>
@[http://lucene.apache.org/solr/]
- blazing-fast, search platform built on Apache Lucene
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Data Routing</span>
ºApache NiFiº
@[https://nifi.apache.org]
- Web-GUI data route+transform
- scalable directed graphs of data routing,
  transformation, and system mediation logic.
- Seamless experience between design, control,
  feedback, and monitoring
- Highly configurable:
  - Loss tolerant vs guaranteed delivery
  - Low latency vs high throughput
  - Dynamic prioritization
  - Flow can be modified at runtime
  - Back pressure
- Data Provenance
  - Track dataflow from beginning to end
- Designed for extension
  - Build your own processors and more
  - Enables rapid development and 
    effective testing
- Secure
  - SSL, SSH, HTTPS, encrypted content, etc...
  - Multi-tenant authorization and internal
    authorization/policy management
REFS:
- NiFi+Spark:
@[https://blogs.apache.org/nifi/entry/stream_processing_nifi_and_spark"]
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Data</span>
<span xsmall>visualization</span>
|ºKibanaº (TODO)
|@[https://www.elastic.co/products/kibana]
|- "A Picture's Worth a Thousand Log Lines"
|- visualize (Elasticsearch) data and navigate
|  the Elastic Stack, learning  understanding 
|  the impact rain might have on your quarterly
|  numbers

ºGrafanaº (TODO)
@[https://grafana.com/]
- time series analytics
- Integrates with Prometheus queries
@[https://logz.io/blog/grafana-vs-kibana/]
</pre>
</div>
<br/>
<hr/>
<div groupv>
<span title>Enterprise Service Bus (ESB)</span>
<pre zoom labels="">
<span xsmall>ESB Architecture</span>
- Can be defined by next feautes:
  - Monitoring of services/messages passed between them
  - wire Protocol bridge between HTTP, AMQP, SOAP, gRPC, CVS in Filesystem,...
  - Scheduling, mapping, QoS management, error handling, ..
  - Data transformation
  - Data pipelines
  - Mule, JBoss Fuse (Camel + "etc..."), BizTalk, Apache ServiceMix, ...
REF: https://en.wikipedia.org/wiki/Enterprise_service_bus#/media/File:ESB_Component_Hive.png
    ^   Special App. Services
    |
E   |   Process Automation                 BPEL, Workflow
n   |
t   |   Application Adapters               RFC, BABI, IDoc, XML-RPC, ...
e m |
r e |   Application Data Consolidation     MDM, OSCo, ...
p s |
r s |   Application Data Mapping           EDI, B2B
i a |   _______________________________
s g |   Business Application Monitoring
e e |   _______________________________
    |   Traffic Monitoring Cockpit
S c |
e h |   Special Message Services           Ex. Test Tools
r a |
v n |   Web Services                       WSDL, REST, CGI
i n |
c e |   Protocol Conversion                XML, XSL, DCOM, CORBA
e l |
    |   Message Consolidation              N.N (data locks, multi-submit,...)
B   |
u   |   Message Routing                    XI, WBI, BIZTALK, Seeburger
s   |
    |   Message Service                    MQ Series, MSMQ, ...
</pre>

<span title>Distributed cache</span>
<pre zoom labels="cache,comparative">
(Extracted from whitepaper by Christoph Engelbert, Soft.Arch.at Hazelcast)
cache hit: data is already available in the cache when requested
           (otherwise it's said cache miss)

- Caches are implemented as simple key-value stores for performance.
- Caching-First: term to describe the situation where you start thinking
       about Caching itself as one of the main domains of your application.

ºUssageº
-ºReference Dataº
  - normally small and used to speed up the dereferencing
    of previously known, fixed number of elements (e.g.
    states of the USA, abbreviations of elements,...).
-ºActive DataSetº
- Grow to their maximum size and evict the oldest or not
  frequently used entries to keep in memory bounds. 

<span xsmall>Caching Strategies</span>
ºCooperative (Distributed) cachingº
  different  cluster-nodes work together to build a huge, shared cache 
  Ussually an "intelligent" partitioning algorithm is used to balance
  load about cluster nodes.
  - common approach when system requires large amounts of data to be cached

ºPartial Cachingº
- not all data is stored in the cache. 

ºGeographical Cachingº
- located in chosen locations to optimize latency 
- CDN (Content Delivery Network) is the best known example of this type of cache
- Works well when  content changes less often.

ºPreemptive Cachingº
- mostly used in conjunction with a Geographical Cache
- Using a warm-up engine a Preemptive Cache is populated on startup
  and tries to update itself based on rules or events.
- The idea behind this cache addition is to reload data from any 
  backend service or central cluster even before a requestor wants
  to retrieve the element. This keeps access time to the cached
  elements constant and prevents accesses to single elements from 
  becoming unexpectedly long.
- Can be difficult to implement properly and requires a lot of
  knowledge of the cached domain and the update workflows

ºLatency SLA Cachingº
- It's able to maintain latency SLAs even if the cache is slow
  or overloaded. This type of cache can be build in two different ways.
  - Having a timeout to exceed before the system either requests
    the potentially cached element from the original source 
    (in parallel to the already running cache request) or simple 
    default answer, using whatever returns first.
  - Always fire both requests in parallel and take whatever returns first.
    (discouraged since it mostly dimiss the value of caching). Can make
    sense if multiple caching layers are available. 

<span xsmall>Caching Topologies</span>
-ºIn-process:º
 - cache share application's memory space.
 - most oftenly used in non-distributed systems. 
 - fastest possible access speed.
 - Easy to build, but complex to grow.

-ºEmbedded Node Cachesº
- the application itself will be part of the cluster. 
- kind of combination between an In-Process Cache and the 
  Cooperative Caching
- it can either use partitioning or full dataset replication.

- CONST: Application and cache cannot be scaled independently

-ºClient-Server Cachesº
- these systems tend to be Cooperative Caches by having a
  multi-server architecture to scale out and have the 
  same feature set as the Embedded Node Caches
  but with the client layer on top.
- This architecture keeps separate clusters of the applications 
  using the cached data and the data itself, offering 
  the possibility to scale the application cluster and the
  caching cluster independently. 

<span xsmall>Evict Strategies</span>
-ºLeast Frequently usedº
 - values that are accessed the least amount of times are 
   remove on memory preasure.
 - each cache record must keep track of its accesses using
   a counter which is increment only.

-ºLeast Recently Usedº
 - values that were last used most far back in terms of time
   are removed on memory preasure.
 - each record keeps must track of its last access timestamp
Other evict strategies can be found at
@[https://en.wikipedia.org/wiki/Cache_replacement_policies]

</pre>
<pre zoom>
<span xsmall>Memcached</span>
@[https://www.memcached.org/
- distributed memory object caching system
- Memcached servers are unaware of each other. There is no crosstalk, no
  syncronization, no broadcasting, no replication. Adding servers increases
  the available memory. Cache invalidation is simplified, as clients delete
  or overwrite data on the server which owns it directly
- initially intended to speed up dynamic web applications alleviating database load

ºMemcached-session-managerº
@[https://github.com/magro/memcached-session-manager]
  tomcat HA/scalable/fault-tolerant session manager
- supports sticky and non-sticky configurations
- Failover is supported via migration of sessions
@[https://www.infoworld.com/article/3063161/why-redis-beats-memcached-for-caching.html]
</pre>
<pre zoom>
<a xsmall href="https://redis.io/">Redis</a>
- in-memory data structure store, used as a key-value database and cache 
- Since it can also notify listener of changes in its state it can
  also be used as message broker (this is the case for example 
  in Kubernetes, where etcd implement an asynchronous message system
  amongst its componentes).
- supports data structures such as strings, hashes, lists, sets, sorted sets
  with range queries, bitmaps, hyperloglogs and geospatial indexes with
  radius queries
- Redis has built-in replication, Lua scripting, LRU eviction, transactions
  and different levels of on-disk persistence, and provides high availability
  via Redis Sentinel and automatic partitioning with Redis Cluster
@[https://www.infoworld.com/article/3063161/why-redis-beats-memcached-for-caching.html]
@[https://www.infoq.com/news/2018/10/Redis-5-Released]
</pre>
<pre zoom>
<span xsmall>Hazelcast</span>
<span xsmall>in-memory</span>
<span xsmall>data grid</span>
@[https://en.wikipedia.org/wiki/Hazelcast]
-  based on Java
</pre>
<pre zoom>
<span xsmall>Ehcache</span>
<span xsmall>(terabyte)</span>
<span xsmall>cache</span>
@[http://www.ehcache.org/]
- Can be used as tcp service (distributed cache) or process-embedded
  TODO:  Same API for local and distributed objects?
- open source, standards-based cache that boosts performance, offloads I/O
- Integrates with other popular libraries and frameworks
- It scales from in-process caching, all the way to mixed 
  in-process/out-of-process deployments withºterabyte-sized cachesº

ºExample Ehcache 3 APIº:
CacheManager cacheManager = 
  CacheManagerBuilder.newCacheManagerBuilder()
  .withCache("preConfigured",
    CacheConfigurationBuilder.newCacheConfigurationBuilder(Long.class, String.class,
      ResourcePoolsBuilder.heap(100))
  .build())
  .build(true);

Cache˂Long, String˃ preConfigured =
    = cacheManager.getCache("preConfigured", Long.class, String.class);

Cache˂Long, String˃ myCache = cacheManager.createCache("myCache",
    CacheConfigurationBuilder.newCacheConfigurationBuilder(Long.class, String.class,
                                  ResourcePoolsBuilder.heap(100)).build());

myCache.put(1L, "da one!");
String value = myCache.get(1L);

cacheManager.close();

(simpler/lighter solution but not so escalable could be to use Google Guava Cache)
</pre>

<pre zoom>
<span TODO xsmall>JBoss Cache</span>
@[http://jbosscache.jboss.org/]
</pre>
<span xsmall>Local In-Memory Cache</span>
<pre zoom>
<span xsmall>Guava Cache</span>
@[https://github.com/google/guava/wiki/CachesExplained]
- ºnon-distributedº easy-to-use Java library for data caching

- A Cache is similar to ConcurrentMap, but not quite the same. The most
  fundamental difference is that a ConcurrentMap persists all elements that
  are added to it until they are explicitly removed. A Cache on the other
  hand is generally configured to evict entries automatically, in order to
  constrain its memory footprint. In some cases a LoadingCache can be useful
  even if it doesn't evict entries, due to its automatic cache loading.
</pre>
</div>

<div groupv>
<span title>Messaging</span>
<pre zoom>
<span xsmall>Summary</span>
Messaging traditionally has two models:
ºqueuingº
  - a pool of consumers may read from a server and 
    each record goes to one of them;
  - Pros: allows to divide up the processing of data
        over multiple consumer instances, which
        lets you scale your processing.
  - Cons: queues aren't multi-subscriber—once
        one process reads the data it's gone.

ºpublish-subscribeº
  - the record is broadcast to all consumers. 
  - Pros: let broadcast data to multiple processes,
  - Cons: no way of scaling processing since every message
          goes to every subscriber


ºMessage Queuesº
Defined by
- message oriented architecture
- Persistence (or durability until comsuption)
- queuing
- Routing: point-to-point / publish-and-subscribe
- No processing/transformation of message/data

ºImplementations and standardsº
- AMQP
@[https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol]
  - Open standard network protocol
  - Often compared to JMS:
    - JMS defines API interfaces, AMQP defines network protocol
    - JMS has no requirement for how messages are formed and
      transmitted and thus every JMS broker can implement the 
      messages in a different (incompatible) format.
      AMQP publishes its specifications in a downloadable XML format,
      allowing library maintainers to generate APIs driven by
      the specs while also automating construction of algorithms to
      marshal and demarshal messages.
  - brokers implementations supporting it: 
    RabbitMQ, ActiveMQ, Qpid, Solace, ...
    @[https://www.amqp.org/about/examples]
    - Apache Qpid (TODO)
    @[http://qpid.apache.org/]
    - INETCO's AMQP protocol analyzer
    @[http://www.inetco.com/resource-library/technology-amqp/]
    - JORAM  JMS + AMPQ
    @[http://joram.ow2.org/] 100% pure Java implementation of JMS
    - Kaazing's AMQPºWeb Clientº
    @[http://kaazing.net/index.html]
    - Azure Service Bus+ AMPQ
- What's wrong with AMQP?
@[https://news.ycombinator.com/item?id=1657574]
- @[http://www.windowsazure.com/en-us/develop/net/how-to-guides/service-bus-amqp-overview/]
- JBoss A-MQ, built from @[http://qpid.apache.org/]
@[http://www.redhat.com/en/technologies/jboss-middleware/amq]
- IBM MQLight
@[https://developer.ibm.com/messaging/mq-light/]
- StormMQ
@[http://stormmq.com/]
  a cloud hosted messaging service based on AMQP
- RabbitMQ (by VMware Inc)
@[http://www.rabbitmq.com/];
  also supported by SpringSource
...
- Java JMS
</pre>
<pre zoom>
<span xsmall>Message Brokers</span>
- Routing
- (De-)Multiplexing of messages from/into multiple messages to different recipients
- Durability
- Transformation (translation of message between formats)
- "things usually get blurry - many solutions are both (message queue and message
  broker) - for example RabbitMQ or QDB.  Samples for message queues are
  Gearman, IronMQ, JMS, SQS or MSMQ."
  Message broker examples are, Qpid, Open AMQ or ActiveMQ.
- Kafka can also be used as message broker but is not its main intention
</pre>
<pre zoom labels="queue">
<span xsmall>OpenHFT</span>
<span xsmall>microSec Messaging</span>
<span xsmall>storing to disk</span>
https://github.com/OpenHFT/
   - Chronicle-Queue: Micro second messaging that stores everything to disk
   - Chronicle-Accelerate: HFT meets Blockchain in Java platform
         XCL is a new cryptocurrency project that, learning from the previous
         Blockchain implementations, aims to solve the issues limiting adoption
         by building an entirely new protocol that can scale to millions of
         transactions per second, delivering consistent sub-second latency. Our
         platform will leverage AI to control volatility and liquidity, require
         low energy and simplify compliance with integrated KYC and AML support.

         The XCL platform combines low latencies (sub millisecond), IoT transaction
         rates (millions/s), open source AI volatility controls and blockchain for
         transfer of value and exchange of value for virtual fiat and crypto
         currencies. This system could be extended to other asset classes such as
         securities and fixed income. It uses a federated services model and
         regionalized payment systems making it more scalable than a blockchain
         which requires global consensus.

         The platform makes use of Chronicle-Salt for encryption and Chronicle-Bytes.

         on Chronicle Core’s direct memory and OS system call access.



   - Chronicle-Logger: A sub microsecond java logger, supporting standard logging
                      APIs such as Slf &amp; Log4J

</pre>
</div>

<div groupv>
<span title>Distributed Storage</span>
<pre zoom>
<span xsmall>Data Lake</span>
@[https://en.wikipedia.org/wiki/Data_lake]
</pre>

<pre zoom>
<span xsmall>NFS</span>
<span xsmall>considered</span>
<span xsmall>harmful</span>
@[http://www.time-travellers.org/shane/papers/NFS_considered_harmful.html]
</pre>
<pre zoom>
<span xsmall>cluster-FS</span>
<span xsmall>comparative</span>
@[http://zgp.org/linux-tists/20040101205016.E5998@shaitan.lightconsulting.com.html]
</pre>
<pre zoom>
<span xsmall>Ceph</span>
<span xsmall>Up-to-exabytes</span>
@[http://ceph.com/ceph-storage/]
Ceph’s RADOS provides you with extraordinary data storage  scalability—
thousands of client hosts or KVMs accessing petabytes to
exabytes of data. Each one of your applications can use the object, block or
file system interfaces to the same RADOS cluster simultaneously, which means
your Ceph storage system serves as a  flexible foundation for all of your
data storage needs. You can use Ceph for free, and deploy it on economical
commodity hardware. Ceph is a  better way to store data.

By decoupling the namespace from the underlying hardware, object-based
storage systems enable you to build much larger storage clusters. You
can scale out object-based storage systems using economical commodity hardware
, and you can replace hardware easily when it malfunctions or fails.

Ceph’s CRUSH algorithm liberates storage clusters from the scalability and
performance limitations imposed by centralized data table mapping. It
replicates and re-balance data within the cluster dynamically—elminating this
tedious task for administrators, while delivering high-performance and
infinite scalability.

See more at: http://ceph.com/ceph-storage/#sthash.KNp2tGf5.dpuf
</pre>
<pre zoom>
<span TODO xsmall>GlusterFS</span>
</pre>
<pre zoom>
<span xsmall>Tachyon</span>
<span xsmall>memory-centric</span>
<span xsmall> distributed FS</span>
@[http://tachyon-project.org/index.html]
- memory-centric distributed file system enabling reliable file sharing at memory-speed
across cluster frameworks, such as Spark and MapReduce. It achieves high performance by leveraging
lineage information and using memory aggressively. Tachyon caches working set files in memory,
thereby avoiding going to disk to load datasets that are frequently read. This enables different
jobs/queries and frameworks to access cached files at memory speed.

Tachyon is Hadoop compatible. Existing Spark and MapReduce programs can run on top of it without
any code change. The project is open source (Apache License 2.0) and is deployed at multiple companies.
It has more than 40 contributors from over 15 institutions, including Yahoo, Intel, and Redhat.

The project is the storage layer of the Berkeley Data Analytics Stack (BDAS) and also part of the
Fedora distribution.
</pre>
<pre bgorange zoom>
<span xsmall>S3QL</span>
<span xsmall>FUSE FS</span>
<span xsmall>(Amazon S2, GCS, </span>
<span xsmall>OpenStack,...)</span>
- FUSE-based file system 
- backed by several cloud storages:
  - such as Amazon S2, Google Cloud Storage, Rackspace CloudFiles, or OpenStack
@[http://xmodulo.com/2014/09/create-cloud-based-encrypted-file-system-linux.html]
- S3QL is one of the most popular open-source cloud-based file systems.
- full featured file system:
  - unlimited capacity
  - up to 2TB file sizes
  - compression
  - UNIX attributes
  - encryption
  - snapshots with copy-on-write
  - immutable trees
  - de-duplication
  - hardlink/symlink support, etc.
- Any bytes written to an S3QL file system are compressed/encrypted
  locally before being transmitted to cloud backend.
- When you attempt to read contents stored in an S3QL file system, the
  corresponding objects are downloaded from cloud (if not in the local
  cache), and decrypted/uncompressed on the fly.
</pre>

<pre zoom labels="storage,distributed">
<span xsmall>Minio.io</span>
@[https://minio.io]
- Private Cloud Storage
- high performance distributed object storage server, designed for
  large-scale private cloud infrastructure. Minio is widely deployed across the
  world with over 146.6M+ docker pulls.
</pre>

</div>

<div groupv>
<span title>Example Architectures</span>
<pre zoom>
<span xsmall>observability: </span>
<span xsmall>      loging</span>
<span xsmall>+ monitoring</span>
<span xsmall>+    tracing</span>
Ex:ºFile Integrity Monitoring at scale: (RSA Conf)º
@[https://www.rsaconference.com/writable/presentations/file_upload/csv-r14-fim-and-system-call-auditing-at-scale-in-a-large-container-deployment.pdf]
Auditing log to gain insights at scale:

                         ┌─→ Pagerduty
            ┌─→ Grafana ─┼─→ Email
   Elastic  │            └─→ Slack
   Search  ─┼─→ Kibana
            │
            └─→ Pre─processing ─→ TensorFlow

Alt1:
  User   │ go-audit-                          User space
  land   │ container                             app
  ───────├─────  Netlink ───── Syscall iface ───────────
  Kernel │        socket           ^
         │          ^              |
                    └─  Kauditd ───┘
</pre>
<pre zoom>
<span xsmall>https://zipkin.io/</span>
</pre>
<span title>Continuous Integration</span>
<pre zoom labels="">
<span xsmall>Shell scripts</span>
</pre>
<pre zoom>
<span TODO xsmall>Jenkins</span>
@[https://jenkins.io/doc/]
@[https://jenkins.io/solutions/pipeline/"
@[https://thenewstack.io/ibm-openstack-engineer-urges-cncf-consider-augmenting-jenkins-zuul/]
</pre>

<pre zoom>
<span xsmall>Zuul</span>
@[https://zuul-ci.org/]
- Use the same Ansible playbooks to 
  deploy your system and run your tests.


REF:@[https://www.mediawiki.org/wiki/Continuous_integration/Zuul]
"""...Zuul is a python daemon which acts as a gateway between
Gerrit and Jenkins. It listens to Gerrit stream-events feed and
trigger jobs function registered by Jenkins using the Jenkins Gearman
plugin. The jobs triggering specification is written in YAML and
hosted in the git repository integration/config.git as /zuul/layout.yaml """

</pre>

<pre zoom>
<span TODO xsmall>Kayenta</span>
<span TODO xsmall>Canary</span>
<span TODO xsmall>Testing</span>
@[https://github.com/spinnaker/kayenta]
- Kayenta platform:  Automated Canary Analysis (ACA)
</pre>
<pre TODO zoom>
<span xsmall>SonarQube</span>
<span xsmall>Apply quality</span>
<span xsmall>metrics to</span>
<span xsmall>source-code</span>
</pre>

<pre TODO zoom>
<span xsmall>Selenium</span>
<span xsmall>Browser test</span>
<span xsmall>automation</span>

</pre>
</div>

<div groupv>
<span title>AAA</span>

<pre zoom labels="">
<span xsmall>keycloak</span>
@[https://www.keycloak.org/]
"""Add authentication to applications and secure services with minimum fuss"""
-  No need to deal with storing users or authenticating users. 
- It's all available out of the box.
- Advanced features such as User Federation, Identity Brokering and Social Login.

   Single-Sign On                        LDAP and Active Directory
   Login once to multiple applications   Connect to existing user directories
   Standard Protocols                    Social Login
   OpenID Connect, OAuth 2.0             Easily enable social login
   and SAML 2.0
   
   Identity Brokering                    Clustering
   OpenID Connect or SAML 2.0 IdPs       For scalability and availability
   High Performance                      Themes
   Lightweight, fast and scalable        Customize look and feel
   
   Extensible
   Customize through code
   Password Policies
   Customize password policies
</pre>
<pre zoom>
<span xsmall>FreeIPA</span>
@[https://www.reddit.com/r/linuxadmin/comments/apbjtc/freeipa_groups_and_linux_usernames/]
</pre>
</div>




<div groupv>
<span title>Non-classified</span>
<pre TODO zoom>
<span xsmall>API Management</span>
https://www.datamation.com/applications/top-api-management-tools.html
</pre>
<pre TODO zoom>
<span xsmall>Behaviour</span>
<span xsmall>Driven</span>
<span xsmall>Development</span>
<span xsmall>(BDD)</span>
- Cucumber, ...
@[https://cucumber.io/docs/guides/10-minute-tutorial/]
</pre>





<pre zoom>
<span xsmall>Loggin@Coinbase</span>
@[https://www.infoq.com/news/2019/02/metrics-logging-coinbase]
</pre>


<br/>

<pre zoom>
<span xsmall>Nexenta</span>
@[https://nexenta.com/]
- Software-Defined Storage Product Family.
</pre>




<pre zoom labels="" TODO>
<span xsmall>Disruptor</span>
  <a href="http://lmax-exchange.github.io/disruptor/files/Disruptor-1.0.pdf">Disruptor</a>
  <a xsmall href="http://prisconapoli.github.io/development/2015/08/01/Disruptor">See also</a>
  <a href="https://lmax-exchange.github.io/disruptor/">lmax-exchange.github.com</a>

  <a href="https://dzone.com/articles/when-disruptor-not-good-fit">When Disruptor is not good fit</a>
</pre>
</div>

<div groupv>
<span title>Big Data</span>

<pre zoom>
<span xsmall>Ambari</span>
<span xsmall>Hadoop cluster</span>
<span xsmall>provisioning</span>
@[https://projects.apache.org/project.html?ambari]
Apache Ambari makes Hadoop cluster provisioning, managing, and monitoring dead simple.
</pre>

<pre zoom labels="">
<span xsmall>Spark</span>
@[http://spark.apache.org/]
- Speed
  - Run workloads 100x faster.
  - Apache Spark achieves high performance for both batch 
    and streaming data, using a state-of-the-art DAG scheduler,
    a query optimizer, and a physical execution engine.

- Ease of Use
  - Write applications quickly in Java, Scala, Python, R, and SQL.
  - Spark offers over 80 high-level operators that make it easy to
    build parallel apps. And you can use it interactively from the
    Scala, Python, R, and SQL shells.
    - Example PythonBºDataFrame APIº. 
      |Bºdfº=ºspark.read.jsonº("logs.json") ← automatic schema inference
      |Bºdfº.where("age ˃ 21")
      |    .select("name.first").show()


- Generality
  - Combine SQL, streaming, and complex analytics.
  - Spark powers a stack of libraries including SQL and DataFrames, 
    MLlib for machine learning, GraphX, and Spark Streaming.
    You can combine these libraries seamlessly in the same application.

- Runs Everywhere
  - Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, 
    or in the cloud. It can access diverse data sources.

- You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN,
  on Mesos, or on Kubernetes. Access data in HDFS, Alluxio, Apache Cassandra,
  Apache HBase, Apache Hive, and hundreds of other data sources.

- Common applications for Spark include real-time marketing campaigns, online 
  product recommendations, cybersecurity analytics and machine log monitoring.

</pre>

<pre zoom>
<span xsmall>Hadoop "vs" Spark</span>
@[https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html]
Hadoop is essentially a distributed data infrastructure: 
 -It distributes massive data collections across multiple nodes 
  within a cluster of commodity servers
 -It also indexes and keeps track of that data, enabling
  big-data processing and analytics far more effectively
  than was possible previously. 
Spark, on the other hand, is a data-processing tool that operates on those 
distributed data collections; it doesn't do distributed storage.

You can use one without the other: 
  - Hadoop includes not just a storage component, known as the 
  Hadoop Distributed File System, but also a processing component called 
  MapReduce, so you don't need Spark to get your processing done.
  - Conversely, you can also use Spark without Hadoop. Spark does not come with
  its own file management system, though, so it needs to be integrated with one
  - if not HDFS, then another cloud-based data platform. Spark was designed for
  Hadoop, however, so many agree they're better together.

Spark is generally a lot faster than MapReduce because of the way it processes 
data. While MapReduce operates in steps, Spark operates on the whole data set 
in one fell swoop:
   "The MapReduce workflow looks like this: read data from the cluster, perform
    an operation, write results to the cluster, read updated data from the 
    cluster, perform next operation, write next results to the cluster, etc.," 
    explained Kirk Borne, principal data scientist at Booz Allen Hamilton. 
    Spark, on the other hand, completes the full data analytics operations 
    in-memory and in near real-time: 
    "Read data from the cluster, perform all of the requisite analytic 
    operations, write results to the cluster, done," Borne said.
Spark can be as much as 10 times faster than MapReduce for batch processing and 
p to 100 times faster for in-memory analytics, he said.
  You may not need Spark's speed. MapReduce's processing style can be just fine 
if your data operations and reporting requirements are mostly static and you 
can wait for batch-mode processing. But if you need to do analytics on 
streaming data, like from sensors on a factory floor, or have applications that
require multiple operations, you probably want to go with Spark.
 Most machine-learning algorithms, for example, require multiple operations. 

Recovery: different, but still good. 
Hadoop is naturally resilient to system faults or failures since data 
are written to disk after every operation, but Spark has similar built-in
resiliency by virtue of the fact that its data objects are stored in something 
called resilient distributed datasets distributed across the data cluster. 
"These data objects can be stored in memory or on disks, and RDD provides full 
recovery from faults or failures," Borne pointed out.
</pre>


<span title>Data Science</span>

<pre zoom labels="datascience" bgorange>
<span xsmall bgorange>Orange</span>
@[https://orange.biolab.si/screenshots/]
Features:
- Interactive Data Visualization
- Visual Programming
- Student's friendly.
- Add-ons
- Python Anaconda Friendly
  $ conda config --add channels conda-forge
  $ conda install orange3
  $ conda install -c defaults pyqt=5 qt
- Python pip  Friendly
  $ pip install orange3
</pre>
</div>


</body>
</html>

<!--
__________________________
https://www.infoq.com/presentations/graph-query-distributed-execution/?itm_source=infoq&itm_medium=popular_widget&itm_campaign=popular_content_list&itm_content=
Life of a Distributed Graph Database Query
__________________
https://projects.apache.org/project.html?hive
https://www.infoq.com/news/2019/04/hivemq-extension-kafka-mqtt/
________________________
https://docs.confluent.io/current/schema-registry/index.html
___________________________________
https://www.infoq.com/news/2019/07/nats-event-messaging-release/
___________________________________
GraphQL: Facebook-developed language that provides a powerful API to get only the dataset you need in a single request, seamlessly combining data sources.
______________________
- <a href="http://www.serverwatch.com/server-news/linuxcon-how-facebook-monitors-hundreds-of-thousands-of-servers-with-netconsole.html">How Facebook Monitors Hundreds of Thousands of Servers with Netconsole </a>

"""""" He added that Facebook had a system in the past for monitoring
that used syslog-ng, but it was less than 60 percent reliable. 
In contrast, Owens stated netconsole is highly scalable and can 
handle enormous log volume with greater than 99.99 percent 
reliability.  """""""
_________________________________
https://logz.io/blog/zipkin-vs-jaeger/
Request tracing is the ultimate insight tool. Request tracing tracks operations inside and across different systems. Practically speaking, this allows engineers to see the how long an operation took in a web server, database, application code, or entirely different systems, all presented along a timeline. Request tracing is especially valuable in distributed systems where a single transaction (such as “create an account”) spans multiple systems
___________________________
https://www.techrepublic.com/article/why-time-series-databases-are-exploding-in-popularity/
____________________________

An example of such data model is the GLUE Schema,[26] which is used in a distributed information system based on LDAP that enable users, applications and services to discover which services exist in a Grid infrastructure and further information about their structure and state.

_______________________________
@[https://www.ibm.com/developerworks/library/l-nilfs-exofs/index.html]
Why Object storage?
Object storage is an interesting idea and makes for a much more scalable system. It removes portions of the file system from the host and pushes them into the storage subsystem. There are trade-offs here, but by distributing portions of the file system to multiple endpoints, you distribute the workload, making the object-based method simpler to scale to much larger storage systems. Rather than the host operating system needing to worry about block-to-file mapping, the storage device itself provides this mapping, allowing the host to operate at the file level.

Object storage systems also provide the ability to query the available metadata. This provides some additional advantages, because the search capability can be distributed to the endpoint object systems.

Object storage has made a comeback recently in the area of cloud storage. Cloud storage providers (which sell storage as a service) represent their storage as objects instead of the traditional block API. These providers implement APIs for object transfer, management, and metadata management.

__________________________
https://www.infoq.com/news/2019/08/badoo-20-billion-events-per-day/
Data Engineering in Badoo: Handling 20 Billion Events Per Day 
-->

