<html>
<head>
   <meta charset="UTF-8">
   <title>Tensorflow Map (ignore)</title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body onLoad='onPageLoaded()'>
<b id='initialMessage' orange>Hint double-click/long-press on elements to zoom!!</b>
<div id='zoomDiv'></div>
<div style="position:fixed; right:0.3%; bottom:0; width:auto;">
<b style="font-size:1.5rem" orange><a onclick="onZoomOut()">[-A]</a></b>
<b style="font-size:1.5rem"       >                                 </b>
<b style="font-size:2.0rem" orange><a onclick="onZoomIn ()">[A+]</a></b>
</div>
<!-- {{{ START }}} -->

<table>
<tr header_delimit {>
  <td topic >topic</td>
  <td summa >summa</td>
  <td col1  ></td>
  <td col2  ></td>
</tr>
<tr {>
  <td topic ><a href='https://www.tensorflow.org/'>Tensor Flow</a></td>

  <td summa >
    <ul>
      <li>
You might think of TensorFlow Core programs as consisting of two discrete sections:<br/>
    - Building the computational graph<br/>
    - Running the computational graph
      </li>
      <li>TensorBoard: display pictures of the Computational graph </li>
    </ul>
  </td>
  <td col1  >
    <pre>
import tensorflow as tf
    </pre>
  </td>
  <td col2  ></td>
</tr }>
<tr header_delimit {>
  <td colspan=4 >Building blocks</td>
</tr>

<tr {>
  <td topic >tensor</td>
  <td summa >
    <ul>
      <li>
    A tensor consists of a set of primitive values shaped into an array of any number of dimensions. <br/>
      </li>
      <li>tensor rank = number of dimensions</li>
      <li></li>
    </ul>
  </td>
  <td col1  ></td>
  <td col2  ></td>
</tr }>


<tr {>
  <td topic >Session</td>
  <td summa >
    <ul>
      <li></li>
    </ul>
  </td>
  <td col1  >
    <ul>
      <li>
        <pre { >
sess = tf.Session()
# init is a handle to TF sub-graph initializing
# all the global variables
init = tf.global_variables_initializer()
sess.run(init)
        </pre }>
      </li>

  </td>
  <td col2  ></td>
</tr }>

<tr {>
  <td topic >(Graph Tensor) Node</td>
  <td summa >
    <ul>
      <li>Each node takes zero or more tensors as inputs and produces a tensor as an output.</li>
      <li>constant node: takes no inputs, outputs a value it stores internally. </li>
      <li>Operations are also nodes</li>
    </ul>
  </td>
  <td col1  >
    <ul>
      <li>create two floating point constants node1 and node2:
        <pre { >
node1 = tf.constant(3.0, dtype=tf.float32)
node2 = tf.constant(4.0) # tf.float32 implicit
sess.run([node1, node2])
> [3.0, 4.0]
node3 = tf.add(node1, node2)
sess.run(node3)
> [4.0]
        </pre }>
      </li>
      <li>Variables (trainable model)
        <pre { >
W = tf.Variable([ .3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
#                ^initial value 
x = tf.placeholder(tf.float32)
linear_model = W * x + b
session.run(linear_model, {x:[1,2,3,4])
> [0. 0.30000001 0.60000002 0.90000004]
# y: input data
y = tf.placeholder(tf.float32)
<p important>
# A loss function measures how far apart 
# the current model is from provided data (y)
squared_deltas = tf.square(linear_model - y)
loss = tf.reduce_sum(squared_deltas)
sess.run(loss, {
  y: [0, -1, -2, -3],
  x: [1, 2, 3, 4]
})
> 23.66
</p>

        </pre }>
      </li>
   </ul>

  </td>
  <td col2  >
   <ul>
      <li>Manual fix loss function
        <pre { >
fixW = tf.assign(W, [-1.])
fixb = tf.assign(b, [1.])
sess.run([fixW, fixb])
sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]})
> 0.0
        </pre }>
      </li>
   </ul>
  </td>
</tr }>

<tr {>
  <td topic >Computational Graph (CG)</td>
  <td summa >
    <ul>
      <li> A CG is a series of TensorFlow operations arranged into a graph of nodes.
      </li>
      <li>placeholders: used to parameterize external inputs (promise of value provided later)
        <pre {>
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = a + b  # + provides a shortcut for tf.add(a, b)
input={a: 3, b: 4}
sess.run(adder_node, feed_dict=input)
> 7
input={a: [1, 3], b: [2, 4]}
sess.run(adder_node, feed_dict=input)
[3., 7.]

        </pre }>
 
</li>
      <li>
        To make the model trainable, we need to be able to
        modify the graph to get new outputs with the same input.
        Variables allow us to add trainable parameters to a
        graph. They are constructed with a type and initial value:
      </li>
      <li></li>
      <li></li>
    </ul>
  </td>
  <td col1  >
graph=[node1, node2]
sess.run(graph)
  </td>
  <td col2  ></td>
</tr }>

<tr {>
  <td topic >Training (tf.train)</td>

  <td summa >
    <ul>
      <li>
      </li>
    </ul>
  </td>
  <td col1  >
    <pre>
    </pre>
  </td>
  <td col2  ></td>
</tr }>

</table>
</body>
<!--
REF: http://www.alanflavell.org.uk/unicode/unidata25.html
─  ┐  ┠   ┰    ╀   ═   ╠   ╰     ┌─────┬─────┐
                                 │     │     │
━  ┑  ┡   ┱    ╁   ║   ╡   ╱     │     │     │
                                 ├─────┼─────┤
│  ┒  ┢   ┲    ╂   ╒   ╢   ╲     │     │     │
                                 │     │     │
┃  ┓  ┣   ┳    ╃   ╓   ╣   ╳     └─────┴─────┘
                                 ← ↑
┄  └  ┤   ┴    ╄   ╔   ╤   ╴     → ↓

┅  ┕  ┥   ┵    ╅   ╕   ╥   ╵     ┌─────────┐
                                 │         │
┆  ┖  ┦   ┶    ╆   ╖   ╦   ╶     │         │
                                 │         │
┇  ┗  ┧   ┷    ╇   ╗   ╧   ╷     │         │
                                 └─────────┘
┈  ┘  ┨   ┸    ╈   ╘   ╨   ╸ 

┉  ┙  ┩   ┹    ╉   ╙   ╩   ╹ 

┊  ┚  ┪   ┺    ╊   ╚   ╪   ╺ 

┋  ┛  ┫   ┻    ╋   ╛   ╫   ╻ 

┌  ├  ┬   ┼    ╌   ╜   ╬   ╼ 

┍  ┝  ┭   ┽    ╍   ╝   ╭   ╽ 

┎  ┞  ┮   ┾    ╎   ╞   ╮   ╾ 

┏  ┟  ┯   ┿    ╏   ╟   ╯   ╿ 


-->
<!--
ma: https://www.tensorflow.org/get_started/get_started
-->

<!--
keras.io:
high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. 
It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.

__________________
aidu, the Chinese Internet giant, has released ApolloScape, a massive dataset for autonomous vehicle simulation and machine learning.

ApolloScape is an order of magnitude bigger and more complex than existing similar datasets such as Kitti and CityScapes. ApolloScape offers 10 times more high-resolution images with pixel-by-pixel annotations, and includes 26 different recognizable objects such as cars, bicycles, pedestrians and buildings. The dataset offers several levels of scene complexity with increasing number of pedestrians and vehicles, up to 100 vehicles in a given scene, as well as a wider set of challenging environments such as heavy weather or extreme lighting conditions. The ApolloScape dataset is a work in progress, and this release corresponds to the first subset, which contains 144k image frames.

The ApolloScape dataset is part of version 2 of Apollo, Baidu's open autonomous driving platform. The Apollo source code, open sourced under an Apache-2.0 license, includes a 2D/3D simulation driving vehicle environment as well as hardware instructions to set up a vehicle for further data collection. Clear instructions can be found in the Apollo GitHub project to help install the simulation environment within a Docker environment.

This dataset will be used to boost research on automated-learning tasks such as finding the roads (Drivable Area Segmentation), detecting the objects (Road Object Detection), allowing model generalization for different locations or weather conditions (Domain Adaptation of Semantic Segmentation) and tracking moving objects (Instance-level Video Movable Object Segmentation).

These research tasks make up the Workshop on Autonomous Driving (WAD) Challenge sponsored by Baidu and taking place next June during CVPR 2018, the IEEE International Conference on Computer Vision and Pattern Recognition. The WAD challenge regroups researchers and engineers across academia and industries to discuss computer vision applications in autonomous driving.

According to ArsTechnica, Waymo, the self-driving unit of Google parent company Alphabet, is currently leading the global innovation in autonomous vehicles along with GM, while Baidu is for the time being viewed more as a contender in the automated driving sector. Opening up the ApolloScape dataset could be interpreted as a move by Baidu to weaken Google's data advantage and increase its own relative position in the industry.

To that effect, Baidu further announced it has joined the Berkeley DeepDrive (BDD) Industry Consortium, a top-tier research alliance which includes Ford, NVIDIA, Qualcomm, and General Motors. BDD focuses on innovations in deep reinforcement learning, cross-modal transfer learning applied to autonomous driving.

Baidu has also partnered with Udacity, an online data-science education website, to launch on online course titled Intro to Apollo which is part of Udacity’s nano degree on self-driving cars. The course start date has not yet been set.

KPMG’s 2018 Index on Autonomous Vehicles Readiness ranks China at number 16 in terms of the 20 countries preparedness for an autonomous vehicle future. Baidu is one of three major Chinese autonomous driving companies along with JingChi.ai and Pony.ai.
____________
[DataAnalysis] pandas
___________________

En su libro ?Competing on Analytics?, Tom Davenport identifica el uso de las 
soluciones de explotación de datos como una fuente sostenible de ventajas 
competitivas. Este tipo de proyectos, englobados dentro del concepto de 
Analytics (Business Intelligence, Big data, Data mining, Inteligencia 
artificial, Machine learning, etc.), han estado presentes entre las 
principales prioridades de inversión en tecnologías de la información durante 
los últimos años.
_____________________
https://opensource.com/article/18/4/common-voice



Mozilla's open source project, Common Voice, is well on its way to becoming the world’s largest repository of human voice data to be used for machine learning. Common Voice recently made its way into Black Duck's annual Open Source Rookies of the Year list.

What’s special about Common Voice is in the details. Every language is spoken differently—with a wide variation of speech patterns, accents, and intonations—throughout the world. A smart speech recognition engine—that has applications over many Internet of Things (IoT) devices and digital accessibility—can recognize speech samples from a diverse group of people only when it learns from a large number of samples. A speech database of recorded speech from people across geographies helps make this ambitious machine learning possible.

With Common Voice, users record their own voice with a simple tap on a button from the project’s website. The contributor front-end is straightforward—simply go to the project’s site at https://voice.mozilla.org and click on the “Speak up, contribute here!” option. That takes you to the “Speak” page, where you read three consecutive sentences, review (and, if needed, re-record), and save. The saved recording then goes to a voicebank.

review_and_submit.png
Review and submit

The voicebank is currently 12GB in size, with more than 500 hours of English-language voice data that have been collected from 112 countries since the project's inception in June 2017. Though English is the only language currently available, there is scope to expand it to multiple languages this year.

The project aims to collect more than 10,000 hours of CC0-licensed free and open voice data in numerous world languages, which can effectively be used to train machine-learning models for content-based industries—particularly IoT and other speech-dependent applications and organizations. The platform is currently being used to train Mozilla’s TensorFlow implementation of Baidu’s DeepSpeech architecture, and Kaldi, an open source speech recognition toolkit.

Michael Henretty, team lead for the Common Voice project, says, “Common Voice is merely the first step in making speech technology more open and accessible to anyone across the globe.” He hopes that “by collecting and freely sharing this data, we can empower a whole new generation of creators, innovators, entrepreneurs, researchers, and even hobbyists to create amazing voice experiences for whomever they want, wherever they want. “For instance,” Henretty continues, “if someone wanted to create an assistive voice app for blind people who only speak Urdu, we hope to provide the data to make this possible. We believe voice interfaces will soon be everywhere, and with more and more internet-connected devices cropping up in the home, it’s important to make sure no one is getting left behind.”

Mozilla is exploring the Internet of Things with its Web of Things Gateway, Common Voice, and the speech recognition engine, DeepSpeech.

Common Voice is open to contributions—anyone can go to the Speak page and contribute by reading the sentences that appear on the screen. All contributions go to the Data page, which anyone can download at any time for their own use. This page also links to many other similar open datasets. Moreover, plenty of resources are available in the project codebase for developers to use for speech recognition.

-->
</html>
